{
  "filename": "InstanceAssemble_compressed.pdf",
  "upload_time": "2025-11-26T22:56:55.570725",
  "data": {
    "total_pages": 21,
    "pages": [
      {
        "page": 1,
        "content": "InstanceAssemble:  Layout-Aware   Image  Generation                 \n                         via Instance Assembling  Attention                         \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                             QiangXiang1,2,ShuangSun2,BingleiLi1,3,                 \n                    DejiaSong2,HuaxiaLi2,YiboChen2, XuTang2,YaoHu2, JunpingZhang1∗  \n                        1ShanghaiKeyLaboratoryofIntelligentInformationProcessing,   \n                      CollegeofComputerScienceandArtificialIntelligence,FudanUniversity\n                            2XiaohongshuInc. 3ShanghaiInnovationInstitute           \n                        {qxiang24, blli24}@m.fudan.edu.cn, jpzhang@fudan.edu.cn,    \n                 {sunshuang1, dejiasong, lihuaxia, zhaohaibo, tangshen, xiahou}@xiaohongshu.com\n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n               Figure1:Layout-awareimagegenerationresultbyInstanceAssemble.Weshowimagegeneration\n               resultunderpreciselayoutcontrol,rangingfromsimpletointricate,sparsetodenselayouts.\n                                       Abstract                                     \n                                                                                    \n                    Diffusionmodelshavedemonstratedremarkablecapabilitiesingeneratinghigh-\n                    qualityimages. RecentadvancementsinLayout-to-Image(L2I)generationhave\n                    leveragedpositionalconditionsandtextualdescriptionstofacilitatepreciseand\n                    controllableimagesynthesis. Despiteoverallprogress,currentL2Imethodsstill\n                    exhibitsuboptimalperformance. Therefore,weproposeInstanceAssemble,anovel\n                    architecturethatincorporateslayoutconditionsviainstance-assemblingattention,\n                    enabling position control with bounding boxes (bbox) and multimodal content\n                    controlincludingtextsandadditionalvisualcontent. Ourmethodachievesflexible\n                    adaptiontoexistingDiT-basedT2Imodelsthroughlight-weightedLoRAmodules.\n                    Additionally,weproposeaLayout-to-Imagebenchmark,Denselayout,acompre-\n                    hensivebenchmarkforlayout-to-imagegeneration,containing5kimageswith90k\n                    instancesintotal. WefurtherintroduceLayoutGroundingScore(LGS),aninter-\n                    pretableevaluationmetrictomorepreciselyassesstheaccuracyofL2Igeneration.\n                    ExperimentsdemonstratethatourInstanceAssemblemethodachievesstate-of-the-\n                    artperformanceundercomplexlayoutconditions,whileexhibitingstrongcompati-\n                    bilitywithdiversestyleLoRAmodules. Thecodeandpretrainedmodelsarepub-\n                    liclyavailableathttps://github.com/FireRedTeam/InstanceAssemble.\n                 ∗Correspondingauthor.                                              \n               39thConferenceonNeuralInformationProcessingSystems(NeurIPS2025).     \n  5202                                                                              \n  tcO                                                                               \n  82                                                                                \n  ]VC.sc[                                                                           \n  2v19661.9052:viXra",
        "char_count": 5597,
        "has_tables": false
      },
      {
        "page": 2,
        "content": "1 Introduction                                                       \n                                                                                    \n               Diffusionmodels[22]haverevolutionizedimagegenerationtask,witharchitectureslikeDiffusion\n               Transformer(DiT)[40]offeringsuperiorqualityovertraditionalUNet-basedframeworks. Recent\n               implementationssuchasStableDiffusion3/3.5[15,49]andFlux.1[4]furtherenhancetext-to-image\n               alignment, paving the way for advancements in layout-controlled generation. Layout-to-Image\n               (L2I)generationisataskthatfocusesoncreatingimagesunderlayoutconditions,allowingusers\n               todefinespatialpositionsandsemanticcontentofeachinstanceexplicitly. Thistaskfacesseveral\n               significantchallenges:(i)ensuringpreciselayoutalignmentwhilemaintaininghighimagequality,(ii)\n               preservingobjectpositionsandsemanticattributesaccuratelyduringtheiterativedenoisingprocess\n               ofdiffusionmodels,and(iii)supportingvarioustypesofreferenceconditions,suchastexts,images\n               andstructureinformation. Thesechallengeshighlightthecomplexityofachievingrobustandflexible\n               layout-controlledimagegeneration.                                    \n               ExistingL2Imethodscanbebroadlycategorizedintotraining-freeandtraining-basedapproaches,\n               bothpossessingdistinctadvantagesandlimitations. Training-freemethods[55,7,13,8,5,27]rely\n               onheuristictechniqueswithoutmodifyingthebasemodel. However,thesemethodsoftenexhibit\n               degradedperformanceincomplexlayouts,demonstratehighsensitivitytohyperparametertuning,\n               andsufferfromslowinferencespeed,whichmakethemlesspracticalforreal-worldapplications.\n               Incontrast,training-basedmethods[63,53,64,29,61]involvetrainingspecificlayoutmodulesto\n               improvelayoutalignment,whichintroducesasignificantamountofextraparametersandincreases\n               trainingcomplexityandresourcerequirements. Additionally, existingL2Ievaluationmetricsex-\n               hibitinaccuracies,suchasfalseacceptanceandlocalizationerrors. Theseidentifiedshortcomings\n               necessitatealgorithminnovationforeffectiveandefficientlayout-controlledimagegeneration.\n               Therefore, we propose InstanceAssemble, a novel framework that systematically tackles these\n               issuesthroughinnovativedesignandefficientimplementation. Ourapproachintroducesacascaded\n               InstanceAssemblestructure,whichemploysamultimodalinteractionparadigmtoprocessglobal\n               prompts and instance-wise layout conditions sequentially. By leveraging the Assemble-MMDiT\n               architecture,weapplyanindependentattentionmechanismtothesemanticcontentofeachinstance,\n               thusenablingeffectivehandlingofdenseandcomplexlayouts. Furthermore,weadoptLoRA[23]\n               forlightweightadaptation,addingonly71MparameterstoSD3-Medium(2B)and102MtoFlux.1\n               (11.8B).Ourmethodenablespositioncontrolwithboundingboxesandmultimodalcontentcontrol\n               includingtextsandadditionalvisualcontent. Thislightweightdesignpreservesthecapabilitiesof\n               thebasemodelwhileenhancingflexibilityandefficiency. Wealsointroduceanovelmetriccalled\n               LayoutGroundingScore(LGS)toensureaccurateevaluationforL2Igeneration,alongsideatest\n               datasetDenseLayout. Thismetricprovidesaconsistentbenchmarkforassessinglayoutalignment.\n               Ourmethodachievesstate-of-the-artperformanceacrossbenchmarksanddemonstratesrobustlayout\n               alignmentunderawidevarietyofscenarios,rangingfromsimpletointricate,sparsetodenselayouts.\n               Notably,despitebeingtrainedonsparselayouts(≤ 10instances),ourapproachmaintainsrobust\n               generalization capability on dense layouts (≥ 10 instances), confirming the effectiveness of our\n               proposedInstanceAssemble. Themaincontributionsarelistedbelow.        \n                  1. WeproposeacascadedInstanceAssemblestructurethatprocessesglobaltextpromptsand\n               layoutconditionssequentially,enablingrobusthandlingofcomplexlayoutsthroughanindependent\n               attentionmechanism.                                                  \n                  2. ByleveragingLoRA[23],weachieveefficientadaptationwithminimalextraparameters\n               (3.46%onSD3-Mediumand0.84%onFlux.1),supportingpositioncontrolwithmultimodalcontent\n               controlwhilepreservingcapabilitiesofbasemodel.                       \n                  3. WeproposeanewtestdatasetDenseLayoutandanovelmetricLayoutGroundingScore\n               (LGS)forLayout-to-Imageevaluation. Experimentalresultsdemonstratethatourapproachachieves\n               state-of-the-artperformanceandrobustcapabilityundercomplexanddenselayoutconditions.\n               2 RelatedWork                                                        \n               Text-to-ImageGenerationText-to-imagesynthesis[47,45,42,40,30,17,6,1,44,3]haswitnessed\n               rapidprogresswiththedevelopmentofdiffusionmodels.Initialworks[44,3,47,45]utilizeUNet[46]\n               as the denoising architecture, leveraging cross-attention mechanisms to inject text-conditioning\n                                          2",
        "char_count": 5895,
        "has_tables": false
      },
      {
        "page": 3,
        "content": "Global Prompt 1. Global Prompt – Image 2. Instances - Image       \n                                Global Prompt        Global Prompt                  \n                “                                                                   \n                a                                                                   \n                o                                                                   \n                 Va                                                                 \n                 f                                                                  \n                 i                                                                  \n                 sh                                                                 \n                 aq                                                                 \n                 si                                                                 \n                 u                                                                  \n                  us kat                                                            \n                  nr                                                                \n                  ao                                                                \n                  ie                                                                \n                  lr g.i                                                            \n                  h                                                                 \n                   c                                                                \n                   tT                                                               \n                   Ia ,hl                                                           \n                   e                                                                \n                   n                                                                \n                   r                                                                \n                   sNm                                                              \n                    im                                                              \n                    too don                                                         \n                    in                                                              \n                    aiu                                                             \n                    nu                                                              \n                    nsm gme                                                         \n                     e                                                              \n                     cen                                                            \n                     on                                                             \n                     et                                                             \n                     nt                                                             \n                     l                                                              \n                     ai                                                             \n                      Co                                                            \n                      s                                                             \n                      oc                                                            \n                      h                                                             \n                      a                                                             \n                      oa                                                            \n                      nt                                                            \n                      r                                                             \n                       te ssd                                                       \n                       et                                                           \n                       e                                                            \n                       .a                                                           \n                       ni ”tn                                                       \n                       u                                                            \n                       t                                                            \n                        e                                                           \n                         redocnEredocnE                                             \n                          EAVtxeT                                                   \n                            L Enay co odut                                          \n                              er                                                    \n                                xN                                                  \n                                 Image MMDiT                                        \n                                           1In                                      \n                                            2I sm taag                              \n                                             n                                      \n                                             3e                                     \n                                             ces                                    \n                                              4                                     \n                                                As Ms Mem Dib Tle- 1In              \n                                                       2I sm taag                   \n                                                       n                            \n                                                        3e                          \n                                                        ces                         \n                                                         4                          \n                                                            redoceD                 \n                                                             EAV                    \n                 Textual Instance Content                                           \n                                                           Assemble-MMDiT           \n                 “A tall, pointed white structure                                   \n                 w s “ s d “ hi p e B itAt h t r sa h e a o ttma r i n ouh i l z rejo c s e i e Sr a cosi l a e aft pz n q l i aco f d u an i e f, tt n a s i h ia i t gw ol a r r ue r\n                    a                                                               \n                    l i i ra s ltb d a ee a a e n sh n t r e w Ld s dr i as t a t ee t h t a pd h\n                     y                                                              \n                     a e t i on t u cb od p o e tr r                                \n                      u                                                             \n                      e p , eo n ta a . dn a k ” .z te. ”e” redocnE txeT FM oL uP   \n                             rier                                                   \n                                egamI mroNreyaLadA latent 1 tacno&C .joV rKQ P      \n                                              .                                     \n                                                nttA                                \n                                                  1    elbmessA FF&mroNreyaL        \n                  Pointe hd                                                         \n                   o                                                                \n                    rr so eof DenseSample secnatsnI 1234 mroNreyaLadA 1234 2 tacno&C .jV oK rQ P\n                                              .                                     \n                                                  2                                 \n                                                   Assemble-Attn                    \n                                                          1234 FF&mroNreyaL 1234    \n                  Historical monument Legend Concatenation Element-wise Addition Frozen Trainable(LoRA)\n               Figure2: TheproposedInstanceAssemblepipeline. Variouslayoutconditionsareprocessedbythe\n               LayoutEncodertoobtaininstancetokens,whichguidetheimagegenerationviaAssemble-MMDiT.\n               InAssemble-MMDiT,theinstancetokensinteractwithimagetokensthroughtheAssembling-Attn.\n               signals. Recently, researches [6, 15, 49, 4, 34] have used the Multimodal Diffusion Transformer\n               (MMDiT)architecture,markingasignificantimprovement.                  \n               Layout-to-ImageGenerationLayout-to-Imagegenerationenablesimagegenerationunderlayout\n               conditions,whichisdefinedasspatialpositionswithtextualdescriptions. Existingapproachescanbe\n               broadlycategorizedintotraining-freeandtraining-basedparadigms.       \n               Training-freemethodsleveragepretrainedtext-to-imagediffusionmodelswithoutadditionaltrain-\n               ing. Acommonstrategyinvolvesgradient-basedguidanceduringdenoisingtoalignwithlayout\n               conditions[55,12,41,13,18,50]. Also,therearemethodsthatdirectlymanipulatelatentsthrough\n               well-defined replacing or merging operations [8, 48, 2] or enforce layout alignment via spatially\n               constrainedattentionmasks[5,20]. GrounDiT[27]exploitssemanticsharinginDiT:acroppednoisy\n               patchandthefullimagebecomesemanticcloneswhendenoisedtogether,enablinglayout-to-image\n               generation by jointly denoising instance regions with their corresponding image context. Other\n               approachesgenerateeachinstanceseparatelyandemployinpaintingtechniquestocomposethefinal\n               image[37]. However,thesemethodsdemonstratedecentperformanceprimarilyonsimpleandsparse\n               layouts,whiletheiraccuracydecreasesinmorecomplexlayouts. Somemethodsrequirehyperparam-\n               etertuningspecifictodifferentlayoutconditions,reducingtheiradaptability. Furthermore,additional\n               gradient computations or latent manipulations result in slow inference speed, thus limiting their\n               applicabilityinreal-worldscenarios.                                  \n               Training-basedmethodsexplicitlyincorporatelayoutconditioningthrougharchitecturalmodifica-\n               tions. Mostapproachesinjectspatialconstraintsviacross-attention[63,57,36,25,59,16,54]or\n               self-attention[10,53,28].Someworksproposededicatedlayoutencodingmodules[9,64,65,58,62]\n               oradoptatwo-stagepipelinethatgeneratesimagesafterpredictingadepthmapwithlayoutcondi-\n               tions[14,66]. Otherworksleverageautoregressiveimagegenerationmodels[19]. Thesemethods\n               sufferhighcomputationalcostsduetoexcessiveparameters.                \n               3 Method                                                             \n               PreliminariesRecentstate-of-the-arttext-to-imagemodelssuchasSD3[15]andFlux[4]adoptthe\n               MultimodalDiffusionTransformer(MMDiT)asthebackboneforgeneration. Unliketraditional\n               UNet-basedcross-attentionapproaches,MMDiTstreatimageandtextmodalitiesinasymmetric\n               manner, which leads to stronger prompt alignment and controllability. These models are trained\n               under the flow matching framework [33], which formulates generation as learning a continuous\n               velocityfieldthattransportsnoisetodata. GivenacleanlatentxandGaussiannoiseϵ∼N(0,I),an\n                                          3                                         \n\n[表格内容]\n2. Instances - Image\nImage\nAssemble-\nInstances MMDiT\n1 2 3 4\n\n |  |  | \n\n |  | \n\n | \n | \n\n |  |  | \n\n |  | \nInstances |  | \n1 | 2 | 3 4\n\n |  | \n\n2 | 3 | 4\n\nnttA | 2\n\nhorse\nHistorical monument |",
        "char_count": 12615,
        "has_tables": true
      },
      {
        "page": 4,
        "content": "interpolatedlatentisdefinedas                                        \n                                 z =(1−t)x+tϵ, t∈[0,1].             (1)             \n                                  t                                                 \n               The training objective minimizes the squared error between the predicted velocity and the target\n               velocity(ϵ−x):                                                       \n                                        h              i                            \n                            L  =E        ∥v (z ,t,y)−(ϵ−x)∥2 ,      (2)             \n                             FM  ϵ∼N(0,I),x,t θ t     2                             \n               wherev isimplementedwithanMMDiTbackbone.                             \n                   θ                                                                \n               Problem Definition Layout-to-Image generation aims to synthesize images with precise control\n               throughaglobalpromptpandinstance-wiselayoutconditionsL. ThelayoutconditionscompriseN\n               instances{l }N ,whereeachinstancel isdefinedbyitsspatialpositionb andcontentc :\n                     i i=1           i                  i       i                   \n                               L={l ,...,l }, wherel =(c ,b ).      (3)             \n                                   1   N       i  i i                               \n               Inourframework,spatialpositionsarerepresentedasboundingboxes,whileinstancecontentcanbe\n               specifiedthroughmultiplemodalities: textualinstancecontentandadditionalvisualinstancecontent,\n               includingreferenceimages,depthmapsandedgemaps.                       \n               WeproposeInstanceAssemble,aframeworkwithaLayoutEncodertoencodethelayoutconditions\n               andAssemble-MMDiTtoeffectivelyintegratetheencodedlayoutconditionswithimagefeatures.\n               3.1 LayoutEncoder                                                    \n               WeuseaLayoutEncoder(Fig.2left-bottompanel)toencodeeachinstancel ,andthetokensare\n                                                          i                         \n               denotedashL =[hl1,...,hlN]whichrepresentsthelayoutinformationofeachinstance. Giventhe\n               spatialpositionoftheinstance(boundingbox),wefirstenhancethespatialrepresentationthrough\n               DenseSample. Givenaboundingboxb =(x ,y ,w,h)∈[0,1]4withtop-leftcoordinates(x ,y )\n                                    i   1 1                        1 1              \n               andsize(w,h),wegenerateK2uniformlyspacedpoints:                      \n                           (cid:26)(cid:18) (cid:19)(cid:12) (cid:27)               \n                                   w       h (cid:12)                               \n                        P i = x 1+k x· K,y 1+k y· K (cid:12) (cid:12)k x,k y ∈{0,...,K−1} (4)\n               Then,followingGLIGEN[28],wecomputethetextualinstancetokensas:        \n                                 hi =MLP([τ(c ),Fourier(P )]),      (5)             \n                                 l        i      i                                  \n               where τ represents the text encoder, Fourier(·) denotes Fourier embedding [51], [·, ·] denotes\n               concatenationalongthefeaturedimension,andMLPisamulti-layerperception.\n               Additionally,wecanuseadditionalvisualinstancecontenttobetterimproveperformance. Given\n               thevisualinstancecontent,wefirstextractfeaturesusingtheVAEencoderofthebasemodel,then\n               projectthemtotheunifiedinstancetokenspacethroughaMLP:                \n                                    hi =MLP(VAE(c )).               (6)             \n                                    l          i                                    \n               3.2 Assemble-MMDiT                                                   \n               Weobservethatapplyingattentionbetweenallimagetokensandinstancetokensresultsinsuboptimal\n               performanceundercomplexlayoutconditions(e.g.,overlapping,tinyobjects). Toaddressthis,we\n               introduce Assemble-MMDiT (Fig. 2, right-bottom panel), which enhances the location of each\n               instancewhilemaintainingcompositionalcoherencewithotherinstances. Ourmethodprocesses\n               eachinstanceindependentlythroughattentionmoduleswithitsassociatedimagetokens,followedby\n               weightedfeatureassembling.                                           \n               Formally,givenimagetokensh∈RC×W×H (whereC denotesthelatentchannelsizeand[W,H]\n               thelatentsize)andinstancetokenshl ∈ RC×N,weapplyAdaLayerNorm[56],followedbyour\n               proposedAssembling-Attn,asshowninFig.2(right-bottompanel). Wecroptheimagetokenshz by\n               thebboxb ofeachinstanceandgethz =hz[b ]∈RC×w×h. Then,weprojectthecroppedimage\n                     i              li   i                                          \n               tokenshz                                                             \n                    li                                                              \n                     andtheircorrespondinginstancetokensl iintoqueries(Qzli,Qli),keys(Kzli,Kli),and\n               values(Vzli,Vli),andthenapplyattention:                              \n                          hz li′ ,hl i′ =Attention(cid:0) [Qzli,Qli],[Kzli,Kli],[Vzli,Vli](cid:1) . (7)\n                                          4",
        "char_count": 6037,
        "has_tables": false
      },
      {
        "page": 5,
        "content": "where[·,·]denotesconcatenationalongthetokendimension. Theupdatedtokensareassembled\n               across instances. Let M ∈ NW×H represent the instance density map, calculating the counts of\n               instances.                                                           \n                     Theassembledimagetokenshz′ andinstancetokenshl′                \n                                                    arecomputedas:                  \n                                      N                                             \n                       hz′ :hz′                                                     \n                            [:,i,j]=                                                \n                                   1  X hz′                                         \n                                          [:,i,j], wherei∈[0,W −1],j ∈[0,H −1]      \n                                 M[i,j]  lk                         (8)             \n                                      k=1                                           \n                       hl′ :hl′ [:,k] =hl k′ .                                      \n               As illustrated in Fig. 3, the top row demonstrates that              \n               ourassemblingmechanismensuresinstancetokensattend                    \n               o l                                                                  \n               e                                                                    \n               c                                                                    \n               ee                                                                   \n               xf                                                                   \n                on f                                                                \n                f                                                                   \n                prl t ey                                                            \n                 r                                                                  \n                 lci                                                                \n                 e                                                                  \n                 int                                                                \n                  ct                                                                \n                  co                                                                \n                  i iv                                                              \n                  tb tr                                                             \n                   e                                                                \n                   sl le a                                                          \n                    al                                                              \n                    pl yc ye akv                                                    \n                     otg                                                            \n                     i.a uaun T tlit                                                \n                       d                                                            \n                       ch pi                                                        \n                        e                                                           \n                        oem                                                         \n                        os                                                          \n                        nsma                                                        \n                         tg                                                         \n                         i                                                          \n                         rg                                                         \n                          tl                                                        \n                          oi ie od                                                  \n                          o                                                         \n                          lbd nr (e al bseg                                         \n                            l                                                       \n                            .                                                       \n                            oi pr tIo to nrn ow os mcm, orw re                      \n                                p                                                   \n                                n                                                   \n                                ov th                                               \n                                 t                                                  \n                                 wree                                               \n                                 t                                                  \n                                 aoar )se l                                         \n                                  k                                                 \n                                   ts r,eu et                                       \n                                    n                                               \n                                    g                                               \n                                    sn h                                            \n                                    us                                              \n                                    er ae lntt tl                                   \n                                      o                                             \n                                      sea th rt                                     \n                                       if                                           \n                                       ae ne                                        \n                                       o                                            \n                                       td                                           \n                                        c                                           \n                                        i                                           \n                                        lm                                          \n                                        o                                           \n                                        or ue                                       \n                                         n                                          \n                                         ce sg ac woi lho                           \n                                           in                                       \n                                           za in                                    \n                                           t                                        \n                                            an                                      \n                                            t                                       \n                                            hs                                      \n                                            h                                       \n                                            ti                                      \n                                             o                                      \n                                             ia s                                   \n                                             e                                      \n                                             oumr                                   \n                                             i                                      \n                                              ne                                    \n                                              r                                     \n                                              t                                     \n                                                tuoyal                              \n                                                /w                                  \n                                                 tpmos re pc n la at bs on lI       \n                                                 G                                  \n               e inrr co or ns s( is\" tB er ni cti is eh sS (\"h do or gth \"a mir\" isi sn inw g)r .onglocation)orsemantic tuoyal\n                                                o/w                                 \n                                                      British ShorthairAmerican robin Maltipoo dog water\n               Furthermore,topreservethegenerationcapabilityofthe Figure3: (Top)instance-imageattention\n               originalmodelandmitigateconditionalconflictsbetween mapw/layout. (Middle)globalprompt-\n               global prompt and layout conditions, we employ a cas- imageattentionmapw/layout.(Bottom)\n               cadedmechanismasshowninFig.2(right-abovepanel). globalprompt-imageattentionmapw/o\n               In our design, the global text prompt and image latents layout.      \n               arepassedthroughoriginalMMDiTfirst,thentheimage                      \n               tokens along with instance tokens are processed by our               \n               Assemble-MMDiTmodule. Thefirststepcapturesglobalcontextandensuresgenerationquality,\n               whilethesecondstepensuresinstancelayoutalignment. Besides,wetrainAssemble-MMDiTwith\n               LoRA,significantlyreducingboththetrainingcostandinferencecosts.      \n                      ‘cRoanldld iye                                                \n                       t                                                            \n                       t iCe oac nrt                                                \n                        :’                                                          \n                                                 SAMbox co‘nTdoiwteiro’n: detect    \n                                    detect        co‘nWdinitdioown’:                \n                               ‘Blaccko nsdhiotlidoenr: bag’ detect                 \n                     (a) Rally Car? (b) Black shoulder bag? (c) Window? (d) Tower?  \n                       CropVQA: Yes –> 1 CropVQA: No –> 0 SAMIoU: 0.77 BinaryIoU: 1 \n                       DetectIoU: 0.60 DetectIoU: 0.54 DetectIoU: 0.00 DetectIoU: 0.81\n               Figure4: Failurecasesofothermetrics. (a)falseacceptanceinCropVQA,(b)falserejectionin\n               CropVQA,(c)localizationerrorinSAMIoU,and(d)discontinuousinBinaryIoU. \n               3.3 Benchmark: DenseLayoutandLayoutGroundingScore                    \n               TheLayout-to-Imagetaskaimstogenerateimagesthatalignpreciselywithprovidedlayouts,eval-\n               uatingbothspatialaccuracyandsemanticconsistency(e.g.,color,texture,andshape,ifprovided).\n               Theexistingmetrics(AP/AR)forobjectdetection[10,28,55,53]aresuboptimal. Theyassumea\n               fixedcategorysetandrelyoninappropriateprecision/recallforbinarylayoutoutcomes. VLM-based\n               croppedVQAmethods[61]sufferfalseacceptance(Fig.4(a))andfalserejection(Fig.4(b)). While\n               spatial-onlymetricslikeSAMIoU[11]ignoreappearanceconsistency(Fig.4(c)),GroundingDINO-\n               based[35]binaryIoUthresholds[64,54]failtocapturecontinuouslayoutprecision(Fig.4(d)). Thus,\n               weproposeLayoutGroundingScore(LGS),whichintegratesbothspatialaccuracyandsemantic\n               accuracy:                                                            \n                  1. SpatialAccuracy(DetectIoU):wedetectallinstancesviaanoff-the-shelfdetector[35],\n               computetheIoUagainstconditionbbox, andreporttheglobalmeanIoUacrossallinstancesfor\n               equalweighting.                                                      \n                  2. SemanticAccuracy: forinstanceswithIoU>0.5,wecropthepredictedregionandassessthe\n               semanticaccuracybyitsattributeconsistency(color,texture,shape)viaVLM-basedVQA[60].\n               LGSsupportsopen-setevaluation,usesDetectIoUtoevaluatespatialaccuracyanddecouplesthe\n               spatial and semantic check to avoid CropVQA [61] failures (shown in Fig. 4). Furthermore, we\n                                          5                                         \n\n[表格内容]\n | tuoyal tpmos re pc n la at bs on lI\n/w\nG\ntuoyal\no/w\nBritish ShorthairAmerican robin Maltipoo dog water\nFigure3: (Top)instance-imageattention\nmapw/layout. (Middle)globalprompt-\nmageattentionmapw/layout.(Bottom)\nglobalprompt-imageattentionmapw/o\nayout.\ncontextandensuresgenerationquality,\nsides,wetrainAssemble-MMDiTwith\nrencecosts.\nSAMbox detect\nco‘nTdoiwteiro’n:\nco‘nWdinitdioown’:\ndetect\n(d) Tower?\n0.77 BinaryIoU: 1\nU: 0.00 DetectIoU: 0.81\nnceinCropVQA,(b)falserejectionin\n‘cRoanldld iye t iCe oac nrt co‘nTdoiwteiro’n:\nt :’\ndetect co‘nWdinitdioown’:\n‘Blaccko nsdhiotlidoenr: bag’ detect\n(a) Rally Car? (b) Black shoulder bag? (c) Window? (d) Tower?\nCropVQA: Yes –> 1 CropVQA: No –> 0 SAMIoU: 0.77 BinaryIoU: 1\nDetectIoU: 0.60 DetectIoU: 0.54 DetectIoU: 0.00 DetectIoU: 0.81\nFailurecasesofothermetrics. (a)falseacceptanceinCropVQA,(b)falserejectionin\n,(c)localizationerrorinSAMIoU,and(d)discontinuousinBinaryIoU.\nchmark: DenseLayoutandLayoutGroundingScore\nut-to-Imagetaskaimstogenerateimagesthatalignpreciselywithprovidedlayouts,eval\nhspatialaccuracyandsemanticconsistency(e.g.,color,texture,andshape,ifprovided)\nngmetrics(AP/AR)forobjectdetection[10,28,55,53]aresuboptimal. Theyassumea\ngorysetandrelyoninappropriateprecision/recallforbinarylayoutoutcomes. VLM-based\nQAmethods[61]sufferfalseacceptance(Fig.4(a))andfalserejection(Fig.4(b)). While\nlymetricslikeSAMIoU[11]ignoreappearanceconsistency(Fig.4(c)),GroundingDINO\nbinaryIoUthresholds[64,54]failtocapturecontinuouslayoutprecision(Fig.4(d)). Thus\neLayoutGroundingScore(LGS),whichintegratesbothspatialaccuracyandsemantic\npatialAccuracy(DetectIoU):wedetectallinstancesviaanoff-the-shelfdetector[35]\nheIoUagainstconditionbbox, andreporttheglobalmeanIoUacrossallinstancesfo\nghting.\nmanticAccuracy: forinstanceswithIoU>0.5,wecropthepredictedregionandassessthe\naccuracybyitsattributeconsistency(color,texture,shape)viaVLM-basedVQA[60].\nortsopen-setevaluation,usesDetectIoUtoevaluatespatialaccuracyanddecouplesthe\nd semantic check to avoid CropVQA [61] failures (shown in Fig. 4). Furthermore, we | co‘nTdoiwteiro’n:\nco‘nWdinitdioown’:\ndetect\n(d) Tower?\n0.77 BinaryIoU: 1\nU: 0.00 DetectIoU: 0.81\nnceinCropVQA,(b)falserejectionin\n\n | \nco‘nWdinitdioown’: | \n | detect\n | \n\n\n\n\n | \n | \n\n | detect\n‘Blaccko nsdhiotlidoenr: bag’ | \n |",
        "char_count": 15392,
        "has_tables": true
      },
      {
        "page": 6,
        "content": "Table1: QuantitativecomparisonbetweenourSD3-basedInstanceAssembleandotherL2I\n               methodsonLayoutSAM-Eval. ⋆ TheCropVQAscoreisproposedinCreatilayout[61]andthe\n               scoreofInstanceDiff,MIGCandCreatiLayoutisborrowedfromCreatiLayout[61].\n                                    CropVQA⋆  LayoutGroundingScore GlobalQuality    \n                  LayoutSAM-Eval                                                    \n                                spatial↑color↑ texture↑shape↑ mIoU↑ color↑ texture↑shape↑ VQA↑ Pick↑ CLIP↑\n                  RealImages(UpperBound) 98.95 98.45 98.90 98.80 88.85 88.07 88.71 88.62\n                  InstanceDiff(SD1.5) 87.99 69.16 72.78 71.08 78.16 63.14 66.82 65.86 86.42 21.16 11.73\n                  MIGC(SD1.4)   85.66 66.97 71.24 69.06 62.87 50.70 52.99 51.77 88.97 20.69 12.56\n                  HICO(realisticVisionV51) 90.92 69.82 73.25 71.69 70.68 53.16 55.71 54.61 86.53 21.77 9.47\n                  CreatiLayout(SD3-M) 92.67 74.45 77.21 75.93 45.82 38.44 39.68 39.24 92.74 21.71 13.82\n                  InstanceAssemble(ours)(SD3-M) 94.97 77.53 80.72 80.11 78.88 63.89 66.27 65.86 93.12 21.79 12.76\n               Table2: QuantitativecomparisonbetweenourInstanceAssembleandotherL2Imethodson\n               DenseLayout.                                                         \n                                         LayoutGroundingScore GlobalQuality         \n                       DenseLayout                                                  \n                                        mIoU↑ color↑ texture↑shape↑ VQA↑ Pick↑ CLIP↑\n                       RealImages(UpperBound) 92.35 76.52 80.78 79.78               \n                       InstanceDiff(SD1.5) 47.31 29.48 33.36 32.43 88.79 20.87 11.73\n                       MIGC(SD1.4)      34.39 22.10 23.99 23.45 91.18 20.74 12.81   \n                       HICO(realisticVisionV51) 22.42 10.52 11.69 11.46 74.42 20.51 8.16\n                       CreatiLayout(SD3-Medium) 15.54 11.69 12.34 12.17 93.42 21.88 12.89\n                       InstanceAssemble(ours)(SD3-Medium) 52.07 33.77 36.21 35.81 93.54 21.68 12.58\n                       Regional-Flux(Flux.1-Dev) 14.06 11.34 11.91 11.84 92.94 22.67 10.66\n                       RAG(Flux.1-Dev)  17.23 14.22 14.62 14.55 92.16 22.28 11.01   \n                       InstanceAssemble(ours)(Flux.1-Dev) 43.42 27.60 29.50 29.14 93.36 21.98 11.38\n                       InstanceAssemble(ours)(Flux.1-Schnell) 45.33 27.73 30.06 29.62 93.52 21.72 10.78\n               introduceDenseLayout,adenseevaluationdatasetforL2I,whichconsistsof5kimageswith90k\n               instances(18.1perimage). TheimagesinDenseLayoutaregeneratedbyFlux.1-Dev, taggedby\n               RAM++[24],detectedbyGroundingDINO[35],recaptionedbyQwen2.5-VL[43],andfilteredto\n               retainthosewith≥15instances,thusprovidingdenselayoutconditions.      \n               3.4 TrainingandInference                                             \n               Duringtraining,wefreezetheparametersofthebasemodelandonlyupdatetheproposedLayout\n               Encoder and Assemble-MMDiT module. We denote the adding parameters by θ′. The training\n               objectiveisgivenby                                                   \n                          L=E ϵ∼N(0,I),x,t,p,Lh(cid:13) (cid:13)v {θ,θ′}(cid:0) z t,t,p,L(cid:1) −(ϵ−x)(cid:13) (cid:13)2 2i , (9)\n               wherez =(1−t)x+tϵ. Duringinference,layout-conditioneddenoisingisappliedduringthefirst\n                   t                                                                \n               30%ofdiffusionsteps,asthelayoutprimarilyformsinearlystages[28,64].   \n               4 Experiments                                                        \n               4.1 ExperimentalSetup                                                \n               ImplementationDetailsThetextual-onlyInstanceAssembleistrainedonSD3-Medium[15]and\n               Flux.1-Dev[4]andtheversionwithadditionalvisualinstancecontentisonlytrainedonSD3-Medium.\n               WefreezethepretrainedMMDiTbackboneandonlyadapttheLayoutEncoderandLoRAmodules\n               ofAssemble-MMDiT.Assemble-MMDiTisinitializedfrompretrainedweights, andLoRAwith\n               rank=4isapplied. IntheSD3-basedmodelallAssemble-MMDiTblocksareadapted,whileinthe\n               Flux-basedmodelweadapteightblocks(sevendouble-blocksandonesingleblock)duetoresource\n               constraints. Duringinference,theLoRA-basedAssemble-MMDiTisactivatedforthefirst30%of\n               denoisingsteps,whiletheglobalprompt–imagephaseusesthefrozenbackbone. Thisdesignyields\n               71M(SD3-M)and102M(Flux.1-Dev)additionalparametersforthetextual-onlysetting;thevariant\n               withadditionalvisualinstancecontent(SD3-M)introduces85Mparameters. Allmodelsaretrained\n               onLayoutSAM[61]at1024×1024withProdigy,for380Kiterations(batchsize2)onSD3-Mand\n                                          6",
        "char_count": 5579,
        "has_tables": false
      },
      {
        "page": 7,
        "content": "Table4: Parameteradditionandtimeefficiencyundersparseanddenselayoutconditions. We\n               evaluatedon10%oftheLayoutSAM-EvalandDenseLayoutdatasetsat1024×1024resolution. ⋆ are\n               optimizedfor512×512resolution,sotheirresultsarereportedatthisscale.  \n                                     ParameterAddition TimeEfficiency(s)(relativeruntimeincrease(%))\n                                   (relativeparameteraddition(%)) SparseLayout DenseLayout\n                 InstanceDiff⋆(SD1.5)     369M(43%) 14.37(+771%) 44.81(+2754%)      \n                 MIGC(SD1.4)              57M(6.64%) 14.41(+25.4%) 21.58(+87.5%)    \n                 HICO⋆(realisticVisionV51) 361M(33.9%) 4.11(+92.9%) 9.93(+320%)     \n                 CreatiLayout(SD3-M)      1.2B(64.0%) 4.37(+14.4%) 4.42(+14.8%)     \n                 Regional-Flux(Flux.1-Dev)     - 15.29(+113%) 37.47(+418%)          \n                 RAG(Flux.1-Dev)               - 15.69(+119%) 21.14(+192%)          \n                 InstanceAssemble(ours)(SD3-M) 71M(3.46%) 7.19(+88.2%) 13.38(+248%) \n                 InstanceAssemble(ours)(Flux.1-Dev) 102M(0.84%) 8.21(+14.3%) 10.28(+41.9%)\n                 InstanceAssemble(ours)(Flux.1-Schnell) 102M(0.84%) 1.41(+8.46%) 1.70(+28.8%)\n               300Kiterations(batchsize1)onFlux.1-Dev,using8×H800GPUs(7daysforSD3-M;5daysfor\n               Flux.1-Dev).                                                         \n               EvaluationDatasetWeuseLayoutSAM-Eval[61]toevaluateperformanceonfine-grainedopen-set\n               sparseL2Idataset,containing5kimagesand19kinstancesintotal(3.8instancesperimage). To\n               assess performance on fine-grained open-set dense L2I evaluation dataset, we use the proposed\n               DenseLayout, which consists of 5kimages and 90k instancesin total (18.1 instancesper image).\n               Followingconventionalpractice,wealsoevaluateoncoarse-grainedclose-setL2Ievaluationdataset\n               COCO[31]. WecombineCOCO-StuffandCOCO-InstanceannotationstocreateourCOCO-Layout\n               evaluationdataset,containing5kimagesand57kinstancesintotal(11.5instancesperimage).\n               EvaluationMetricWeevaluatetheaccuracyofL2IgenerationusingourproposedLGSmetricalong\n               withCropVQAproposedbyCreatiLayout[61],measuringspatialandsemanticaccuracy. Wealso\n               employmultipleestablishedmetricstomeasureoverallimagequalityandglobalpromptalignment,\n               includingVQAScore[32],PickScore[26]andCLIPScore[21].                 \n               4.2 EvaluationonL2IwithTextual-OnlyContent                           \n               Fine-GrainedOpen-SetSparseL2IGenerationTab.1presentsthequantitativeresultsofInstance-\n               Assemble on LayoutSAM-Eval [61], reporting results using our proposed LGS, CropVQA [61]\n               andglobalqualitymetrics. OurproposedInstanceAssemblenotonlyachievesSOTAinspatialand\n               semanticaccuracyofeachinstance,butalsodemonstratessuperiorglobalquality.\n               Fine-GrainedOpen-SetDenseL2IGener-                                   \n               ation Tab. 2 presents results on DenseLay- Table 3: Comparison between our SD3-based\n               out. WiththesameSD3-Mediumbackbone, InstanceAssemble and other L2I methods on\n               InstanceAssemble significantly outperforms COCO-Layout. SinceCOCOdon\"thavedetailed\n               CreatiLayout(mIoU:52.07vs. 15.54)while descriptionforeachinstance,wecannotevaluatethe\n               maintaining comparable global quality. On attributeaccuracyandonlyreportthespatialaccu-\n               Flux.1,italsoyieldslargegainsoverRegional- racy-mIoU.                \n               FluxandRAG(e.g.,mIoU:43.42vs.17.23for                                \n               RAG),showingthatourcascadedAssemble-      LGS GlobalQuality          \n                                          COCO-Layout                               \n               Attndesigngeneralizeswellacrossbackbones. mIoU↑ VQA↑ Pick↑ CLIP↑     \n               ComparedtoearlierUNet-basedapproaches RealImages(UpperBound) 49.14   \n               suchasInstanceDiffandMIGC,ourmethod                                  \n                                          InstanceDiff(SD1.5) 30.39 75.77 20.75 24.41\n               achieveshigherspatialandsemanticaccuracy MIGC(SD1.4) 27.36 70.32 20.20 23.58\n               (mIoU:52.07vs.47.31)withoutsacrificingre- HICO(realisticVisionV51) 18.88 50.61 20.38 20.72\n                                          CreatiLayout(SD3-M) 7.12 87.79 21.22 25.59\n               alism. Overall,InstanceAssembleestablishes                           \n                                          InstanceAssemble(ours)(SD3-M) 27.85 89.06 21.58 25.68\n               consistentimprovementsinlayoutalignment                              \n               whileensuringhighimagequalityunderchal-                              \n               lengingdenselayouts.                                                 \n               Coarse-GrainedClosed-SetL2IGenerationTab.3presentsthequantitativeresultofInstance-\n               AssembleonCOCO-Layout. OurproposedInstanceAssemblesurpassespreviousmethodsinoverall\n               imagequalitybutlagsslightlybehindInstanceDiff[53]inlayoutprecision. Weattributethisgapto:\n                                          7",
        "char_count": 5876,
        "has_tables": false
      },
      {
        "page": 8,
        "content": "Layout InstanceDiff MIGC HICO CreatiLayout Regional-Flux RAG Ours  \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                    Figure5: QualitativecomparisonofInstanceAssemblewithothermethods.\n                                                                                    \n                                                                                    \n               (i)InstanceDiff’sfine-grainedCOCOtrainingdatawithper-entityattributeannotations,and(ii)its\n               entity-wisegenerationstrategy,whichimprovesprecisionatsignificantcomputationalcost(Tab.4).\n               QualitativeComparisonThecomparativeresultsinFig.5                    \n               demonstrate that our proposed InstanceAssemble method                \n                                                                  text              \n               achieves superior spatial precision and instance-caption 4te 6.x 1ture text+image\n                                                                  text+depth        \n               alignment compared to baseline methods. For example, 4 3 . 7color text+edge\n                                                        34.6                        \n               inthethirdrow,bothInstanceDiff[53]andMIGC[64]gen- 32.8               \n               eratemorethanoneshoes; HICO[9]failstogeneratethe shape 23.0 21.9     \n                                                  45.8                              \n               specifiedNewBalanceshoe;Regional-Flux[5]doesnotad- 34.4 22.9 11.5 10.9\n               here to the layout conditions; and the shoe generated by 11.5 25.2 35.6 46.0 5 6 m.4 IoU\n               RAG[8]isnotproperlyfusedwiththebackground. Incon-                    \n                                                        84.9                        \n                                                           10.7                     \n               trast,ourmethodgeneratesthecorrectinstance,accurately 87.9 20.0      \n                                                    90.8    11.7                    \n               placedandseamlesslyintegratedwiththescene. VQA                       \n                                                  93.7   20.7 12.7                  \n               Time Efficiency and Parameter Addition We compare 21.4 13.7          \n                                                               CLIP                 \n               timeefficiencyandparameteradditionwithotherL2Imeth-                  \n                                                       Pick2 2 .1                   \n               ods,asshowninTab.4. OurmethodachievesSOTAperfor-                     \n               manceonlayoutalignmentwithacceptabletimeefficiency Figure6: QuantitativeresultsofIn-\n               andminimalparameteraddition.      stanceAssemble with additional vi- \n                                                 sualinstancecontent.               \n               4.3 EvaluationonL2IwithAdditionalVisualContent                       \n               Weevaluatethreeadditionalvisualinstancecontent: image,               \n               depth,andedge(seeFig.6). Unsurprisingly,usingimage                   \n               asadditionalinstancecontentyieldsthebestperformance, asitprovidesrichvisualinformation.\n               Although depth and edge capture texture and shape features, their performance remains inferior\n               comparedtoimageinstancecontent. Nevertheless,visualmodalitiesoutperformtextual-onlyinstance\n                                          8                                         \n\n[表格内容]",
        "char_count": 5956,
        "has_tables": true
      },
      {
        "page": 9,
        "content": "Table5: AblationstudyonourproposedcomponentsonDenseLayout. \"Assemble\"referstothe\n               presenceoftheAssemble-Attnmodule(architecturaldesign). \"Cascaded\"indicatestheinteraction\n               order: (✔)meansglobalprompt–imageinteractionfollowedbyinstance–imageinteraction(cascaded\n               structure),while(✘)meansbothareappliedinparallel. \"LoRA\"specifiesthetrainingstrategyforthe\n               Assemble-MMDiTmodule: (✔)indicatestrainingwithLoRA,while(✘)indicatesfullfine-tuning.\n               \"DenseSample\"denoteswhethertheDenseSamplespatialencodingisused.      \n                                               LayoutGroundingScore                 \n                        Assemble Cascaded LoRA DenseSample                          \n                                              mIoU↑ color↑ texture↑shape↑ VQA↑      \n                         ✘     ✘    ✘    ✘    11.69 9.16 9.68 9.56 93.75            \n                         ✔     ✘    ✘    ✘    43.98 24.19 26.95 26.75 84.57         \n                         ✔     ✔    ✘    ✘    45.96 29.61 31.50 31.09 92.71         \n                         ✔     ✔   ✔     ✘    51.28 32.68 34.94 34.58 93.33         \n                         ✔     ✔   ✔     ✔    52.07 33.77 36.21 35.81 93.54         \n               content. Qualitativecomparisons(Fig.7)furtherdemonstratethatvisualinstancecontentleadsto\n               superiortextureandshapealignmentcomparedtotext.                      \n                   Layout     Text+Image Text+Depth  Text+Edge Text                 \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                Figure7: QualitativeresultsofInstanceAssemblewithadditionalvisualinstancecontent.\n                                                                                    \n                                                                                    \n               4.4 AblationStudy                                                    \n                                                                                    \n               We evaluated the contribution of each proposed component on SD3-M based InstanceAssemble\n               inTab.5. Thebasemodel(SD3-Mediumwithoutadditionalmodules)yieldsaverylowLayout\n               Grounding Score, indicating poor layout and content control when instance information is not\n               explicitlymodeled. TheintroductionofAssemble-Attnmoduleelevatesspatialaccuracy(mIoUto\n               43.98)andboostssemanticmetrics(color/texture/shapeto24.19/26.95/26.75). Thecascadeddesign\n               (✔:prompt–imagefollowedbyinstance–image;✘:parallel)resolvesglobalqualitydegradationwhile\n               maintaininglayoutalignment.UsingLoRAtotrainAssemble-MMDiTimprovesperformancefortwo\n               reasons: (1)itretainsthebasemodel’scapabilitiescomparedtothefullyfine-tunedversion,and(2)\n               itenableseffectivelayoutcontrolwithfarfewertrainableparametersbyintroducingonlylightweight\n               low-rankmatricesonattentionprojections. Finally,DenseSamplefurtherenhancesspatialaccuracy,\n               instancesemanticaccuracyandimagequality. Together,theserefinementsprogressivelycollectively\n               optimizelayouttoimagemodelingwithoutcompromisinggenerationability.   \n               4.5 Applications                                                     \n               WedemonstratethatInstanceAssembleisversatileandapplicabletovarioustasks. Itseamlessly\n               integrateswithdomain-specificLoRAmodulesformulti-domainstyletransferwhilemaintaining\n               layoutconsistency,asshowninFig.8. OurproposedInstanceAssemblecancooperatewithdistilled\n               modelssuchasFlux.1-Schnell[4],asillustratedinTab.2,achievinggeometriclayoutcontroland\n               detailedsynthesis. Ourapproachdemonstratesbothstyleadaptabilityandcomputationalefficiency,\n               makingitwell-suitedforcontrollablegenerativedesignapplications.      \n                                          9                                         \n                                                                                    \n                                                                                    \n\n[表格内容]",
        "char_count": 5516,
        "has_tables": true
      },
      {
        "page": 10,
        "content": "D3etUC                                                              \n                                                                                    \n                                                                                    \n                gnitniap                                                            \n                                                                                    \n                liO                                                                 \n                                                                                    \n                                                                                    \n                ilbihG                                                              \n                                                                                    \n                                                                                    \n               Figure8: TheadaptionofCute3D[52]/OilPainting[39]/Ghibli[38]LoRAwithourmethods.\n               The proposed InstanceAssemble successfully adapts diverse style lora and maintaining superior\n               layoutalignment.                                                     \n                                                                                    \n               5 Conclusion                                                         \n                                                                                    \n               We present InstanceAssemble, a novel approach for Layout-to-Image generation. Our method\n               achievesstate-of-the-artlayoutalignmentwhilemaintaininghigh-qualitygenerationcapabilitiesof\n               DiT-basedarchitectures. WevalidateInstanceAssembleacrosstextualinstancecontentandadditional\n               visualinstancecontent,demonstratingitsversatilityandrobustness. Ourlayoutcontrolschemealso\n               successfullyadaptsdiversestyleLoRAswhilemaintainingsuperiorlayoutalignment,demonstrating\n               cross-domaingeneralizationcapability. Futhermore,weintroduceLayoutGroundingScoremetric\n               andaDenseLayoutevaluationdatasettovalidateperformanceundercomplexlayoutconditions.\n               LimitationsandFutureWorkWhileourworkadvancescontrollablegenerationbyunifyingprecise\n               layoutcontrolwiththeexpressivepowerofdiffusionmodels,severallimitationsremain. First,our\n               designcurrentlyrequiressequentialAssemble-MMDiTcalls,whichmayincurinefficiency;exploring\n               parallelizationstrategiesisanimportantdirection. Second,althoughourapproachiseffectiveundera\n               widerangeoflayouts,imagefidelitycandegradeinextremelydenseorhighlycomplexcases.\n               BroaderImpactsInstanceAssembleexpandsthefrontierofstructuredvisualsynthesisbyproviding\n               fine-grainedlayoutcontrolandhigh-qualitymultimodalgeneration. However,itspowerfulgenerative\n               capabilities may also introduce risks. In particular, malicious use could enable the creation of\n               misleadingordeceptivelayouts,exacerbatingthespreadofdisinformation. Themodelmayalso\n               raiseprivacyconcernsifappliedtosensitivedata,andlikemanygenerativesystems,itinheritsand\n               mayamplifysocietalbiasespresentintrainingcorpora. Weencourageresponsibledeploymentand\n               continuedinvestigationintosafeguardsthatmitigatetheseriskswhileenablingbeneficialapplications\n               indesign,education,andaccessibility.                                 \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                          10",
        "char_count": 5488,
        "has_tables": false
      },
      {
        "page": 11,
        "content": "AcknowledgmentsandDisclosureofFunding                                \n               ThisresearchwassupportedbytheNationalNaturalScienceFoundationofChina(NSFC62576103,\n               62176059). ThecomputationswereconductedusingtheCFFFplatformatFudanUniversity. Partof\n               thisworkwascarriedoutduringaninternshipatXiaohongshu.                \n                                                                                    \n               References                                                           \n                                                                                    \n                [1] F.Bao,S.Nie,K.Xue,Y.Cao,C.Li,H.Su,andJ.Zhu. AllareWorthWords: AViTBackbone\n                  forDiffusionModels. InProceedingsoftheIEEE/CVFConferenceonComputerVisionand\n                  PatternRecognition,pages22669–22679.IEEE,2023.                    \n                [2] O.Bar-Tal, L.Yariv, Y.Lipman, andT.Dekel. MultiDiffusion: FusingDiffusionPathsfor\n                  Controlled Image Generation. In Proceedings of the International Conference on Machine\n                  Learning,volume202ofProceedingsofMachineLearningResearch,pages1737–1752.PMLR,\n                  2023.                                                             \n                [3] J.Betker,G.Goh,L.Jing,T.Brooks,J.Wang,L.Li,L.Ouyang,J.Zhuang,J.Lee,Y.Guo,\n                  etal. Improvingimagegenerationwithbettercaptions,2023.            \n                [4] BlackForestLabs. Flux. https://github.com/black-forest-labs/flux,2024.\n                                                                                    \n                [5] A.Chen,J.Xu,W.Zheng,G.Dai,Y.Wang,R.Zhang,H.Wang,andS.Zhang. Training-free\n                  RegionalPromptingforDiffusionTransformers,2024.URLhttps://arxiv.org/abs/2411.\n                  02395.                                                            \n                [6] J.Chen,J.Yu,C.Ge,L.Yao,E.Xie,Z.Wang,J.T.Kwok,P.Luo,H.Lu,andZ.Li. PixArt-\n                  α: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis. In\n                  ProceedingsoftheInternationalConferenceonLearningRepresentations.OpenReview.net,\n                  2024.                                                             \n                [7] M.Chen,I.Laina,andA.Vedaldi.Training-FreeLayoutControlwithCross-AttentionGuidance.\n                  InWinterConferenceonApplicationsofComputerVision,pages5331–5341.IEEE,2024.\n                [8] Z. Chen, Y. Li, H. Wang, Z. Chen, Z. Jiang, J. Li, Q. Wang, J. Yang, and Y. Tai. Region-\n                  AwareText-to-ImageGenerationviaHardBindingandSoftRefinement,2024. URLhttps:\n                  //arxiv.org/abs/2411.06558.                                       \n                [9] B.Cheng,Y.Ma,L.Wu,S.Liu,A.Ma,X.Wu,D.Leng,andY.Yin. HiCo: HierarchicalCon-\n                  trollableDiffusionModelforLayout-to-imageGeneration. InAdvancesinNeuralInformation\n                  ProcessingSystems,2024.                                           \n                                                                                    \n               [10] J.Cheng,X.Liang,X.Shi,T.He,T.Xiao,andM.Li. LayoutDiffuse: AdaptingFoundational\n                  DiffusionModelsforLayout-to-ImageGeneration,2023. URLhttps://arxiv.org/abs/\n                  2302.08908.                                                       \n               [11] J. Cheng, Z. Zhao, T. He, T. Xiao, Z. Zhang, and Y. Zhou. Rethinking The Training And\n                  EvaluationofRich-ContextLayout-to-ImageGeneration. InAdvancesinNeuralInformation\n                  ProcessingSystems,2024.                                           \n               [12] G. Couairon, M. Careil, M. Cord, S. Lathuilière, and J. Verbeek. Zero-shot spatial layout\n                  conditioningfortext-to-imagediffusionmodels. InProceedingsoftheIEEE/CVFInternational\n                  ConferenceonComputerVision,pages2174–2183.IEEE,2023.              \n               [13] O. Dahary, O. Patashnik, K. Aberman, and D. Cohen-Or. Be Yourself: Bounded Attention\n                  for Multi-subject Text-to-Image Generation. In Proceedings of the European Conference\n                  onComputerVision, volume15072ofLectureNotesinComputerScience, pages432–448.\n                  Springer,2024.                                                    \n               [14] deweiZhou,J.Xie,Z.Yang,andY.Yang. 3DIS:Depth-DrivenDecoupledImageSynthesis\n                  forUniversalMulti-InstanceGeneration. InProceedingsoftheInternationalConferenceon\n                  LearningRepresentations,2025. URLhttps://openreview.net/forum?id=MagmwodCAB.\n                                                                                    \n                                          11",
        "char_count": 5541,
        "has_tables": false
      },
      {
        "page": 12,
        "content": "[15] P.Esser,S.Kulal,A.Blattmann,R.Entezari,J.Müller,H.Saini,Y.Levi,D.Lorenz,A.Sauer,\n                  F.Boesel,D.Podell,T.Dockhorn,Z.English,andR.Rombach. ScalingRectifiedFlowTrans-\n                  formersforHigh-ResolutionImageSynthesis. InProceedingsoftheInternationalConference\n                  onMachineLearning.OpenReview.net,2024.                            \n               [16] Y. Feng, B. Gong, D. Chen, Y. Shen, Y. Liu, and J. Zhou. Ranni: Taming Text-to-Image\n                  DiffusionforAccurateInstructionFollowing. InProceedingsoftheIEEE/CVFConferenceon\n                  ComputerVisionandPatternRecognition,pages4744–4753.IEEE,2024.     \n               [17] P.Gao,L.Zhuo,D.Liu,R.Du,X.Luo,L.Qiu,Y.Zhang,C.Lin,R.Huang,S.Geng,R.Zhang,\n                  J.Xi,W.Shao,Z.Jiang,T.Yang,W.Ye,H.Tong,J.He,Y.Qiao,andH.Li. Lumina-T2X:\n                  TransformingTextintoAnyModality,Resolution,andDurationviaFlow-basedLargeDiffusion\n                  Transformers,2024. URLhttps://arxiv.org/abs/2405.05945.           \n               [18] B.Gong,S.Huang,Y.Feng,S.Zhang,Y.Li,andY.Liu. Check,Locate,Rectify: ATraining-\n                  FreeLayoutCalibrationSystemforText-to-ImageGeneration.InProceedingsoftheIEEE/CVF\n                  ConferenceonComputerVisionandPatternRecognition,pages6624–6634.IEEE,2024.\n               [19] R.He,B.Cheng,Y.Ma,Q.Jia,S.Liu,A.Ma,X.Wu,L.Wu,D.Leng,andY.Yin. PlanGen:\n                  TowardsUnifiedLayoutPlanningandImageGenerationinAuto-RegressiveVisionLanguage\n                  Models,2025. URLhttps://arxiv.org/abs/2503.10127.                 \n               [20] Y.He,R.Salakhutdinov,andJ.Z.Kolter. Localizedtext-to-imagegenerationforfreeviacross\n                  attentioncontrol,2023. URLhttps://arxiv.org/abs/2306.14636.       \n               [21] J.Hessel,A.Holtzman,M.Forbes,R.L.Bras,andY.Choi. CLIPScore: AReference-free\n                  EvaluationMetricforImageCaptioning. InProceedingsoftheEmpiricalMethodsinNatural\n                  LanguageProcessing,pages7514–7528.AssociationforComputationalLinguistics,2021.\n               [22] J.Ho,A.Jain,andP.Abbeel. DenoisingDiffusionProbabilisticModels. InNeurIPS,2020.\n                                                                                    \n               [23] E.J.Hu,Y.Shen,P.Wallis,Z.Allen-Zhu,Y.Li,S.Wang,L.Wang,andW.Chen. LoRA:Low-\n                  RankAdaptationofLargeLanguageModels. InProceedingsoftheInternationalConference\n                  onLearningRepresentations.OpenReview.net,2022.                    \n               [24] X.Huang,Y.-J.Huang,Y.Zhang,W.Tian,R.Feng,Y.Zhang,Y.Xie,Y.Li,andL.Zhang.\n                  Open-SetImageTaggingwithMulti-GrainedTextSupervision,2023. URLhttps://arxiv.\n                  org/abs/2310.15200.                                               \n               [25] C.Jia,M.Luo,Z.Dang,G.Dai,X.Chang,M.Wang,andJ.Wang. SSMG:Spatial-Semantic\n                  MapGuidedDiffusionModelforFree-FormLayout-to-ImageGeneration. InProceedingsof\n                  theAAAIConferenceonArtificialIntelligence,pages2480–2488.AAAIPress,2024.\n               [26] Y.Kirstain,A.Polyak,U.Singer,S.Matiana,J.Penna,andO.Levy. Pick-a-Pic: AnOpen\n                  DatasetofUserPreferencesforText-to-ImageGeneration. InAdvancesinNeuralInformation\n                  ProcessingSystems,2023.                                           \n               [27] Y.Lee,T.Yoon,andM.Sung. GrounDiT:GroundingDiffusionTransformersviaNoisyPatch\n                  Transplantation. InAdvancesinNeuralInformationProcessingSystems,2024.\n               [28] Y.Li,H.Liu,Q.Wu,F.Mu,J.Yang,J.Gao,C.Li,andY.J.Lee. GLIGEN:Open-SetGrounded\n                  Text-to-ImageGeneration. InProceedingsoftheIEEE/CVFConferenceonComputerVision\n                  andPatternRecognition,pages22511–22521.IEEE,2023.                 \n               [29] Y. Li, M. Keuper, D. Zhang, and A. Khoreva. Adversarial Supervision Makes Layout-to-\n                  ImageDiffusionModelsThrive. InProceedingsoftheInternationalConferenceonLearning\n                  Representations.OpenReview.net,2024.                              \n                                                                                    \n               [30] Z.Li, J.Zhang, Q.Lin, J.Xiong, Y.Long, X.Deng, Y.Zhang, X.Liu, M.Huang, Z.Xiao,\n                  D.Chen,J.He,J.Li,W.Li,C.Zhang,R.Quan,J.Lu,J.Huang,X.Yuan,X.Zheng,Y.Li,\n                  J.Zhang,C.Zhang,M.Chen,J.Liu,Z.Fang,W.Wang,J.Xue,Y.Tao,J.Zhu,K.Liu,S.Lin,\n                  Y.Sun,Y.Li,D.Wang,M.Chen,Z.Hu,X.Xiao,Y.Chen,Y.Liu,W.Liu,D.Wang,Y.Yang,\n                  J.Jiang,andQ.Lu. Hunyuan-DiT:APowerfulMulti-ResolutionDiffusionTransformerwith\n                  Fine-GrainedChineseUnderstanding,2024. URLhttps://arxiv.org/abs/2405.08748.\n                                          12",
        "char_count": 5539,
        "has_tables": false
      },
      {
        "page": 13,
        "content": "[31] T.Lin,M.Maire,S.J.Belongie,J.Hays,P.Perona,D.Ramanan,P.Dollár,andC.L.Zitnick.\n                  MicrosoftCOCO:CommonObjectsinContext. InProceedingsoftheEuropeanConferenceon\n                  ComputerVision,volume8693ofLectureNotesinComputerScience,pages740–755.Springer,\n                  2014.                                                             \n               [32] Z. Lin, D. Pathak, B. Li, J. Li, X. Xia, G. Neubig, P. Zhang, and D. Ramanan. Evaluating\n                  Text-to-VisualGenerationwithImage-to-TextGeneration. InProceedingsoftheEuropean\n                  ConferenceonComputerVision,volume15067ofLectureNotesinComputerScience,pages\n                  366–384.Springer,2024.                                            \n               [33] Y.Lipman,R.T.Q.Chen,H.Ben-Hamu,M.Nickel,andM.Le. Flowmatchingforgenera-\n                  tivemodeling. InProceedingsoftheInternationalConferenceonLearningRepresentations.\n                  OpenReview.net,2023.                                              \n               [34] B.Liu,E.Akhgari,A.Visheratin,A.Kamko,L.Xu,S.Shrirao,C.Lambert,J.Souza,S.Doshi,\n                  and D. Li. Playground v3: Improving Text-to-Image Alignment with Deep-Fusion Large\n                  LanguageModels,2024. URLhttps://arxiv.org/abs/2409.10695.         \n               [35] S.Liu,Z.Zeng,T.Ren,F.Li,H.Zhang,J.Yang,Q.Jiang,C.Li,J.Yang,H.Su,J.Zhu,and\n                  L.Zhang. GroundingDINO:MarryingDINOwithGroundedPre-trainingforOpen-SetObject\n                  Detection. InProceedingsoftheEuropeanConferenceonComputerVision,volume15105of\n                  LectureNotesinComputerScience,pages38–55.Springer,2024.           \n               [36] Z. Lv, Y. Wei, W. Zuo, and K. K. Wong. PLACE: Adaptive Layout-Semantic Fusion for\n                  SemanticImageSynthesis. InProceedingsoftheIEEE/CVFConferenceonComputerVision\n                  andPatternRecognition,pages9264–9274.IEEE,2024.                   \n                                                                                    \n               [37] M.Ohanyan,H.Manukyan,Z.Wang,S.Navasardyan,andH.Shi. Zero-Painter: Training-Free\n                  LayoutControlforText-to-ImageSynthesis. InProceedingsoftheIEEE/CVFConferenceon\n                  ComputerVisionandPatternRecognition,pages8764–8774.IEEE,2024.     \n               [38] openfree. flux-chatgpt-ghibli-lora. https://huggingface.co/openfree/\n                  flux-chatgpt-ghibli-lora,2025.                                    \n               [39] PatrickStarrrr. FLUX - Oil painting. https://civitai.com/models/1455014/\n                  chatgpt-4o-renderer?modelVersionId=1697982,2024.                  \n               [40] W.PeeblesandS.Xie. ScalableDiffusionModelswithTransformers. InProceedingsofthe\n                  IEEE/CVFInternationalConferenceonComputerVision,pages4172–4182.IEEE,2023.\n                                                                                    \n               [41] Q.Phung,S.Ge,andJ.Huang. GroundedText-to-ImageSynthesiswithAttentionRefocusing.\n                  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n                  pages7932–7942.IEEE,2024.                                         \n               [42] D.Podell,Z.English,K.Lacey,A.Blattmann,T.Dockhorn,J.Müller,J.Penna,andR.Rom-\n                  bach. SDXL:ImprovingLatentDiffusionModelsforHigh-ResolutionImageSynthesis. In\n                  ProceedingsoftheInternationalConferenceonLearningRepresentations.OpenReview.net,\n                  2024.                                                             \n               [43] QwenTeam. Qwen2.5-VL,January2025. URLhttps://qwenlm.github.io/blog/qwen2.\n                  5-vl/.                                                            \n                                                                                    \n               [44] A.Ramesh,P.Dhariwal,A.Nichol,C.Chu,andM.Chen. HierarchicalText-ConditionalImage\n                  GenerationwithCLIPLatents,2022. URLhttps://arxiv.org/abs/2204.06125.\n               [45] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-Resolution Image\n                  Synthesis with Latent Diffusion Models. In Proceedings of the IEEE/CVF Conference on\n                  ComputerVisionandPatternRecognition,pages10674–10685.IEEE,2022.   \n               [46] O.Ronneberger,P.Fischer,andT.Brox. U-Net: ConvolutionalNetworksforBiomedicalImage\n                  Segmentation. InMedicalImageComputingandComputer-AssistedIntervention,volume9351\n                  ofLectureNotesinComputerScience,pages234–241.Springer,2015.       \n                                                                                    \n                                          13",
        "char_count": 5514,
        "has_tables": false
      },
      {
        "page": 14,
        "content": "[47] C.Saharia,W.Chan,S.Saxena,L.Li,J.Whang,E.L.Denton,S.K.S.Ghasemipour,R.G.\n                  Lopes,B.K.Ayan,T.Salimans,J.Ho,D.J.Fleet,andM.Norouzi.PhotorealisticText-to-Image\n                  DiffusionModelswithDeepLanguageUnderstanding. InAdvancesinNeuralInformation\n                  ProcessingSystems,2022.                                           \n               [48] T.ShirakawaandS.Uchida. NoiseCollage: ALayout-AwareText-to-ImageDiffusionModel\n                  Based on Noise Cropping and Merging. In Proceedings of the IEEE/CVF Conference on\n                  ComputerVisionandPatternRecognition,pages8921–8930.IEEE,2024.     \n               [49] stability.ai. Stable Diffusion 3.5. https://stability.ai/news/  \n                  introducing-stable-diffusion-3-5,Nov2024.                         \n               [50] A.Taghipour,M.Ghahremani,M.Bennamoun,A.M.Rekavandi,H.Laga,andF.Boussaid.\n                  BoxIttoBindIt: UnifiedLayoutControlandAttributeBindinginT2IDiffusionModels,2024.\n                  URLhttps://arxiv.org/abs/2402.17910.                              \n                                                                                    \n               [51] M.Tancik,P.P.Srinivasan,B.Mildenhall,S.Fridovich-Keil,N.Raghavan,U.Singhal,R.Ra-\n                  mamoorthi,J.T.Barron,andR.Ng. FourierFeaturesLetNetworksLearnHighFrequency\n                  Functions in Low Dimensional Domains. In Advances in Neural Information Processing\n                  Systems,2020.                                                     \n               [52] vjleoliu. ChatGPT-4o Renderer. https://civitai.com/models/1455014/\n                  chatgpt-4o-renderer?modelVersionId=1697982,2025.                  \n               [53] X.Wang,T.Darrell,S.S.Rambhatla,R.Girdhar,andI.Misra. InstanceDiffusion: Instance-\n                  LevelControlforImageGeneration. InProceedingsoftheIEEE/CVFConferenceonComputer\n                  VisionandPatternRecognition,pages6232–6242.IEEE,2024.             \n               [54] Y.Wu,X.Zhou,B.Ma,X.Su,K.Ma,andX.Wang. IFAdapter: InstanceFeatureControlfor\n                  GroundedText-to-ImageGeneration,2024. URLhttps://arxiv.org/abs/2409.08240.\n                                                                                    \n               [55] J.Xie,Y.Li,Y.Huang,H.Liu,W.Zhang,Y.Zheng,andM.Z.Shou. BoxDiff: Text-to-Image\n                  SynthesiswithTraining-FreeBox-ConstrainedDiffusion. InProceedingsoftheIEEE/CVF\n                  InternationalConferenceonComputerVision,pages7418–7427.IEEE,2023. \n               [56] J.Xu,X.Sun,Z.Zhang,G.Zhao,andJ.Lin.UnderstandingandImprovingLayerNormalization.\n                  InAdvancesinNeuralInformationProcessingSystems,pages4383–4393,2019.\n               [57] H.Xue,Z.Huang,Q.Sun,L.Song,andW.Zhang. FreestyleLayout-to-ImageSynthesis. In\n                  ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages\n                  14256–14266.IEEE,2023.                                            \n               [58] B.Yang,Y.Luo,Z.Chen,G.Wang,X.Liang,andL.Lin.LAW-Diffusion:ComplexSceneGen-\n                  erationbyDiffusionwithLayouts. InProceedingsoftheIEEE/CVFInternationalConference\n                  onComputerVision,pages22612–22622.IEEE,2023.                      \n                                                                                    \n               [59] Z. Yang, J. Wang, Z. Gan, L. Li, K. Lin, C. Wu, N. Duan, Z. Liu, C. Liu, M. Zeng, and\n                  L.Wang.ReCo:Region-ControlledText-to-ImageGeneration.InProceedingsoftheIEEE/CVF\n                  ConferenceonComputerVisionandPatternRecognition,pages14246–14255.IEEE,2023.\n               [60] Y.Yao, T.Yu, A.Zhang, C.Wang, J.Cui, H.Zhu, T.Cai, H.Li, W.Zhao, Z.He, Q.Chen,\n                  H. Zhou, Z. Zou, H. Zhang, S. Hu, Z. Zheng, J. Zhou, J. Cai, X. Han, G. Zeng, D. Li,\n                  Z. Liu, and M. Sun. MiniCPM-V: A GPT-4V Level MLLM on Your Phone, 2024. URL\n                  https://arxiv.org/abs/2408.01800.                                 \n               [61] H.Zhang,D.Hong,Y.Wang,J.Shao,X.Wu,Z.Wu,andY.-G.Jiang. CreatiLayout: Siamese\n                  Multimodal Diffusion Transformer for Creative Layout-to-Image Generation, 2025. URL\n                  https://arxiv.org/abs/2412.03859.                                 \n               [62] L.Zhang,A.Rao,andM.Agrawala. Addingconditionalcontroltotext-to-imagediffusion\n                  models. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,pages\n                  3813–3824.IEEE,2023.                                              \n                                                                                    \n                                          14",
        "char_count": 5496,
        "has_tables": false
      },
      {
        "page": 15,
        "content": "[63] G.Zheng,X.Zhou,X.Li,Z.Qi,Y.Shan,andX.Li. LayoutDiffusion: ControllableDiffusion\n                  Model for Layout-to-Image Generation. In Proceedings of the IEEE/CVF Conference on\n                  ComputerVisionandPatternRecognition,pages22490–22499.IEEE,2023.   \n               [64] D.Zhou,Y.Li,F.Ma,X.Zhang,andY.Yang. MIGC:Multi-InstanceGenerationControllerfor\n                  Text-to-ImageSynthesis. InProceedingsoftheIEEE/CVFConferenceonComputerVisionand\n                  PatternRecognition,pages6818–6828.IEEE,2024.                      \n               [65] D. Zhou, Y. Li, F. Ma, Z. Yang, and Y. Yang. MIGC++: Advanced Multi-Instance Gener-\n                  ationControllerforImageSynthesis. IEEETransactionsonPatternAnalysisandMachine\n                  Intelligence,47(3):1714–1728,2025.                                \n               [66] D. Zhou, J. Xie, Z. Yang, and Y. Yang. 3DIS-FLUX: simple and efficient multi-instance\n                  generationwithDiTrendering,2025. URLhttps://arxiv.org/abs/2501.05131.\n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                          15",
        "char_count": 5211,
        "has_tables": false
      },
      {
        "page": 16,
        "content": "Supplementary   Material                              \n                                                                                    \n                                                                                    \n               A  Early-StageLayoutControlinAssemble-MMDiT                          \n                                                                                    \n               WeapplytheAssemble-MMDiTlayoutcontrolmoduleexclusivelyduringtheinitial30%ofthe\n               denoisingtrajectoryanddeactivateitfortheremaining70%. Thistwo-stagestrategyprovidesrobust\n               low-frequencystructuralguidanceintheearlystagestofacilitatelayoutalignment,whileallowing\n               subsequentunconstrainedrefinementofhigh-frequencydetailsduringlaterdenoisingphases.\n                                                                                    \n                                                                                    \n                   0.9                                                              \n                                                                                    \n                   0.8                                                              \n                   0.7                                                              \n                                                                                    \n                   0.6                                                              \n                                                                                    \n                   0.5                                                              \n                                                                                    \n                   0.4                                                              \n                   0.3                                                              \n                                                                                    \n                   0.2                                                              \n                      0.0     0.2     0.4     0.6    0.8     1.0                    \n                                        Control Ratio                               \n                   erocS                                                            \n                                   3.0                                              \n                                   :detceleS                                        \n                                     Ablation on Control Ratio                      \n                                                                10                  \n                                                                8                   \n                                                                6                   \n                                                           mIoU (LGS) 4             \n                                                           Color (LGS)              \n                                                           Texture (LGS)            \n                                                           Shape (LGS)              \n                                                           VQA                      \n                                                                  )s(               \n                                                                  emiT              \n                                                            Time (s)                \n               Figure9: Impactoftheproportionofdiffusionstepsincorporatinglayoutconditioningongeneration\n               quality.                                                             \n               AsillustratedinFigure9,restrictinglayoutcontroltolessthan30%ofthediffusionprocessresults\n               ininsufficientlayoutalignmentwiththetargetboundingboxes. Incontrast,extendingcontrolbeyond\n               thisoptimalthresholdleadstoadeclineinoutputquality. Furthermore,increasingtheproportionof\n               layout-guidedstepsresultsinsignificantadditionalcomputationalcost.   \n               B  AdditionalAblationStudies                                         \n               B.1 EffectofBboxEncodingandDenseSample                               \n               To clarify the role of bounding box encoding and DenseSample, we further ablated the SD3-M\n               basedInstanceAssemblemodelonDenseLayout. Boundingboxembeddingsguidecorrectobject\n               placement,whileDenseSampleprovidesadditionalimprovementsinspatialaccuracyandinstance-\n               levelsemantics. TheresultsinTable6demonstratethatbothcomponentscontributetotheoverall\n               performance.                                                         \n                         Table6: AblationonboundingboxencodingandDenseSample.       \n                        Setting         mIoU↑ color↑ texture↑ shape↑ VQA↑           \n                        w/obboxencoding,w/oDenseSample 51.22 32.15 34.04 33.53 93.30\n                        w/bboxencoding,w/oDenseSample 51.28 32.68 34.94 34.58 93.33 \n                        w/bboxencoding,w/DenseSample 52.07 33.77 36.21 35.81 93.54  \n               B.2 ComparisonwithAttentionMask-basedRegionInjection                 \n               WealsocompareourAssemble-Attndesignwithattentionmask-basedregioninjection. Whileboth\n               canbeviewedasregion-wiseattentionmechanisms,attentionmasksoperategloballyandmaycause\n                                          16                                        \n\n[表格内容]\n | Time (s)\n3.0\n:detceleS mIoU (LGS)\nColor (LGS)\nTexture (LGS)\nShape (LGS)\nVQA |  | Time (s)\n |  | mIoU (LGS)\nColor (LGS)\nTexture (LGS)\nShape (LGS)\nVQA |",
        "char_count": 6180,
        "has_tables": true
      },
      {
        "page": 17,
        "content": "semanticleakageinoverlappingregions. Ourmethodinsteadappliesinstance-wiseself-attention\n               oncroppedlatentregionsandthenfusestheupdatedfeaturesviatheAssemblestep,whichismore\n               effective in dense layouts. As shown in Table 7, our design achieves superior instance attribute\n               consistencyandahigherVQAscorecomparedtotheattentionmaskbaseline.     \n                   Table7: Comparisonbetweenattentionmask-basedinjectionandourAssemble-Attn.\n                                                                                    \n                         Method        spatial↑ color↑ texture↑ shape↑ VQA↑         \n                         SD3-Medium(basemodel) 77.49 60.28 62.55 60.38 93.30        \n                         Attentionmask(SD3-M) 94.11 74.28 77.58 76.54 91.53         \n                         InstanceAssemble(ours,SD3-M) 94.97 77.53 80.72 80.11 93.12 \n                                                                                    \n               C  Underlyingdataforradar-chartvisualizations                        \n                                                                                    \n               InSections4.3,weutilizedaradarcharttodepicteachquantitativevariablealongequi-angularaxes,\n               providinganintuitivecomparison. Thisvisualizationhighlightsthemultifacetedsuperiorityofour\n               method. Here, wepresentthecorrespondingrawevaluationresultsintabularform. Specifically,\n               Tab.8correspondstoFig.6,thusensuringaclearmappingbetweeneachradar-chartsubfigureand\n               itsunderlyingdata.                                                   \n                     Table8: QuantitativeresultsofadditionalvisualcontentonDenseLayout.\n                                                                                    \n                                       LayoutGroundingScore GlobalQuality           \n                         DenseLayout                                                \n                                      mIoU↑ color↑ texture↑shape↑ VQA↑ Pick↑ CLIP↑  \n                         RealImages(UpperBound) 92.35 76.52 80.78 79.78             \n                         text         43.72 26.57 28.56 28.39 93.37 21.63 12.45     \n                         text+image   55.29 42.15 44.50 44.24 91.66 22.05 12.95     \n                         text+depth   49.64 28.25 31.82 31.62 92.83 21.28 13.25     \n                         text+edge    50.73 29.45 33.92 33.84 90.13 21.26 13.55     \n               D  MoreDetailsonDenseLayoutEvaluationDataset                         \n               D.1 ConstructionPipelineofDenseLayoutDataset                         \n               The DenseLayout dataset is constructed through a multi-stage pipeline designed to extract high-\n               density and semantically-rich layout information from synthetic images. The pipeline includes\n               followingsteps:                                                      \n                                                                                    \n                  1. ImageGenerationusingFlux.1-Dev[4]                              \n                    A diverse set of synthetic images is generated using Flux.1-Dev, a text-to-image model.\n                    Theinputpromptsaregenerictextualdescriptions,sampledfromtheLayoutSAMdataset,\n                    whichisbasedonSA-1B.Theimagesareresizedtomaintainthesameaspectratioasthe\n                    originalSA-1Bimages,withthelongeredgesetto1024pixels. Thisstepprovidesavisually\n                    complexbaseforextractinglayoutstructures.                       \n                  2. Multi-labelTaggingusingRAM++[24]                               \n                    ThegeneratedimagesaretaggedusingRAM++,thenext-generationmodelofRAM,which\n                    supportsopen-setrecognition. Thesetagsofferhigh-levelsemanticguidanceforsubsequent\n                    grounding.                                                      \n                  3. ObjectDetectionviaGroundingDINO[35]                            \n                    Usingtheimageanditspredictedtagsasinput,GroundingDINOperformsopen-setobject\n                    detection.Itoutputsboundingboxesandclasslabelsforalldetectedentities.Thedetectionis\n                    configuredwithabox_thresholdof0.35andatext_thresholdof0.25. Eachdetected\n                    boundingboxistreatedasaninstanceboundingbox, andthecorrespondingpredicted\n                    labelisrecordedastheinstancedescription.                        \n                  4. DetailedCaptioningwithQwen2.5-VL[43]                           \n                    EachboundingboxregioniscroppedfromtheoriginalimageandfedintoQwen2.5-VL\n                                          17",
        "char_count": 5428,
        "has_tables": false
      },
      {
        "page": 18,
        "content": "togenerateafine-grainedcaption. Theseregion-levelcaptionsarestoredasthedetailed\n                    descriptionforeachinstance,enrichingthesemanticinformationbeyondcategorylabels.\n                  5. DensityFiltering                                               \n                    Toensurehighlayoutcomplexity,onlyimageswith15ormoredetectedinstances(asoutput\n                    byGroundingDINO)areretained. Thisresultsinadenselayoutdistributionsuitablefor\n                    layout-conditionedgenerationtasks. ThedistributionofinstancecountisshowninFig.10.\n               Finally,theDenseLayoutdatasetcontains5,000imagesand90,339instances,withanaverageof\n               18.1instancesperimage.                                               \n                                                                                    \n                                                                                    \n                           1200                                                     \n                           1000                                                     \n                                                                                    \n                           800                                                      \n                           600                                                      \n                           400                                                      \n                                                                                    \n                           200                                                      \n                            0 15 20  25 30  35 40  45 50                            \n                                       Number of Instances per Image                \n                          tnuoC                                                     \n                          egamI                                                     \n                                    DenseLayout: Instance Count Distribution        \n                                                      Mean: 18.1                    \n                                                      Median: 17.0 Std: 3.7         \n                        Figure10: InstancecountdistributionperimageinDenseLayout.   \n               AnnotationFormat. Theannotationforeachimageconsistsof:               \n                   • global_caption: theoriginalpromptusedforimagegeneration.       \n                   • image_info: metadataoftheimage,includingheightandwidth.        \n                   • instance_info: alistofinstances,eachwith:                      \n                     – bbox: theboundingboxoftheinstance,formattingas[x ,y ,x ,y ]. \n                                                      1 1 2 2                       \n                     – description: thecategorylabelpredictedbyGroundingDINO.       \n                     – detail_description: a fine-grained caption generated by Qwen2.5-VL for the\n                       croppedregion.                                               \n               D.2 SamplesofDenseLayoutDataset                                      \n                                      \"instance_info\": [                            \n                                      {                                             \n                                        \"bbox\": [129,489,283,642],                  \n                                        \"description\": \"nightstand\",                \n                                        \"detail_description\": \"The nightstand is dark brown,\n                                           compact, with a drawer.\"                 \n                                      },                                            \n                                      {                                             \n                                        \"bbox\": [306,170,430,339],                  \n                                        \"description\": \"picture frame\",             \n                                        \"detail_description\": \"Brown wooden frame containing a\n                                           watercolor painting of green leaves on a white\n                                           background.\"                             \n                                      },                                            \n                                      {                                             \n                                        \"bbox\": [603,170,727,340],                  \n                                        \"description\": \"picture frame\",             \n                                        \"detail_description\": \"A simple brown wooden frame holds\n                                           a botanical print with detailed leaves and stems.\"\n                                      },                                            \n                                      ...                                           \n                                      ]                                             \n                           Figure11: AsampleofDenseLayoutanditsannotation.          \n                                          18                                        \n\n[表格内容]\n |  |  |  |  |  |  |  |  |  |  |  |  |  |  | M\nMe | ean: 18.1\ndian: 17.0 | \n |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | Std: 3.7 | \n |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | \n\n\n\"instance_info\": [\n{\n\"bbox\": [129,489,283,642],\n\"description\": \"nightstand\",\n\"detail_description\": \"The nightstand is dark brown,\ncompact, with a drawer.\"\n},\n{\n\"bbox\": [306,170,430,339],\n\"description\": \"picture frame\",\n\"detail_description\": \"Brown wooden frame containing a\nwatercolor painting of green leaves on a white\nbackground.\"\n},\n{\n\"bbox\": [603,170,727,340],\n\"description\": \"picture frame\",\n\"detail_description\": \"A simple brown wooden frame holds\na botanical print with detailed leaves and stems.\"\n},\n...\n]",
        "char_count": 6936,
        "has_tables": true
      },
      {
        "page": 19,
        "content": "This is a photo showcasing a traditional Chinese-style pavilion and This is a photo depicting a traditional Asian floating market scene. In This is a photo showcasing a Chinese-style building and a statue. The\n               boats on the river. The pavilion is located on the right side of the the picture, two women are sitting in their respective boats, each building is brightly colored, with a red roof and a yellow door, and the\n               picture, with a beautifully decorated roof and a golden plaque hanging wearing a traditional conical hat, and dressed in brightly colored statue is located in front of the building, standing on a pedestal. The\n               in the middle. In front of the pavilion is a row of wooden boats, each traditional clothing. The woman on the left is wearing a blue top and a surrounding environment is a spacious square, with several pedestrians\n               with a red flag hanging on it, and people are gathered on the boat dock. dark skirt, while the woman on the right is wearing a yellow top and a walking around the building. The background is a clear blue sky and lush\n               The river is calm, and the sky is clear, with a few clouds leisurely light-colored skirt. Their boats are filled with a variety of goods, trees.\n               drifting by.      including fresh flowers, fruits, and other food items. The boats are\n                                 adorned with bright colored cloths, adding a festive atmosphere to the\n                                 scene. The entire scene is captured under natural light, presenting a\n                                 tranquil and vibrant market atmosphere.            \n               This is a photo showcasing a modern interior design style, with the This is a realistic-style photograph depicting a city street scene after This is a photograph showcasing a famous archway in a city. The archway\n               focus on a spacious and bright room. The room is furnished with wooden a flood. In the photo, vehicles and pedestrians are struggling to is a reddish-brown structure, adorned with golden decorations and\n               furniture, including a long wooden table and a matching wooden bench. On navigate through the flooded streets. A yellow tricycle is parked in the sculptures, and its design is very intricate. The archway is located on\n               the table, there are some books and a laptop, while on the bench, there middle of the street, surrounded by vehicles covered with blue a wide street, with pedestrians and cyclists weaving through the road.\n               is a black desk lamp. The room's lighting comes from several minimalist waterproof cloth. Pedestrians are using umbrellas to shield themselves The sky is clear, with a few clouds scattered in the blue sky. The\n               chandeliers and a large window, through which you can see the green from the rain, some are pushing bicycles, while others are walking. The buildings in the background have a Mediterranean style, with the domes\n               plants outside. The floor is covered with a red carpet, and there are buildings on both sides of the street are submerged in water, and the of some buildings visible in the distance.\n               several decorations on the floor, including a red vase and a decorative wires and poles above the street are also submerged, adding to the\n               sculpture. The entire scene is illuminated by natural light, creating a severity of the flooding. The entire scene is shrouded in a gloomy\n               warm and comfortable atmosphere. atmosphere, with no sunlight piercing through the clouds.\n               T f t b r a a ph oo h a r ri or e r f e es fe n e s sg g w c ei .r r a o ns o a n c v t Iu s d a e ia nn s r r n d l a s e gp f, a d h r n s p ao oi d m a w t nt , a r i to ts l k t r t l e h as os h e d nh fu e r d qo r r i e uw tf e h n n ic ha o s la ec a u f e s e r s r ai b e e o t nn up , n r dg ir s t e le e b e pa ds v o o s e ie e t f , ar nn r h cu gt a t a er si l w h n fa ,n i e d ul g w t l t o h f t l ha o e h ra e d t n e un rv e r c rd ei n a e s as v d . k lc ii b i y a sd u t I ap i i n i te ag l o s m, r d n t o re i a h f sw oe n l e i pi wn g l ht s w b l eh oa , o a e r fn o c d ea d i d k . w n e g wg oy c n r ir oe l o ta dl u s u hs el d t n s no i r d wl w n u , ha f g c in ec t t td no a u h e cl r e i eo l e cn sr a s h l ,. r i ot g a l uh wO e n l de in r d s s t ,h T m d e d a p sx th a e e ap ai g p c f tr is n i o i te n i c r g es ei f t a u rs ds i s t r ni c i e so ga e a v n l n e i a. ap t f n n sh i e dT so s g l a h t c u e fe wo u r m h l i l e e e os ns p n l rc dh t i t m au oo u n s e ll ww r , t p sc e a . dt ,a i eu s l l n T sr ti o o c h ie hn c n l e g rg a g u ni o t d a ss ut e r i l , gh d o n t s he b g a au i e r nr wi n , a dr hn i o it t h s s tu ce h o t hn hr e l a d ed i d t e e to c i u c wd hr e n e o a e n g r lb o t o a ly lf e a f t s i r e e ga c a d ax h o r rq tc f o f w eu h s i i i fu t s g t as rr h , u h di oc e r ot mh w e c re , a i o n o l t h m es uw t h o p dt ti a l l a st r a d e wt ih . i x iu d s n te et T o g g hs h h l e se e e a o ca h m m on if s n c e ld no c r t o ec u o r r su l s i f s p s c u i t l no u a ,n r n e da T w g s s i a be eh i l u n v ni t a r c fe cs h s r l er h s o u wa ei a u d l ss a n i p , m n d n es a o d e g de e d d et np e m r ss jh r e b e t oo n t y d ro yt a i- f io l s b as n e r nt s gs s v i sy t h t e c l a to r r k oe i hw u a n r ec c l b b s a t u tu ss u b i hi a ui r u l el n nn e i d d d sg s l i si h . d n qn b ia i g ug e n T n s a n em h g ri c .o e s a en h d n , e Te s o d t s hr q f sh en u m oe f a d o m o su r i d ec r kr e f e e yb f r an p a i e n rt e in s r ee o s e g r p s p n l w, l cq a t a a e lu v s li ea e a s kt t ar d r is o re c c n ,. w h u ge r i i r ,x e wT t t t t s ih h e a se t te c i or . h g t n mi s r u eo T aq e r w r h u y a a a e fa l l rc er s l eo s we l s s m q a t . sp u wi b y io a hs s l T ts r i , e h te e ts s e id ep w , r n i a i e go sc t f i h a o o r nu e s ,\n                                                                                    \n                                 adding a warm and sacred atmosphere to the interior. clouds scattered in the blue sky.\n               Figure12: RepresentativesamplesfromDenseLayoutdatasetdemonstrating: (a)High-densityscene\n               with≥15instances,(b)Complexinstancerelationshipswithpreciseattributespecifications.\n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                          19",
        "char_count": 9520,
        "has_tables": false
      },
      {
        "page": 20,
        "content": "E  Moreresultswithtextual-onlycontent                                \n               E.1 InstanceAssemblebasedonSD3-Medium                                \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                    x                                                               \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                  Figure13: MoreresultsofInstanceAssemblebasedonSD3-Mediumwithtextual-onlycontent.\n                                                                                    \n                                                                                    \n               E.2 InstanceAssemblebasedonFlux.1-Dev                                \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                   Figure14: MoreresultsofInstanceAssemblebasedonFlux.1-Devwithtextual-onlycontent.\n                                                                                    \n               E.3 InstanceAssemblebasedonFlux.1-Schnell                            \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                  Figure15: MoreresultsofInstanceAssemblebasedonFlux.1-Schnellwithtextual-onlycontent.\n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                          20                                        \n                                                                                    \n                                                                                    \n\n[表格内容]\n |  |  |  |  |  | \n |  |  |  |  |  | \n |  |  |  |  |  | \n |  |  |  |  |  |",
        "char_count": 5232,
        "has_tables": true
      },
      {
        "page": 21,
        "content": "F  Moreresultswithadditionalvisualcontent                            \n               F.1 AdditionalImage                                                  \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                             Figure16: Moreresultswithadditionalimage.              \n                                                                                    \n                                                                                    \n               F.2 AdditionalDepth                                                  \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                              Figure17: Moreresultswithadditionaldepth.             \n                                                                                    \n               F.3 AdditionalEdge                                                   \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                              Figure18: Moreresultswithadditionaledge.              \n                                                                                    \n                                                                                    \n                                          21                                        \n                                                                                    \n                                                                                    \n\n[表格内容]\nFigure16: Moreresultswithadditionalimage.\n.2 AdditionalDepth\nFigure17: Moreresultswithadditionaldepth.\n.3 AdditionalEdge\n\nFigure18: Moreresultswithadditionaledge.\n21",
        "char_count": 5282,
        "has_tables": true
      }
    ],
    "full_text": "InstanceAssemble:  Layout-Aware   Image  Generation                 \n                         via Instance Assembling  Attention                         \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                             QiangXiang1,2,ShuangSun2,BingleiLi1,3,                 \n                    DejiaSong2,HuaxiaLi2,YiboChen2, XuTang2,YaoHu2, JunpingZhang1∗  \n                        1ShanghaiKeyLaboratoryofIntelligentInformationProcessing,   \n                      CollegeofComputerScienceandArtificialIntelligence,FudanUniversity\n                            2XiaohongshuInc. 3ShanghaiInnovationInstitute           \n                        {qxiang24, blli24}@m.fudan.edu.cn, jpzhang@fudan.edu.cn,    \n                 {sunshuang1, dejiasong, lihuaxia, zhaohaibo, tangshen, xiahou}@xiaohongshu.com\n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n               Figure1:Layout-awareimagegenerationresultbyInstanceAssemble.Weshowimagegeneration\n               resultunderpreciselayoutcontrol,rangingfromsimpletointricate,sparsetodenselayouts.\n                                       Abstract                                     \n                                                                                    \n                    Diffusionmodelshavedemonstratedremarkablecapabilitiesingeneratinghigh-\n                    qualityimages. RecentadvancementsinLayout-to-Image(L2I)generationhave\n                    leveragedpositionalconditionsandtextualdescriptionstofacilitatepreciseand\n                    controllableimagesynthesis. Despiteoverallprogress,currentL2Imethodsstill\n                    exhibitsuboptimalperformance. Therefore,weproposeInstanceAssemble,anovel\n                    architecturethatincorporateslayoutconditionsviainstance-assemblingattention,\n                    enabling position control with bounding boxes (bbox) and multimodal content\n                    controlincludingtextsandadditionalvisualcontent. Ourmethodachievesflexible\n                    adaptiontoexistingDiT-basedT2Imodelsthroughlight-weightedLoRAmodules.\n                    Additionally,weproposeaLayout-to-Imagebenchmark,Denselayout,acompre-\n                    hensivebenchmarkforlayout-to-imagegeneration,containing5kimageswith90k\n                    instancesintotal. WefurtherintroduceLayoutGroundingScore(LGS),aninter-\n                    pretableevaluationmetrictomorepreciselyassesstheaccuracyofL2Igeneration.\n                    ExperimentsdemonstratethatourInstanceAssemblemethodachievesstate-of-the-\n                    artperformanceundercomplexlayoutconditions,whileexhibitingstrongcompati-\n                    bilitywithdiversestyleLoRAmodules. Thecodeandpretrainedmodelsarepub-\n                    liclyavailableathttps://github.com/FireRedTeam/InstanceAssemble.\n                 ∗Correspondingauthor.                                              \n               39thConferenceonNeuralInformationProcessingSystems(NeurIPS2025).     \n  5202                                                                              \n  tcO                                                                               \n  82                                                                                \n  ]VC.sc[                                                                           \n  2v19661.9052:viXra\n\n1 Introduction                                                       \n                                                                                    \n               Diffusionmodels[22]haverevolutionizedimagegenerationtask,witharchitectureslikeDiffusion\n               Transformer(DiT)[40]offeringsuperiorqualityovertraditionalUNet-basedframeworks. Recent\n               implementationssuchasStableDiffusion3/3.5[15,49]andFlux.1[4]furtherenhancetext-to-image\n               alignment, paving the way for advancements in layout-controlled generation. Layout-to-Image\n               (L2I)generationisataskthatfocusesoncreatingimagesunderlayoutconditions,allowingusers\n               todefinespatialpositionsandsemanticcontentofeachinstanceexplicitly. Thistaskfacesseveral\n               significantchallenges:(i)ensuringpreciselayoutalignmentwhilemaintaininghighimagequality,(ii)\n               preservingobjectpositionsandsemanticattributesaccuratelyduringtheiterativedenoisingprocess\n               ofdiffusionmodels,and(iii)supportingvarioustypesofreferenceconditions,suchastexts,images\n               andstructureinformation. Thesechallengeshighlightthecomplexityofachievingrobustandflexible\n               layout-controlledimagegeneration.                                    \n               ExistingL2Imethodscanbebroadlycategorizedintotraining-freeandtraining-basedapproaches,\n               bothpossessingdistinctadvantagesandlimitations. Training-freemethods[55,7,13,8,5,27]rely\n               onheuristictechniqueswithoutmodifyingthebasemodel. However,thesemethodsoftenexhibit\n               degradedperformanceincomplexlayouts,demonstratehighsensitivitytohyperparametertuning,\n               andsufferfromslowinferencespeed,whichmakethemlesspracticalforreal-worldapplications.\n               Incontrast,training-basedmethods[63,53,64,29,61]involvetrainingspecificlayoutmodulesto\n               improvelayoutalignment,whichintroducesasignificantamountofextraparametersandincreases\n               trainingcomplexityandresourcerequirements. Additionally, existingL2Ievaluationmetricsex-\n               hibitinaccuracies,suchasfalseacceptanceandlocalizationerrors. Theseidentifiedshortcomings\n               necessitatealgorithminnovationforeffectiveandefficientlayout-controlledimagegeneration.\n               Therefore, we propose InstanceAssemble, a novel framework that systematically tackles these\n               issuesthroughinnovativedesignandefficientimplementation. Ourapproachintroducesacascaded\n               InstanceAssemblestructure,whichemploysamultimodalinteractionparadigmtoprocessglobal\n               prompts and instance-wise layout conditions sequentially. By leveraging the Assemble-MMDiT\n               architecture,weapplyanindependentattentionmechanismtothesemanticcontentofeachinstance,\n               thusenablingeffectivehandlingofdenseandcomplexlayouts. Furthermore,weadoptLoRA[23]\n               forlightweightadaptation,addingonly71MparameterstoSD3-Medium(2B)and102MtoFlux.1\n               (11.8B).Ourmethodenablespositioncontrolwithboundingboxesandmultimodalcontentcontrol\n               includingtextsandadditionalvisualcontent. Thislightweightdesignpreservesthecapabilitiesof\n               thebasemodelwhileenhancingflexibilityandefficiency. Wealsointroduceanovelmetriccalled\n               LayoutGroundingScore(LGS)toensureaccurateevaluationforL2Igeneration,alongsideatest\n               datasetDenseLayout. Thismetricprovidesaconsistentbenchmarkforassessinglayoutalignment.\n               Ourmethodachievesstate-of-the-artperformanceacrossbenchmarksanddemonstratesrobustlayout\n               alignmentunderawidevarietyofscenarios,rangingfromsimpletointricate,sparsetodenselayouts.\n               Notably,despitebeingtrainedonsparselayouts(≤ 10instances),ourapproachmaintainsrobust\n               generalization capability on dense layouts (≥ 10 instances), confirming the effectiveness of our\n               proposedInstanceAssemble. Themaincontributionsarelistedbelow.        \n                  1. WeproposeacascadedInstanceAssemblestructurethatprocessesglobaltextpromptsand\n               layoutconditionssequentially,enablingrobusthandlingofcomplexlayoutsthroughanindependent\n               attentionmechanism.                                                  \n                  2. ByleveragingLoRA[23],weachieveefficientadaptationwithminimalextraparameters\n               (3.46%onSD3-Mediumand0.84%onFlux.1),supportingpositioncontrolwithmultimodalcontent\n               controlwhilepreservingcapabilitiesofbasemodel.                       \n                  3. WeproposeanewtestdatasetDenseLayoutandanovelmetricLayoutGroundingScore\n               (LGS)forLayout-to-Imageevaluation. Experimentalresultsdemonstratethatourapproachachieves\n               state-of-the-artperformanceandrobustcapabilityundercomplexanddenselayoutconditions.\n               2 RelatedWork                                                        \n               Text-to-ImageGenerationText-to-imagesynthesis[47,45,42,40,30,17,6,1,44,3]haswitnessed\n               rapidprogresswiththedevelopmentofdiffusionmodels.Initialworks[44,3,47,45]utilizeUNet[46]\n               as the denoising architecture, leveraging cross-attention mechanisms to inject text-conditioning\n                                          2\n\nGlobal Prompt 1. Global Prompt – Image 2. Instances - Image       \n                                Global Prompt        Global Prompt                  \n                “                                                                   \n                a                                                                   \n                o                                                                   \n                 Va                                                                 \n                 f                                                                  \n                 i                                                                  \n                 sh                                                                 \n                 aq                                                                 \n                 si                                                                 \n                 u                                                                  \n                  us kat                                                            \n                  nr                                                                \n                  ao                                                                \n                  ie                                                                \n                  lr g.i                                                            \n                  h                                                                 \n                   c                                                                \n                   tT                                                               \n                   Ia ,hl                                                           \n                   e                                                                \n                   n                                                                \n                   r                                                                \n                   sNm                                                              \n                    im                                                              \n                    too don                                                         \n                    in                                                              \n                    aiu                                                             \n                    nu                                                              \n                    nsm gme                                                         \n                     e                                                              \n                     cen                                                            \n                     on                                                             \n                     et                                                             \n                     nt                                                             \n                     l                                                              \n                     ai                                                             \n                      Co                                                            \n                      s                                                             \n                      oc                                                            \n                      h                                                             \n                      a                                                             \n                      oa                                                            \n                      nt                                                            \n                      r                                                             \n                       te ssd                                                       \n                       et                                                           \n                       e                                                            \n                       .a                                                           \n                       ni ”tn                                                       \n                       u                                                            \n                       t                                                            \n                        e                                                           \n                         redocnEredocnE                                             \n                          EAVtxeT                                                   \n                            L Enay co odut                                          \n                              er                                                    \n                                xN                                                  \n                                 Image MMDiT                                        \n                                           1In                                      \n                                            2I sm taag                              \n                                             n                                      \n                                             3e                                     \n                                             ces                                    \n                                              4                                     \n                                                As Ms Mem Dib Tle- 1In              \n                                                       2I sm taag                   \n                                                       n                            \n                                                        3e                          \n                                                        ces                         \n                                                         4                          \n                                                            redoceD                 \n                                                             EAV                    \n                 Textual Instance Content                                           \n                                                           Assemble-MMDiT           \n                 “A tall, pointed white structure                                   \n                 w s “ s d “ hi p e B itAt h t r sa h e a o ttma r i n ouh i l z rejo c s e i e Sr a cosi l a e aft pz n q l i aco f d u an i e f, tt n a s i h ia i t gw ol a r r ue r\n                    a                                                               \n                    l i i ra s ltb d a ee a a e n sh n t r e w Ld s dr i as t a t ee t h t a pd h\n                     y                                                              \n                     a e t i on t u cb od p o e tr r                                \n                      u                                                             \n                      e p , eo n ta a . dn a k ” .z te. ”e” redocnE txeT FM oL uP   \n                             rier                                                   \n                                egamI mroNreyaLadA latent 1 tacno&C .joV rKQ P      \n                                              .                                     \n                                                nttA                                \n                                                  1    elbmessA FF&mroNreyaL        \n                  Pointe hd                                                         \n                   o                                                                \n                    rr so eof DenseSample secnatsnI 1234 mroNreyaLadA 1234 2 tacno&C .jV oK rQ P\n                                              .                                     \n                                                  2                                 \n                                                   Assemble-Attn                    \n                                                          1234 FF&mroNreyaL 1234    \n                  Historical monument Legend Concatenation Element-wise Addition Frozen Trainable(LoRA)\n               Figure2: TheproposedInstanceAssemblepipeline. Variouslayoutconditionsareprocessedbythe\n               LayoutEncodertoobtaininstancetokens,whichguidetheimagegenerationviaAssemble-MMDiT.\n               InAssemble-MMDiT,theinstancetokensinteractwithimagetokensthroughtheAssembling-Attn.\n               signals. Recently, researches [6, 15, 49, 4, 34] have used the Multimodal Diffusion Transformer\n               (MMDiT)architecture,markingasignificantimprovement.                  \n               Layout-to-ImageGenerationLayout-to-Imagegenerationenablesimagegenerationunderlayout\n               conditions,whichisdefinedasspatialpositionswithtextualdescriptions. Existingapproachescanbe\n               broadlycategorizedintotraining-freeandtraining-basedparadigms.       \n               Training-freemethodsleveragepretrainedtext-to-imagediffusionmodelswithoutadditionaltrain-\n               ing. Acommonstrategyinvolvesgradient-basedguidanceduringdenoisingtoalignwithlayout\n               conditions[55,12,41,13,18,50]. Also,therearemethodsthatdirectlymanipulatelatentsthrough\n               well-defined replacing or merging operations [8, 48, 2] or enforce layout alignment via spatially\n               constrainedattentionmasks[5,20]. GrounDiT[27]exploitssemanticsharinginDiT:acroppednoisy\n               patchandthefullimagebecomesemanticcloneswhendenoisedtogether,enablinglayout-to-image\n               generation by jointly denoising instance regions with their corresponding image context. Other\n               approachesgenerateeachinstanceseparatelyandemployinpaintingtechniquestocomposethefinal\n               image[37]. However,thesemethodsdemonstratedecentperformanceprimarilyonsimpleandsparse\n               layouts,whiletheiraccuracydecreasesinmorecomplexlayouts. Somemethodsrequirehyperparam-\n               etertuningspecifictodifferentlayoutconditions,reducingtheiradaptability. Furthermore,additional\n               gradient computations or latent manipulations result in slow inference speed, thus limiting their\n               applicabilityinreal-worldscenarios.                                  \n               Training-basedmethodsexplicitlyincorporatelayoutconditioningthrougharchitecturalmodifica-\n               tions. Mostapproachesinjectspatialconstraintsviacross-attention[63,57,36,25,59,16,54]or\n               self-attention[10,53,28].Someworksproposededicatedlayoutencodingmodules[9,64,65,58,62]\n               oradoptatwo-stagepipelinethatgeneratesimagesafterpredictingadepthmapwithlayoutcondi-\n               tions[14,66]. Otherworksleverageautoregressiveimagegenerationmodels[19]. Thesemethods\n               sufferhighcomputationalcostsduetoexcessiveparameters.                \n               3 Method                                                             \n               PreliminariesRecentstate-of-the-arttext-to-imagemodelssuchasSD3[15]andFlux[4]adoptthe\n               MultimodalDiffusionTransformer(MMDiT)asthebackboneforgeneration. Unliketraditional\n               UNet-basedcross-attentionapproaches,MMDiTstreatimageandtextmodalitiesinasymmetric\n               manner, which leads to stronger prompt alignment and controllability. These models are trained\n               under the flow matching framework [33], which formulates generation as learning a continuous\n               velocityfieldthattransportsnoisetodata. GivenacleanlatentxandGaussiannoiseϵ∼N(0,I),an\n                                          3                                         \n\n[表格内容]\n2. Instances - Image\nImage\nAssemble-\nInstances MMDiT\n1 2 3 4\n\n |  |  | \n\n |  | \n\n | \n | \n\n |  |  | \n\n |  | \nInstances |  | \n1 | 2 | 3 4\n\n |  | \n\n2 | 3 | 4\n\nnttA | 2\n\nhorse\nHistorical monument |\n\ninterpolatedlatentisdefinedas                                        \n                                 z =(1−t)x+tϵ, t∈[0,1].             (1)             \n                                  t                                                 \n               The training objective minimizes the squared error between the predicted velocity and the target\n               velocity(ϵ−x):                                                       \n                                        h              i                            \n                            L  =E        ∥v (z ,t,y)−(ϵ−x)∥2 ,      (2)             \n                             FM  ϵ∼N(0,I),x,t θ t     2                             \n               wherev isimplementedwithanMMDiTbackbone.                             \n                   θ                                                                \n               Problem Definition Layout-to-Image generation aims to synthesize images with precise control\n               throughaglobalpromptpandinstance-wiselayoutconditionsL. ThelayoutconditionscompriseN\n               instances{l }N ,whereeachinstancel isdefinedbyitsspatialpositionb andcontentc :\n                     i i=1           i                  i       i                   \n                               L={l ,...,l }, wherel =(c ,b ).      (3)             \n                                   1   N       i  i i                               \n               Inourframework,spatialpositionsarerepresentedasboundingboxes,whileinstancecontentcanbe\n               specifiedthroughmultiplemodalities: textualinstancecontentandadditionalvisualinstancecontent,\n               includingreferenceimages,depthmapsandedgemaps.                       \n               WeproposeInstanceAssemble,aframeworkwithaLayoutEncodertoencodethelayoutconditions\n               andAssemble-MMDiTtoeffectivelyintegratetheencodedlayoutconditionswithimagefeatures.\n               3.1 LayoutEncoder                                                    \n               WeuseaLayoutEncoder(Fig.2left-bottompanel)toencodeeachinstancel ,andthetokensare\n                                                          i                         \n               denotedashL =[hl1,...,hlN]whichrepresentsthelayoutinformationofeachinstance. Giventhe\n               spatialpositionoftheinstance(boundingbox),wefirstenhancethespatialrepresentationthrough\n               DenseSample. Givenaboundingboxb =(x ,y ,w,h)∈[0,1]4withtop-leftcoordinates(x ,y )\n                                    i   1 1                        1 1              \n               andsize(w,h),wegenerateK2uniformlyspacedpoints:                      \n                           (cid:26)(cid:18) (cid:19)(cid:12) (cid:27)               \n                                   w       h (cid:12)                               \n                        P i = x 1+k x· K,y 1+k y· K (cid:12) (cid:12)k x,k y ∈{0,...,K−1} (4)\n               Then,followingGLIGEN[28],wecomputethetextualinstancetokensas:        \n                                 hi =MLP([τ(c ),Fourier(P )]),      (5)             \n                                 l        i      i                                  \n               where τ represents the text encoder, Fourier(·) denotes Fourier embedding [51], [·, ·] denotes\n               concatenationalongthefeaturedimension,andMLPisamulti-layerperception.\n               Additionally,wecanuseadditionalvisualinstancecontenttobetterimproveperformance. Given\n               thevisualinstancecontent,wefirstextractfeaturesusingtheVAEencoderofthebasemodel,then\n               projectthemtotheunifiedinstancetokenspacethroughaMLP:                \n                                    hi =MLP(VAE(c )).               (6)             \n                                    l          i                                    \n               3.2 Assemble-MMDiT                                                   \n               Weobservethatapplyingattentionbetweenallimagetokensandinstancetokensresultsinsuboptimal\n               performanceundercomplexlayoutconditions(e.g.,overlapping,tinyobjects). Toaddressthis,we\n               introduce Assemble-MMDiT (Fig. 2, right-bottom panel), which enhances the location of each\n               instancewhilemaintainingcompositionalcoherencewithotherinstances. Ourmethodprocesses\n               eachinstanceindependentlythroughattentionmoduleswithitsassociatedimagetokens,followedby\n               weightedfeatureassembling.                                           \n               Formally,givenimagetokensh∈RC×W×H (whereC denotesthelatentchannelsizeand[W,H]\n               thelatentsize)andinstancetokenshl ∈ RC×N,weapplyAdaLayerNorm[56],followedbyour\n               proposedAssembling-Attn,asshowninFig.2(right-bottompanel). Wecroptheimagetokenshz by\n               thebboxb ofeachinstanceandgethz =hz[b ]∈RC×w×h. Then,weprojectthecroppedimage\n                     i              li   i                                          \n               tokenshz                                                             \n                    li                                                              \n                     andtheircorrespondinginstancetokensl iintoqueries(Qzli,Qli),keys(Kzli,Kli),and\n               values(Vzli,Vli),andthenapplyattention:                              \n                          hz li′ ,hl i′ =Attention(cid:0) [Qzli,Qli],[Kzli,Kli],[Vzli,Vli](cid:1) . (7)\n                                          4\n\nwhere[·,·]denotesconcatenationalongthetokendimension. Theupdatedtokensareassembled\n               across instances. Let M ∈ NW×H represent the instance density map, calculating the counts of\n               instances.                                                           \n                     Theassembledimagetokenshz′ andinstancetokenshl′                \n                                                    arecomputedas:                  \n                                      N                                             \n                       hz′ :hz′                                                     \n                            [:,i,j]=                                                \n                                   1  X hz′                                         \n                                          [:,i,j], wherei∈[0,W −1],j ∈[0,H −1]      \n                                 M[i,j]  lk                         (8)             \n                                      k=1                                           \n                       hl′ :hl′ [:,k] =hl k′ .                                      \n               As illustrated in Fig. 3, the top row demonstrates that              \n               ourassemblingmechanismensuresinstancetokensattend                    \n               o l                                                                  \n               e                                                                    \n               c                                                                    \n               ee                                                                   \n               xf                                                                   \n                on f                                                                \n                f                                                                   \n                prl t ey                                                            \n                 r                                                                  \n                 lci                                                                \n                 e                                                                  \n                 int                                                                \n                  ct                                                                \n                  co                                                                \n                  i iv                                                              \n                  tb tr                                                             \n                   e                                                                \n                   sl le a                                                          \n                    al                                                              \n                    pl yc ye akv                                                    \n                     otg                                                            \n                     i.a uaun T tlit                                                \n                       d                                                            \n                       ch pi                                                        \n                        e                                                           \n                        oem                                                         \n                        os                                                          \n                        nsma                                                        \n                         tg                                                         \n                         i                                                          \n                         rg                                                         \n                          tl                                                        \n                          oi ie od                                                  \n                          o                                                         \n                          lbd nr (e al bseg                                         \n                            l                                                       \n                            .                                                       \n                            oi pr tIo to nrn ow os mcm, orw re                      \n                                p                                                   \n                                n                                                   \n                                ov th                                               \n                                 t                                                  \n                                 wree                                               \n                                 t                                                  \n                                 aoar )se l                                         \n                                  k                                                 \n                                   ts r,eu et                                       \n                                    n                                               \n                                    g                                               \n                                    sn h                                            \n                                    us                                              \n                                    er ae lntt tl                                   \n                                      o                                             \n                                      sea th rt                                     \n                                       if                                           \n                                       ae ne                                        \n                                       o                                            \n                                       td                                           \n                                        c                                           \n                                        i                                           \n                                        lm                                          \n                                        o                                           \n                                        or ue                                       \n                                         n                                          \n                                         ce sg ac woi lho                           \n                                           in                                       \n                                           za in                                    \n                                           t                                        \n                                            an                                      \n                                            t                                       \n                                            hs                                      \n                                            h                                       \n                                            ti                                      \n                                             o                                      \n                                             ia s                                   \n                                             e                                      \n                                             oumr                                   \n                                             i                                      \n                                              ne                                    \n                                              r                                     \n                                              t                                     \n                                                tuoyal                              \n                                                /w                                  \n                                                 tpmos re pc n la at bs on lI       \n                                                 G                                  \n               e inrr co or ns s( is\" tB er ni cti is eh sS (\"h do or gth \"a mir\" isi sn inw g)r .onglocation)orsemantic tuoyal\n                                                o/w                                 \n                                                      British ShorthairAmerican robin Maltipoo dog water\n               Furthermore,topreservethegenerationcapabilityofthe Figure3: (Top)instance-imageattention\n               originalmodelandmitigateconditionalconflictsbetween mapw/layout. (Middle)globalprompt-\n               global prompt and layout conditions, we employ a cas- imageattentionmapw/layout.(Bottom)\n               cadedmechanismasshowninFig.2(right-abovepanel). globalprompt-imageattentionmapw/o\n               In our design, the global text prompt and image latents layout.      \n               arepassedthroughoriginalMMDiTfirst,thentheimage                      \n               tokens along with instance tokens are processed by our               \n               Assemble-MMDiTmodule. Thefirststepcapturesglobalcontextandensuresgenerationquality,\n               whilethesecondstepensuresinstancelayoutalignment. Besides,wetrainAssemble-MMDiTwith\n               LoRA,significantlyreducingboththetrainingcostandinferencecosts.      \n                      ‘cRoanldld iye                                                \n                       t                                                            \n                       t iCe oac nrt                                                \n                        :’                                                          \n                                                 SAMbox co‘nTdoiwteiro’n: detect    \n                                    detect        co‘nWdinitdioown’:                \n                               ‘Blaccko nsdhiotlidoenr: bag’ detect                 \n                     (a) Rally Car? (b) Black shoulder bag? (c) Window? (d) Tower?  \n                       CropVQA: Yes –> 1 CropVQA: No –> 0 SAMIoU: 0.77 BinaryIoU: 1 \n                       DetectIoU: 0.60 DetectIoU: 0.54 DetectIoU: 0.00 DetectIoU: 0.81\n               Figure4: Failurecasesofothermetrics. (a)falseacceptanceinCropVQA,(b)falserejectionin\n               CropVQA,(c)localizationerrorinSAMIoU,and(d)discontinuousinBinaryIoU. \n               3.3 Benchmark: DenseLayoutandLayoutGroundingScore                    \n               TheLayout-to-Imagetaskaimstogenerateimagesthatalignpreciselywithprovidedlayouts,eval-\n               uatingbothspatialaccuracyandsemanticconsistency(e.g.,color,texture,andshape,ifprovided).\n               Theexistingmetrics(AP/AR)forobjectdetection[10,28,55,53]aresuboptimal. Theyassumea\n               fixedcategorysetandrelyoninappropriateprecision/recallforbinarylayoutoutcomes. VLM-based\n               croppedVQAmethods[61]sufferfalseacceptance(Fig.4(a))andfalserejection(Fig.4(b)). While\n               spatial-onlymetricslikeSAMIoU[11]ignoreappearanceconsistency(Fig.4(c)),GroundingDINO-\n               based[35]binaryIoUthresholds[64,54]failtocapturecontinuouslayoutprecision(Fig.4(d)). Thus,\n               weproposeLayoutGroundingScore(LGS),whichintegratesbothspatialaccuracyandsemantic\n               accuracy:                                                            \n                  1. SpatialAccuracy(DetectIoU):wedetectallinstancesviaanoff-the-shelfdetector[35],\n               computetheIoUagainstconditionbbox, andreporttheglobalmeanIoUacrossallinstancesfor\n               equalweighting.                                                      \n                  2. SemanticAccuracy: forinstanceswithIoU>0.5,wecropthepredictedregionandassessthe\n               semanticaccuracybyitsattributeconsistency(color,texture,shape)viaVLM-basedVQA[60].\n               LGSsupportsopen-setevaluation,usesDetectIoUtoevaluatespatialaccuracyanddecouplesthe\n               spatial and semantic check to avoid CropVQA [61] failures (shown in Fig. 4). Furthermore, we\n                                          5                                         \n\n[表格内容]\n | tuoyal tpmos re pc n la at bs on lI\n/w\nG\ntuoyal\no/w\nBritish ShorthairAmerican robin Maltipoo dog water\nFigure3: (Top)instance-imageattention\nmapw/layout. (Middle)globalprompt-\nmageattentionmapw/layout.(Bottom)\nglobalprompt-imageattentionmapw/o\nayout.\ncontextandensuresgenerationquality,\nsides,wetrainAssemble-MMDiTwith\nrencecosts.\nSAMbox detect\nco‘nTdoiwteiro’n:\nco‘nWdinitdioown’:\ndetect\n(d) Tower?\n0.77 BinaryIoU: 1\nU: 0.00 DetectIoU: 0.81\nnceinCropVQA,(b)falserejectionin\n‘cRoanldld iye t iCe oac nrt co‘nTdoiwteiro’n:\nt :’\ndetect co‘nWdinitdioown’:\n‘Blaccko nsdhiotlidoenr: bag’ detect\n(a) Rally Car? (b) Black shoulder bag? (c) Window? (d) Tower?\nCropVQA: Yes –> 1 CropVQA: No –> 0 SAMIoU: 0.77 BinaryIoU: 1\nDetectIoU: 0.60 DetectIoU: 0.54 DetectIoU: 0.00 DetectIoU: 0.81\nFailurecasesofothermetrics. (a)falseacceptanceinCropVQA,(b)falserejectionin\n,(c)localizationerrorinSAMIoU,and(d)discontinuousinBinaryIoU.\nchmark: DenseLayoutandLayoutGroundingScore\nut-to-Imagetaskaimstogenerateimagesthatalignpreciselywithprovidedlayouts,eval\nhspatialaccuracyandsemanticconsistency(e.g.,color,texture,andshape,ifprovided)\nngmetrics(AP/AR)forobjectdetection[10,28,55,53]aresuboptimal. Theyassumea\ngorysetandrelyoninappropriateprecision/recallforbinarylayoutoutcomes. VLM-based\nQAmethods[61]sufferfalseacceptance(Fig.4(a))andfalserejection(Fig.4(b)). While\nlymetricslikeSAMIoU[11]ignoreappearanceconsistency(Fig.4(c)),GroundingDINO\nbinaryIoUthresholds[64,54]failtocapturecontinuouslayoutprecision(Fig.4(d)). Thus\neLayoutGroundingScore(LGS),whichintegratesbothspatialaccuracyandsemantic\npatialAccuracy(DetectIoU):wedetectallinstancesviaanoff-the-shelfdetector[35]\nheIoUagainstconditionbbox, andreporttheglobalmeanIoUacrossallinstancesfo\nghting.\nmanticAccuracy: forinstanceswithIoU>0.5,wecropthepredictedregionandassessthe\naccuracybyitsattributeconsistency(color,texture,shape)viaVLM-basedVQA[60].\nortsopen-setevaluation,usesDetectIoUtoevaluatespatialaccuracyanddecouplesthe\nd semantic check to avoid CropVQA [61] failures (shown in Fig. 4). Furthermore, we | co‘nTdoiwteiro’n:\nco‘nWdinitdioown’:\ndetect\n(d) Tower?\n0.77 BinaryIoU: 1\nU: 0.00 DetectIoU: 0.81\nnceinCropVQA,(b)falserejectionin\n\n | \nco‘nWdinitdioown’: | \n | detect\n | \n\n\n\n\n | \n | \n\n | detect\n‘Blaccko nsdhiotlidoenr: bag’ | \n |\n\nTable1: QuantitativecomparisonbetweenourSD3-basedInstanceAssembleandotherL2I\n               methodsonLayoutSAM-Eval. ⋆ TheCropVQAscoreisproposedinCreatilayout[61]andthe\n               scoreofInstanceDiff,MIGCandCreatiLayoutisborrowedfromCreatiLayout[61].\n                                    CropVQA⋆  LayoutGroundingScore GlobalQuality    \n                  LayoutSAM-Eval                                                    \n                                spatial↑color↑ texture↑shape↑ mIoU↑ color↑ texture↑shape↑ VQA↑ Pick↑ CLIP↑\n                  RealImages(UpperBound) 98.95 98.45 98.90 98.80 88.85 88.07 88.71 88.62\n                  InstanceDiff(SD1.5) 87.99 69.16 72.78 71.08 78.16 63.14 66.82 65.86 86.42 21.16 11.73\n                  MIGC(SD1.4)   85.66 66.97 71.24 69.06 62.87 50.70 52.99 51.77 88.97 20.69 12.56\n                  HICO(realisticVisionV51) 90.92 69.82 73.25 71.69 70.68 53.16 55.71 54.61 86.53 21.77 9.47\n                  CreatiLayout(SD3-M) 92.67 74.45 77.21 75.93 45.82 38.44 39.68 39.24 92.74 21.71 13.82\n                  InstanceAssemble(ours)(SD3-M) 94.97 77.53 80.72 80.11 78.88 63.89 66.27 65.86 93.12 21.79 12.76\n               Table2: QuantitativecomparisonbetweenourInstanceAssembleandotherL2Imethodson\n               DenseLayout.                                                         \n                                         LayoutGroundingScore GlobalQuality         \n                       DenseLayout                                                  \n                                        mIoU↑ color↑ texture↑shape↑ VQA↑ Pick↑ CLIP↑\n                       RealImages(UpperBound) 92.35 76.52 80.78 79.78               \n                       InstanceDiff(SD1.5) 47.31 29.48 33.36 32.43 88.79 20.87 11.73\n                       MIGC(SD1.4)      34.39 22.10 23.99 23.45 91.18 20.74 12.81   \n                       HICO(realisticVisionV51) 22.42 10.52 11.69 11.46 74.42 20.51 8.16\n                       CreatiLayout(SD3-Medium) 15.54 11.69 12.34 12.17 93.42 21.88 12.89\n                       InstanceAssemble(ours)(SD3-Medium) 52.07 33.77 36.21 35.81 93.54 21.68 12.58\n                       Regional-Flux(Flux.1-Dev) 14.06 11.34 11.91 11.84 92.94 22.67 10.66\n                       RAG(Flux.1-Dev)  17.23 14.22 14.62 14.55 92.16 22.28 11.01   \n                       InstanceAssemble(ours)(Flux.1-Dev) 43.42 27.60 29.50 29.14 93.36 21.98 11.38\n                       InstanceAssemble(ours)(Flux.1-Schnell) 45.33 27.73 30.06 29.62 93.52 21.72 10.78\n               introduceDenseLayout,adenseevaluationdatasetforL2I,whichconsistsof5kimageswith90k\n               instances(18.1perimage). TheimagesinDenseLayoutaregeneratedbyFlux.1-Dev, taggedby\n               RAM++[24],detectedbyGroundingDINO[35],recaptionedbyQwen2.5-VL[43],andfilteredto\n               retainthosewith≥15instances,thusprovidingdenselayoutconditions.      \n               3.4 TrainingandInference                                             \n               Duringtraining,wefreezetheparametersofthebasemodelandonlyupdatetheproposedLayout\n               Encoder and Assemble-MMDiT module. We denote the adding parameters by θ′. The training\n               objectiveisgivenby                                                   \n                          L=E ϵ∼N(0,I),x,t,p,Lh(cid:13) (cid:13)v {θ,θ′}(cid:0) z t,t,p,L(cid:1) −(ϵ−x)(cid:13) (cid:13)2 2i , (9)\n               wherez =(1−t)x+tϵ. Duringinference,layout-conditioneddenoisingisappliedduringthefirst\n                   t                                                                \n               30%ofdiffusionsteps,asthelayoutprimarilyformsinearlystages[28,64].   \n               4 Experiments                                                        \n               4.1 ExperimentalSetup                                                \n               ImplementationDetailsThetextual-onlyInstanceAssembleistrainedonSD3-Medium[15]and\n               Flux.1-Dev[4]andtheversionwithadditionalvisualinstancecontentisonlytrainedonSD3-Medium.\n               WefreezethepretrainedMMDiTbackboneandonlyadapttheLayoutEncoderandLoRAmodules\n               ofAssemble-MMDiT.Assemble-MMDiTisinitializedfrompretrainedweights, andLoRAwith\n               rank=4isapplied. IntheSD3-basedmodelallAssemble-MMDiTblocksareadapted,whileinthe\n               Flux-basedmodelweadapteightblocks(sevendouble-blocksandonesingleblock)duetoresource\n               constraints. Duringinference,theLoRA-basedAssemble-MMDiTisactivatedforthefirst30%of\n               denoisingsteps,whiletheglobalprompt–imagephaseusesthefrozenbackbone. Thisdesignyields\n               71M(SD3-M)and102M(Flux.1-Dev)additionalparametersforthetextual-onlysetting;thevariant\n               withadditionalvisualinstancecontent(SD3-M)introduces85Mparameters. Allmodelsaretrained\n               onLayoutSAM[61]at1024×1024withProdigy,for380Kiterations(batchsize2)onSD3-Mand\n                                          6\n\nTable4: Parameteradditionandtimeefficiencyundersparseanddenselayoutconditions. We\n               evaluatedon10%oftheLayoutSAM-EvalandDenseLayoutdatasetsat1024×1024resolution. ⋆ are\n               optimizedfor512×512resolution,sotheirresultsarereportedatthisscale.  \n                                     ParameterAddition TimeEfficiency(s)(relativeruntimeincrease(%))\n                                   (relativeparameteraddition(%)) SparseLayout DenseLayout\n                 InstanceDiff⋆(SD1.5)     369M(43%) 14.37(+771%) 44.81(+2754%)      \n                 MIGC(SD1.4)              57M(6.64%) 14.41(+25.4%) 21.58(+87.5%)    \n                 HICO⋆(realisticVisionV51) 361M(33.9%) 4.11(+92.9%) 9.93(+320%)     \n                 CreatiLayout(SD3-M)      1.2B(64.0%) 4.37(+14.4%) 4.42(+14.8%)     \n                 Regional-Flux(Flux.1-Dev)     - 15.29(+113%) 37.47(+418%)          \n                 RAG(Flux.1-Dev)               - 15.69(+119%) 21.14(+192%)          \n                 InstanceAssemble(ours)(SD3-M) 71M(3.46%) 7.19(+88.2%) 13.38(+248%) \n                 InstanceAssemble(ours)(Flux.1-Dev) 102M(0.84%) 8.21(+14.3%) 10.28(+41.9%)\n                 InstanceAssemble(ours)(Flux.1-Schnell) 102M(0.84%) 1.41(+8.46%) 1.70(+28.8%)\n               300Kiterations(batchsize1)onFlux.1-Dev,using8×H800GPUs(7daysforSD3-M;5daysfor\n               Flux.1-Dev).                                                         \n               EvaluationDatasetWeuseLayoutSAM-Eval[61]toevaluateperformanceonfine-grainedopen-set\n               sparseL2Idataset,containing5kimagesand19kinstancesintotal(3.8instancesperimage). To\n               assess performance on fine-grained open-set dense L2I evaluation dataset, we use the proposed\n               DenseLayout, which consists of 5kimages and 90k instancesin total (18.1 instancesper image).\n               Followingconventionalpractice,wealsoevaluateoncoarse-grainedclose-setL2Ievaluationdataset\n               COCO[31]. WecombineCOCO-StuffandCOCO-InstanceannotationstocreateourCOCO-Layout\n               evaluationdataset,containing5kimagesand57kinstancesintotal(11.5instancesperimage).\n               EvaluationMetricWeevaluatetheaccuracyofL2IgenerationusingourproposedLGSmetricalong\n               withCropVQAproposedbyCreatiLayout[61],measuringspatialandsemanticaccuracy. Wealso\n               employmultipleestablishedmetricstomeasureoverallimagequalityandglobalpromptalignment,\n               includingVQAScore[32],PickScore[26]andCLIPScore[21].                 \n               4.2 EvaluationonL2IwithTextual-OnlyContent                           \n               Fine-GrainedOpen-SetSparseL2IGenerationTab.1presentsthequantitativeresultsofInstance-\n               Assemble on LayoutSAM-Eval [61], reporting results using our proposed LGS, CropVQA [61]\n               andglobalqualitymetrics. OurproposedInstanceAssemblenotonlyachievesSOTAinspatialand\n               semanticaccuracyofeachinstance,butalsodemonstratessuperiorglobalquality.\n               Fine-GrainedOpen-SetDenseL2IGener-                                   \n               ation Tab. 2 presents results on DenseLay- Table 3: Comparison between our SD3-based\n               out. WiththesameSD3-Mediumbackbone, InstanceAssemble and other L2I methods on\n               InstanceAssemble significantly outperforms COCO-Layout. SinceCOCOdon\"thavedetailed\n               CreatiLayout(mIoU:52.07vs. 15.54)while descriptionforeachinstance,wecannotevaluatethe\n               maintaining comparable global quality. On attributeaccuracyandonlyreportthespatialaccu-\n               Flux.1,italsoyieldslargegainsoverRegional- racy-mIoU.                \n               FluxandRAG(e.g.,mIoU:43.42vs.17.23for                                \n               RAG),showingthatourcascadedAssemble-      LGS GlobalQuality          \n                                          COCO-Layout                               \n               Attndesigngeneralizeswellacrossbackbones. mIoU↑ VQA↑ Pick↑ CLIP↑     \n               ComparedtoearlierUNet-basedapproaches RealImages(UpperBound) 49.14   \n               suchasInstanceDiffandMIGC,ourmethod                                  \n                                          InstanceDiff(SD1.5) 30.39 75.77 20.75 24.41\n               achieveshigherspatialandsemanticaccuracy MIGC(SD1.4) 27.36 70.32 20.20 23.58\n               (mIoU:52.07vs.47.31)withoutsacrificingre- HICO(realisticVisionV51) 18.88 50.61 20.38 20.72\n                                          CreatiLayout(SD3-M) 7.12 87.79 21.22 25.59\n               alism. Overall,InstanceAssembleestablishes                           \n                                          InstanceAssemble(ours)(SD3-M) 27.85 89.06 21.58 25.68\n               consistentimprovementsinlayoutalignment                              \n               whileensuringhighimagequalityunderchal-                              \n               lengingdenselayouts.                                                 \n               Coarse-GrainedClosed-SetL2IGenerationTab.3presentsthequantitativeresultofInstance-\n               AssembleonCOCO-Layout. OurproposedInstanceAssemblesurpassespreviousmethodsinoverall\n               imagequalitybutlagsslightlybehindInstanceDiff[53]inlayoutprecision. Weattributethisgapto:\n                                          7\n\nLayout InstanceDiff MIGC HICO CreatiLayout Regional-Flux RAG Ours  \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                    Figure5: QualitativecomparisonofInstanceAssemblewithothermethods.\n                                                                                    \n                                                                                    \n               (i)InstanceDiff’sfine-grainedCOCOtrainingdatawithper-entityattributeannotations,and(ii)its\n               entity-wisegenerationstrategy,whichimprovesprecisionatsignificantcomputationalcost(Tab.4).\n               QualitativeComparisonThecomparativeresultsinFig.5                    \n               demonstrate that our proposed InstanceAssemble method                \n                                                                  text              \n               achieves superior spatial precision and instance-caption 4te 6.x 1ture text+image\n                                                                  text+depth        \n               alignment compared to baseline methods. For example, 4 3 . 7color text+edge\n                                                        34.6                        \n               inthethirdrow,bothInstanceDiff[53]andMIGC[64]gen- 32.8               \n               eratemorethanoneshoes; HICO[9]failstogeneratethe shape 23.0 21.9     \n                                                  45.8                              \n               specifiedNewBalanceshoe;Regional-Flux[5]doesnotad- 34.4 22.9 11.5 10.9\n               here to the layout conditions; and the shoe generated by 11.5 25.2 35.6 46.0 5 6 m.4 IoU\n               RAG[8]isnotproperlyfusedwiththebackground. Incon-                    \n                                                        84.9                        \n                                                           10.7                     \n               trast,ourmethodgeneratesthecorrectinstance,accurately 87.9 20.0      \n                                                    90.8    11.7                    \n               placedandseamlesslyintegratedwiththescene. VQA                       \n                                                  93.7   20.7 12.7                  \n               Time Efficiency and Parameter Addition We compare 21.4 13.7          \n                                                               CLIP                 \n               timeefficiencyandparameteradditionwithotherL2Imeth-                  \n                                                       Pick2 2 .1                   \n               ods,asshowninTab.4. OurmethodachievesSOTAperfor-                     \n               manceonlayoutalignmentwithacceptabletimeefficiency Figure6: QuantitativeresultsofIn-\n               andminimalparameteraddition.      stanceAssemble with additional vi- \n                                                 sualinstancecontent.               \n               4.3 EvaluationonL2IwithAdditionalVisualContent                       \n               Weevaluatethreeadditionalvisualinstancecontent: image,               \n               depth,andedge(seeFig.6). Unsurprisingly,usingimage                   \n               asadditionalinstancecontentyieldsthebestperformance, asitprovidesrichvisualinformation.\n               Although depth and edge capture texture and shape features, their performance remains inferior\n               comparedtoimageinstancecontent. Nevertheless,visualmodalitiesoutperformtextual-onlyinstance\n                                          8                                         \n\n[表格内容]\n\nTable5: AblationstudyonourproposedcomponentsonDenseLayout. \"Assemble\"referstothe\n               presenceoftheAssemble-Attnmodule(architecturaldesign). \"Cascaded\"indicatestheinteraction\n               order: (✔)meansglobalprompt–imageinteractionfollowedbyinstance–imageinteraction(cascaded\n               structure),while(✘)meansbothareappliedinparallel. \"LoRA\"specifiesthetrainingstrategyforthe\n               Assemble-MMDiTmodule: (✔)indicatestrainingwithLoRA,while(✘)indicatesfullfine-tuning.\n               \"DenseSample\"denoteswhethertheDenseSamplespatialencodingisused.      \n                                               LayoutGroundingScore                 \n                        Assemble Cascaded LoRA DenseSample                          \n                                              mIoU↑ color↑ texture↑shape↑ VQA↑      \n                         ✘     ✘    ✘    ✘    11.69 9.16 9.68 9.56 93.75            \n                         ✔     ✘    ✘    ✘    43.98 24.19 26.95 26.75 84.57         \n                         ✔     ✔    ✘    ✘    45.96 29.61 31.50 31.09 92.71         \n                         ✔     ✔   ✔     ✘    51.28 32.68 34.94 34.58 93.33         \n                         ✔     ✔   ✔     ✔    52.07 33.77 36.21 35.81 93.54         \n               content. Qualitativecomparisons(Fig.7)furtherdemonstratethatvisualinstancecontentleadsto\n               superiortextureandshapealignmentcomparedtotext.                      \n                   Layout     Text+Image Text+Depth  Text+Edge Text                 \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                Figure7: QualitativeresultsofInstanceAssemblewithadditionalvisualinstancecontent.\n                                                                                    \n                                                                                    \n               4.4 AblationStudy                                                    \n                                                                                    \n               We evaluated the contribution of each proposed component on SD3-M based InstanceAssemble\n               inTab.5. Thebasemodel(SD3-Mediumwithoutadditionalmodules)yieldsaverylowLayout\n               Grounding Score, indicating poor layout and content control when instance information is not\n               explicitlymodeled. TheintroductionofAssemble-Attnmoduleelevatesspatialaccuracy(mIoUto\n               43.98)andboostssemanticmetrics(color/texture/shapeto24.19/26.95/26.75). Thecascadeddesign\n               (✔:prompt–imagefollowedbyinstance–image;✘:parallel)resolvesglobalqualitydegradationwhile\n               maintaininglayoutalignment.UsingLoRAtotrainAssemble-MMDiTimprovesperformancefortwo\n               reasons: (1)itretainsthebasemodel’scapabilitiescomparedtothefullyfine-tunedversion,and(2)\n               itenableseffectivelayoutcontrolwithfarfewertrainableparametersbyintroducingonlylightweight\n               low-rankmatricesonattentionprojections. Finally,DenseSamplefurtherenhancesspatialaccuracy,\n               instancesemanticaccuracyandimagequality. Together,theserefinementsprogressivelycollectively\n               optimizelayouttoimagemodelingwithoutcompromisinggenerationability.   \n               4.5 Applications                                                     \n               WedemonstratethatInstanceAssembleisversatileandapplicabletovarioustasks. Itseamlessly\n               integrateswithdomain-specificLoRAmodulesformulti-domainstyletransferwhilemaintaining\n               layoutconsistency,asshowninFig.8. OurproposedInstanceAssemblecancooperatewithdistilled\n               modelssuchasFlux.1-Schnell[4],asillustratedinTab.2,achievinggeometriclayoutcontroland\n               detailedsynthesis. Ourapproachdemonstratesbothstyleadaptabilityandcomputationalefficiency,\n               makingitwell-suitedforcontrollablegenerativedesignapplications.      \n                                          9                                         \n                                                                                    \n                                                                                    \n\n[表格内容]\n\nD3etUC                                                              \n                                                                                    \n                                                                                    \n                gnitniap                                                            \n                                                                                    \n                liO                                                                 \n                                                                                    \n                                                                                    \n                ilbihG                                                              \n                                                                                    \n                                                                                    \n               Figure8: TheadaptionofCute3D[52]/OilPainting[39]/Ghibli[38]LoRAwithourmethods.\n               The proposed InstanceAssemble successfully adapts diverse style lora and maintaining superior\n               layoutalignment.                                                     \n                                                                                    \n               5 Conclusion                                                         \n                                                                                    \n               We present InstanceAssemble, a novel approach for Layout-to-Image generation. Our method\n               achievesstate-of-the-artlayoutalignmentwhilemaintaininghigh-qualitygenerationcapabilitiesof\n               DiT-basedarchitectures. WevalidateInstanceAssembleacrosstextualinstancecontentandadditional\n               visualinstancecontent,demonstratingitsversatilityandrobustness. Ourlayoutcontrolschemealso\n               successfullyadaptsdiversestyleLoRAswhilemaintainingsuperiorlayoutalignment,demonstrating\n               cross-domaingeneralizationcapability. Futhermore,weintroduceLayoutGroundingScoremetric\n               andaDenseLayoutevaluationdatasettovalidateperformanceundercomplexlayoutconditions.\n               LimitationsandFutureWorkWhileourworkadvancescontrollablegenerationbyunifyingprecise\n               layoutcontrolwiththeexpressivepowerofdiffusionmodels,severallimitationsremain. First,our\n               designcurrentlyrequiressequentialAssemble-MMDiTcalls,whichmayincurinefficiency;exploring\n               parallelizationstrategiesisanimportantdirection. Second,althoughourapproachiseffectiveundera\n               widerangeoflayouts,imagefidelitycandegradeinextremelydenseorhighlycomplexcases.\n               BroaderImpactsInstanceAssembleexpandsthefrontierofstructuredvisualsynthesisbyproviding\n               fine-grainedlayoutcontrolandhigh-qualitymultimodalgeneration. However,itspowerfulgenerative\n               capabilities may also introduce risks. In particular, malicious use could enable the creation of\n               misleadingordeceptivelayouts,exacerbatingthespreadofdisinformation. Themodelmayalso\n               raiseprivacyconcernsifappliedtosensitivedata,andlikemanygenerativesystems,itinheritsand\n               mayamplifysocietalbiasespresentintrainingcorpora. Weencourageresponsibledeploymentand\n               continuedinvestigationintosafeguardsthatmitigatetheseriskswhileenablingbeneficialapplications\n               indesign,education,andaccessibility.                                 \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                          10\n\nAcknowledgmentsandDisclosureofFunding                                \n               ThisresearchwassupportedbytheNationalNaturalScienceFoundationofChina(NSFC62576103,\n               62176059). ThecomputationswereconductedusingtheCFFFplatformatFudanUniversity. Partof\n               thisworkwascarriedoutduringaninternshipatXiaohongshu.                \n                                                                                    \n               References                                                           \n                                                                                    \n                [1] F.Bao,S.Nie,K.Xue,Y.Cao,C.Li,H.Su,andJ.Zhu. AllareWorthWords: AViTBackbone\n                  forDiffusionModels. InProceedingsoftheIEEE/CVFConferenceonComputerVisionand\n                  PatternRecognition,pages22669–22679.IEEE,2023.                    \n                [2] O.Bar-Tal, L.Yariv, Y.Lipman, andT.Dekel. MultiDiffusion: FusingDiffusionPathsfor\n                  Controlled Image Generation. In Proceedings of the International Conference on Machine\n                  Learning,volume202ofProceedingsofMachineLearningResearch,pages1737–1752.PMLR,\n                  2023.                                                             \n                [3] J.Betker,G.Goh,L.Jing,T.Brooks,J.Wang,L.Li,L.Ouyang,J.Zhuang,J.Lee,Y.Guo,\n                  etal. Improvingimagegenerationwithbettercaptions,2023.            \n                [4] BlackForestLabs. Flux. https://github.com/black-forest-labs/flux,2024.\n                                                                                    \n                [5] A.Chen,J.Xu,W.Zheng,G.Dai,Y.Wang,R.Zhang,H.Wang,andS.Zhang. Training-free\n                  RegionalPromptingforDiffusionTransformers,2024.URLhttps://arxiv.org/abs/2411.\n                  02395.                                                            \n                [6] J.Chen,J.Yu,C.Ge,L.Yao,E.Xie,Z.Wang,J.T.Kwok,P.Luo,H.Lu,andZ.Li. PixArt-\n                  α: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis. In\n                  ProceedingsoftheInternationalConferenceonLearningRepresentations.OpenReview.net,\n                  2024.                                                             \n                [7] M.Chen,I.Laina,andA.Vedaldi.Training-FreeLayoutControlwithCross-AttentionGuidance.\n                  InWinterConferenceonApplicationsofComputerVision,pages5331–5341.IEEE,2024.\n                [8] Z. Chen, Y. Li, H. Wang, Z. Chen, Z. Jiang, J. Li, Q. Wang, J. Yang, and Y. Tai. Region-\n                  AwareText-to-ImageGenerationviaHardBindingandSoftRefinement,2024. URLhttps:\n                  //arxiv.org/abs/2411.06558.                                       \n                [9] B.Cheng,Y.Ma,L.Wu,S.Liu,A.Ma,X.Wu,D.Leng,andY.Yin. HiCo: HierarchicalCon-\n                  trollableDiffusionModelforLayout-to-imageGeneration. InAdvancesinNeuralInformation\n                  ProcessingSystems,2024.                                           \n                                                                                    \n               [10] J.Cheng,X.Liang,X.Shi,T.He,T.Xiao,andM.Li. LayoutDiffuse: AdaptingFoundational\n                  DiffusionModelsforLayout-to-ImageGeneration,2023. URLhttps://arxiv.org/abs/\n                  2302.08908.                                                       \n               [11] J. Cheng, Z. Zhao, T. He, T. Xiao, Z. Zhang, and Y. Zhou. Rethinking The Training And\n                  EvaluationofRich-ContextLayout-to-ImageGeneration. InAdvancesinNeuralInformation\n                  ProcessingSystems,2024.                                           \n               [12] G. Couairon, M. Careil, M. Cord, S. Lathuilière, and J. Verbeek. Zero-shot spatial layout\n                  conditioningfortext-to-imagediffusionmodels. InProceedingsoftheIEEE/CVFInternational\n                  ConferenceonComputerVision,pages2174–2183.IEEE,2023.              \n               [13] O. Dahary, O. Patashnik, K. Aberman, and D. Cohen-Or. Be Yourself: Bounded Attention\n                  for Multi-subject Text-to-Image Generation. In Proceedings of the European Conference\n                  onComputerVision, volume15072ofLectureNotesinComputerScience, pages432–448.\n                  Springer,2024.                                                    \n               [14] deweiZhou,J.Xie,Z.Yang,andY.Yang. 3DIS:Depth-DrivenDecoupledImageSynthesis\n                  forUniversalMulti-InstanceGeneration. InProceedingsoftheInternationalConferenceon\n                  LearningRepresentations,2025. URLhttps://openreview.net/forum?id=MagmwodCAB.\n                                                                                    \n                                          11\n\n[15] P.Esser,S.Kulal,A.Blattmann,R.Entezari,J.Müller,H.Saini,Y.Levi,D.Lorenz,A.Sauer,\n                  F.Boesel,D.Podell,T.Dockhorn,Z.English,andR.Rombach. ScalingRectifiedFlowTrans-\n                  formersforHigh-ResolutionImageSynthesis. InProceedingsoftheInternationalConference\n                  onMachineLearning.OpenReview.net,2024.                            \n               [16] Y. Feng, B. Gong, D. Chen, Y. Shen, Y. Liu, and J. Zhou. Ranni: Taming Text-to-Image\n                  DiffusionforAccurateInstructionFollowing. InProceedingsoftheIEEE/CVFConferenceon\n                  ComputerVisionandPatternRecognition,pages4744–4753.IEEE,2024.     \n               [17] P.Gao,L.Zhuo,D.Liu,R.Du,X.Luo,L.Qiu,Y.Zhang,C.Lin,R.Huang,S.Geng,R.Zhang,\n                  J.Xi,W.Shao,Z.Jiang,T.Yang,W.Ye,H.Tong,J.He,Y.Qiao,andH.Li. Lumina-T2X:\n                  TransformingTextintoAnyModality,Resolution,andDurationviaFlow-basedLargeDiffusion\n                  Transformers,2024. URLhttps://arxiv.org/abs/2405.05945.           \n               [18] B.Gong,S.Huang,Y.Feng,S.Zhang,Y.Li,andY.Liu. Check,Locate,Rectify: ATraining-\n                  FreeLayoutCalibrationSystemforText-to-ImageGeneration.InProceedingsoftheIEEE/CVF\n                  ConferenceonComputerVisionandPatternRecognition,pages6624–6634.IEEE,2024.\n               [19] R.He,B.Cheng,Y.Ma,Q.Jia,S.Liu,A.Ma,X.Wu,L.Wu,D.Leng,andY.Yin. PlanGen:\n                  TowardsUnifiedLayoutPlanningandImageGenerationinAuto-RegressiveVisionLanguage\n                  Models,2025. URLhttps://arxiv.org/abs/2503.10127.                 \n               [20] Y.He,R.Salakhutdinov,andJ.Z.Kolter. Localizedtext-to-imagegenerationforfreeviacross\n                  attentioncontrol,2023. URLhttps://arxiv.org/abs/2306.14636.       \n               [21] J.Hessel,A.Holtzman,M.Forbes,R.L.Bras,andY.Choi. CLIPScore: AReference-free\n                  EvaluationMetricforImageCaptioning. InProceedingsoftheEmpiricalMethodsinNatural\n                  LanguageProcessing,pages7514–7528.AssociationforComputationalLinguistics,2021.\n               [22] J.Ho,A.Jain,andP.Abbeel. DenoisingDiffusionProbabilisticModels. InNeurIPS,2020.\n                                                                                    \n               [23] E.J.Hu,Y.Shen,P.Wallis,Z.Allen-Zhu,Y.Li,S.Wang,L.Wang,andW.Chen. LoRA:Low-\n                  RankAdaptationofLargeLanguageModels. InProceedingsoftheInternationalConference\n                  onLearningRepresentations.OpenReview.net,2022.                    \n               [24] X.Huang,Y.-J.Huang,Y.Zhang,W.Tian,R.Feng,Y.Zhang,Y.Xie,Y.Li,andL.Zhang.\n                  Open-SetImageTaggingwithMulti-GrainedTextSupervision,2023. URLhttps://arxiv.\n                  org/abs/2310.15200.                                               \n               [25] C.Jia,M.Luo,Z.Dang,G.Dai,X.Chang,M.Wang,andJ.Wang. SSMG:Spatial-Semantic\n                  MapGuidedDiffusionModelforFree-FormLayout-to-ImageGeneration. InProceedingsof\n                  theAAAIConferenceonArtificialIntelligence,pages2480–2488.AAAIPress,2024.\n               [26] Y.Kirstain,A.Polyak,U.Singer,S.Matiana,J.Penna,andO.Levy. Pick-a-Pic: AnOpen\n                  DatasetofUserPreferencesforText-to-ImageGeneration. InAdvancesinNeuralInformation\n                  ProcessingSystems,2023.                                           \n               [27] Y.Lee,T.Yoon,andM.Sung. GrounDiT:GroundingDiffusionTransformersviaNoisyPatch\n                  Transplantation. InAdvancesinNeuralInformationProcessingSystems,2024.\n               [28] Y.Li,H.Liu,Q.Wu,F.Mu,J.Yang,J.Gao,C.Li,andY.J.Lee. GLIGEN:Open-SetGrounded\n                  Text-to-ImageGeneration. InProceedingsoftheIEEE/CVFConferenceonComputerVision\n                  andPatternRecognition,pages22511–22521.IEEE,2023.                 \n               [29] Y. Li, M. Keuper, D. Zhang, and A. Khoreva. Adversarial Supervision Makes Layout-to-\n                  ImageDiffusionModelsThrive. InProceedingsoftheInternationalConferenceonLearning\n                  Representations.OpenReview.net,2024.                              \n                                                                                    \n               [30] Z.Li, J.Zhang, Q.Lin, J.Xiong, Y.Long, X.Deng, Y.Zhang, X.Liu, M.Huang, Z.Xiao,\n                  D.Chen,J.He,J.Li,W.Li,C.Zhang,R.Quan,J.Lu,J.Huang,X.Yuan,X.Zheng,Y.Li,\n                  J.Zhang,C.Zhang,M.Chen,J.Liu,Z.Fang,W.Wang,J.Xue,Y.Tao,J.Zhu,K.Liu,S.Lin,\n                  Y.Sun,Y.Li,D.Wang,M.Chen,Z.Hu,X.Xiao,Y.Chen,Y.Liu,W.Liu,D.Wang,Y.Yang,\n                  J.Jiang,andQ.Lu. Hunyuan-DiT:APowerfulMulti-ResolutionDiffusionTransformerwith\n                  Fine-GrainedChineseUnderstanding,2024. URLhttps://arxiv.org/abs/2405.08748.\n                                          12\n\n[31] T.Lin,M.Maire,S.J.Belongie,J.Hays,P.Perona,D.Ramanan,P.Dollár,andC.L.Zitnick.\n                  MicrosoftCOCO:CommonObjectsinContext. InProceedingsoftheEuropeanConferenceon\n                  ComputerVision,volume8693ofLectureNotesinComputerScience,pages740–755.Springer,\n                  2014.                                                             \n               [32] Z. Lin, D. Pathak, B. Li, J. Li, X. Xia, G. Neubig, P. Zhang, and D. Ramanan. Evaluating\n                  Text-to-VisualGenerationwithImage-to-TextGeneration. InProceedingsoftheEuropean\n                  ConferenceonComputerVision,volume15067ofLectureNotesinComputerScience,pages\n                  366–384.Springer,2024.                                            \n               [33] Y.Lipman,R.T.Q.Chen,H.Ben-Hamu,M.Nickel,andM.Le. Flowmatchingforgenera-\n                  tivemodeling. InProceedingsoftheInternationalConferenceonLearningRepresentations.\n                  OpenReview.net,2023.                                              \n               [34] B.Liu,E.Akhgari,A.Visheratin,A.Kamko,L.Xu,S.Shrirao,C.Lambert,J.Souza,S.Doshi,\n                  and D. Li. Playground v3: Improving Text-to-Image Alignment with Deep-Fusion Large\n                  LanguageModels,2024. URLhttps://arxiv.org/abs/2409.10695.         \n               [35] S.Liu,Z.Zeng,T.Ren,F.Li,H.Zhang,J.Yang,Q.Jiang,C.Li,J.Yang,H.Su,J.Zhu,and\n                  L.Zhang. GroundingDINO:MarryingDINOwithGroundedPre-trainingforOpen-SetObject\n                  Detection. InProceedingsoftheEuropeanConferenceonComputerVision,volume15105of\n                  LectureNotesinComputerScience,pages38–55.Springer,2024.           \n               [36] Z. Lv, Y. Wei, W. Zuo, and K. K. Wong. PLACE: Adaptive Layout-Semantic Fusion for\n                  SemanticImageSynthesis. InProceedingsoftheIEEE/CVFConferenceonComputerVision\n                  andPatternRecognition,pages9264–9274.IEEE,2024.                   \n                                                                                    \n               [37] M.Ohanyan,H.Manukyan,Z.Wang,S.Navasardyan,andH.Shi. Zero-Painter: Training-Free\n                  LayoutControlforText-to-ImageSynthesis. InProceedingsoftheIEEE/CVFConferenceon\n                  ComputerVisionandPatternRecognition,pages8764–8774.IEEE,2024.     \n               [38] openfree. flux-chatgpt-ghibli-lora. https://huggingface.co/openfree/\n                  flux-chatgpt-ghibli-lora,2025.                                    \n               [39] PatrickStarrrr. FLUX - Oil painting. https://civitai.com/models/1455014/\n                  chatgpt-4o-renderer?modelVersionId=1697982,2024.                  \n               [40] W.PeeblesandS.Xie. ScalableDiffusionModelswithTransformers. InProceedingsofthe\n                  IEEE/CVFInternationalConferenceonComputerVision,pages4172–4182.IEEE,2023.\n                                                                                    \n               [41] Q.Phung,S.Ge,andJ.Huang. GroundedText-to-ImageSynthesiswithAttentionRefocusing.\n                  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n                  pages7932–7942.IEEE,2024.                                         \n               [42] D.Podell,Z.English,K.Lacey,A.Blattmann,T.Dockhorn,J.Müller,J.Penna,andR.Rom-\n                  bach. SDXL:ImprovingLatentDiffusionModelsforHigh-ResolutionImageSynthesis. In\n                  ProceedingsoftheInternationalConferenceonLearningRepresentations.OpenReview.net,\n                  2024.                                                             \n               [43] QwenTeam. Qwen2.5-VL,January2025. URLhttps://qwenlm.github.io/blog/qwen2.\n                  5-vl/.                                                            \n                                                                                    \n               [44] A.Ramesh,P.Dhariwal,A.Nichol,C.Chu,andM.Chen. HierarchicalText-ConditionalImage\n                  GenerationwithCLIPLatents,2022. URLhttps://arxiv.org/abs/2204.06125.\n               [45] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-Resolution Image\n                  Synthesis with Latent Diffusion Models. In Proceedings of the IEEE/CVF Conference on\n                  ComputerVisionandPatternRecognition,pages10674–10685.IEEE,2022.   \n               [46] O.Ronneberger,P.Fischer,andT.Brox. U-Net: ConvolutionalNetworksforBiomedicalImage\n                  Segmentation. InMedicalImageComputingandComputer-AssistedIntervention,volume9351\n                  ofLectureNotesinComputerScience,pages234–241.Springer,2015.       \n                                                                                    \n                                          13\n\n[47] C.Saharia,W.Chan,S.Saxena,L.Li,J.Whang,E.L.Denton,S.K.S.Ghasemipour,R.G.\n                  Lopes,B.K.Ayan,T.Salimans,J.Ho,D.J.Fleet,andM.Norouzi.PhotorealisticText-to-Image\n                  DiffusionModelswithDeepLanguageUnderstanding. InAdvancesinNeuralInformation\n                  ProcessingSystems,2022.                                           \n               [48] T.ShirakawaandS.Uchida. NoiseCollage: ALayout-AwareText-to-ImageDiffusionModel\n                  Based on Noise Cropping and Merging. In Proceedings of the IEEE/CVF Conference on\n                  ComputerVisionandPatternRecognition,pages8921–8930.IEEE,2024.     \n               [49] stability.ai. Stable Diffusion 3.5. https://stability.ai/news/  \n                  introducing-stable-diffusion-3-5,Nov2024.                         \n               [50] A.Taghipour,M.Ghahremani,M.Bennamoun,A.M.Rekavandi,H.Laga,andF.Boussaid.\n                  BoxIttoBindIt: UnifiedLayoutControlandAttributeBindinginT2IDiffusionModels,2024.\n                  URLhttps://arxiv.org/abs/2402.17910.                              \n                                                                                    \n               [51] M.Tancik,P.P.Srinivasan,B.Mildenhall,S.Fridovich-Keil,N.Raghavan,U.Singhal,R.Ra-\n                  mamoorthi,J.T.Barron,andR.Ng. FourierFeaturesLetNetworksLearnHighFrequency\n                  Functions in Low Dimensional Domains. In Advances in Neural Information Processing\n                  Systems,2020.                                                     \n               [52] vjleoliu. ChatGPT-4o Renderer. https://civitai.com/models/1455014/\n                  chatgpt-4o-renderer?modelVersionId=1697982,2025.                  \n               [53] X.Wang,T.Darrell,S.S.Rambhatla,R.Girdhar,andI.Misra. InstanceDiffusion: Instance-\n                  LevelControlforImageGeneration. InProceedingsoftheIEEE/CVFConferenceonComputer\n                  VisionandPatternRecognition,pages6232–6242.IEEE,2024.             \n               [54] Y.Wu,X.Zhou,B.Ma,X.Su,K.Ma,andX.Wang. IFAdapter: InstanceFeatureControlfor\n                  GroundedText-to-ImageGeneration,2024. URLhttps://arxiv.org/abs/2409.08240.\n                                                                                    \n               [55] J.Xie,Y.Li,Y.Huang,H.Liu,W.Zhang,Y.Zheng,andM.Z.Shou. BoxDiff: Text-to-Image\n                  SynthesiswithTraining-FreeBox-ConstrainedDiffusion. InProceedingsoftheIEEE/CVF\n                  InternationalConferenceonComputerVision,pages7418–7427.IEEE,2023. \n               [56] J.Xu,X.Sun,Z.Zhang,G.Zhao,andJ.Lin.UnderstandingandImprovingLayerNormalization.\n                  InAdvancesinNeuralInformationProcessingSystems,pages4383–4393,2019.\n               [57] H.Xue,Z.Huang,Q.Sun,L.Song,andW.Zhang. FreestyleLayout-to-ImageSynthesis. In\n                  ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages\n                  14256–14266.IEEE,2023.                                            \n               [58] B.Yang,Y.Luo,Z.Chen,G.Wang,X.Liang,andL.Lin.LAW-Diffusion:ComplexSceneGen-\n                  erationbyDiffusionwithLayouts. InProceedingsoftheIEEE/CVFInternationalConference\n                  onComputerVision,pages22612–22622.IEEE,2023.                      \n                                                                                    \n               [59] Z. Yang, J. Wang, Z. Gan, L. Li, K. Lin, C. Wu, N. Duan, Z. Liu, C. Liu, M. Zeng, and\n                  L.Wang.ReCo:Region-ControlledText-to-ImageGeneration.InProceedingsoftheIEEE/CVF\n                  ConferenceonComputerVisionandPatternRecognition,pages14246–14255.IEEE,2023.\n               [60] Y.Yao, T.Yu, A.Zhang, C.Wang, J.Cui, H.Zhu, T.Cai, H.Li, W.Zhao, Z.He, Q.Chen,\n                  H. Zhou, Z. Zou, H. Zhang, S. Hu, Z. Zheng, J. Zhou, J. Cai, X. Han, G. Zeng, D. Li,\n                  Z. Liu, and M. Sun. MiniCPM-V: A GPT-4V Level MLLM on Your Phone, 2024. URL\n                  https://arxiv.org/abs/2408.01800.                                 \n               [61] H.Zhang,D.Hong,Y.Wang,J.Shao,X.Wu,Z.Wu,andY.-G.Jiang. CreatiLayout: Siamese\n                  Multimodal Diffusion Transformer for Creative Layout-to-Image Generation, 2025. URL\n                  https://arxiv.org/abs/2412.03859.                                 \n               [62] L.Zhang,A.Rao,andM.Agrawala. Addingconditionalcontroltotext-to-imagediffusion\n                  models. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,pages\n                  3813–3824.IEEE,2023.                                              \n                                                                                    \n                                          14\n\n[63] G.Zheng,X.Zhou,X.Li,Z.Qi,Y.Shan,andX.Li. LayoutDiffusion: ControllableDiffusion\n                  Model for Layout-to-Image Generation. In Proceedings of the IEEE/CVF Conference on\n                  ComputerVisionandPatternRecognition,pages22490–22499.IEEE,2023.   \n               [64] D.Zhou,Y.Li,F.Ma,X.Zhang,andY.Yang. MIGC:Multi-InstanceGenerationControllerfor\n                  Text-to-ImageSynthesis. InProceedingsoftheIEEE/CVFConferenceonComputerVisionand\n                  PatternRecognition,pages6818–6828.IEEE,2024.                      \n               [65] D. Zhou, Y. Li, F. Ma, Z. Yang, and Y. Yang. MIGC++: Advanced Multi-Instance Gener-\n                  ationControllerforImageSynthesis. IEEETransactionsonPatternAnalysisandMachine\n                  Intelligence,47(3):1714–1728,2025.                                \n               [66] D. Zhou, J. Xie, Z. Yang, and Y. Yang. 3DIS-FLUX: simple and efficient multi-instance\n                  generationwithDiTrendering,2025. URLhttps://arxiv.org/abs/2501.05131.\n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                          15\n\nSupplementary   Material                              \n                                                                                    \n                                                                                    \n               A  Early-StageLayoutControlinAssemble-MMDiT                          \n                                                                                    \n               WeapplytheAssemble-MMDiTlayoutcontrolmoduleexclusivelyduringtheinitial30%ofthe\n               denoisingtrajectoryanddeactivateitfortheremaining70%. Thistwo-stagestrategyprovidesrobust\n               low-frequencystructuralguidanceintheearlystagestofacilitatelayoutalignment,whileallowing\n               subsequentunconstrainedrefinementofhigh-frequencydetailsduringlaterdenoisingphases.\n                                                                                    \n                                                                                    \n                   0.9                                                              \n                                                                                    \n                   0.8                                                              \n                   0.7                                                              \n                                                                                    \n                   0.6                                                              \n                                                                                    \n                   0.5                                                              \n                                                                                    \n                   0.4                                                              \n                   0.3                                                              \n                                                                                    \n                   0.2                                                              \n                      0.0     0.2     0.4     0.6    0.8     1.0                    \n                                        Control Ratio                               \n                   erocS                                                            \n                                   3.0                                              \n                                   :detceleS                                        \n                                     Ablation on Control Ratio                      \n                                                                10                  \n                                                                8                   \n                                                                6                   \n                                                           mIoU (LGS) 4             \n                                                           Color (LGS)              \n                                                           Texture (LGS)            \n                                                           Shape (LGS)              \n                                                           VQA                      \n                                                                  )s(               \n                                                                  emiT              \n                                                            Time (s)                \n               Figure9: Impactoftheproportionofdiffusionstepsincorporatinglayoutconditioningongeneration\n               quality.                                                             \n               AsillustratedinFigure9,restrictinglayoutcontroltolessthan30%ofthediffusionprocessresults\n               ininsufficientlayoutalignmentwiththetargetboundingboxes. Incontrast,extendingcontrolbeyond\n               thisoptimalthresholdleadstoadeclineinoutputquality. Furthermore,increasingtheproportionof\n               layout-guidedstepsresultsinsignificantadditionalcomputationalcost.   \n               B  AdditionalAblationStudies                                         \n               B.1 EffectofBboxEncodingandDenseSample                               \n               To clarify the role of bounding box encoding and DenseSample, we further ablated the SD3-M\n               basedInstanceAssemblemodelonDenseLayout. Boundingboxembeddingsguidecorrectobject\n               placement,whileDenseSampleprovidesadditionalimprovementsinspatialaccuracyandinstance-\n               levelsemantics. TheresultsinTable6demonstratethatbothcomponentscontributetotheoverall\n               performance.                                                         \n                         Table6: AblationonboundingboxencodingandDenseSample.       \n                        Setting         mIoU↑ color↑ texture↑ shape↑ VQA↑           \n                        w/obboxencoding,w/oDenseSample 51.22 32.15 34.04 33.53 93.30\n                        w/bboxencoding,w/oDenseSample 51.28 32.68 34.94 34.58 93.33 \n                        w/bboxencoding,w/DenseSample 52.07 33.77 36.21 35.81 93.54  \n               B.2 ComparisonwithAttentionMask-basedRegionInjection                 \n               WealsocompareourAssemble-Attndesignwithattentionmask-basedregioninjection. Whileboth\n               canbeviewedasregion-wiseattentionmechanisms,attentionmasksoperategloballyandmaycause\n                                          16                                        \n\n[表格内容]\n | Time (s)\n3.0\n:detceleS mIoU (LGS)\nColor (LGS)\nTexture (LGS)\nShape (LGS)\nVQA |  | Time (s)\n |  | mIoU (LGS)\nColor (LGS)\nTexture (LGS)\nShape (LGS)\nVQA |\n\nsemanticleakageinoverlappingregions. Ourmethodinsteadappliesinstance-wiseself-attention\n               oncroppedlatentregionsandthenfusestheupdatedfeaturesviatheAssemblestep,whichismore\n               effective in dense layouts. As shown in Table 7, our design achieves superior instance attribute\n               consistencyandahigherVQAscorecomparedtotheattentionmaskbaseline.     \n                   Table7: Comparisonbetweenattentionmask-basedinjectionandourAssemble-Attn.\n                                                                                    \n                         Method        spatial↑ color↑ texture↑ shape↑ VQA↑         \n                         SD3-Medium(basemodel) 77.49 60.28 62.55 60.38 93.30        \n                         Attentionmask(SD3-M) 94.11 74.28 77.58 76.54 91.53         \n                         InstanceAssemble(ours,SD3-M) 94.97 77.53 80.72 80.11 93.12 \n                                                                                    \n               C  Underlyingdataforradar-chartvisualizations                        \n                                                                                    \n               InSections4.3,weutilizedaradarcharttodepicteachquantitativevariablealongequi-angularaxes,\n               providinganintuitivecomparison. Thisvisualizationhighlightsthemultifacetedsuperiorityofour\n               method. Here, wepresentthecorrespondingrawevaluationresultsintabularform. Specifically,\n               Tab.8correspondstoFig.6,thusensuringaclearmappingbetweeneachradar-chartsubfigureand\n               itsunderlyingdata.                                                   \n                     Table8: QuantitativeresultsofadditionalvisualcontentonDenseLayout.\n                                                                                    \n                                       LayoutGroundingScore GlobalQuality           \n                         DenseLayout                                                \n                                      mIoU↑ color↑ texture↑shape↑ VQA↑ Pick↑ CLIP↑  \n                         RealImages(UpperBound) 92.35 76.52 80.78 79.78             \n                         text         43.72 26.57 28.56 28.39 93.37 21.63 12.45     \n                         text+image   55.29 42.15 44.50 44.24 91.66 22.05 12.95     \n                         text+depth   49.64 28.25 31.82 31.62 92.83 21.28 13.25     \n                         text+edge    50.73 29.45 33.92 33.84 90.13 21.26 13.55     \n               D  MoreDetailsonDenseLayoutEvaluationDataset                         \n               D.1 ConstructionPipelineofDenseLayoutDataset                         \n               The DenseLayout dataset is constructed through a multi-stage pipeline designed to extract high-\n               density and semantically-rich layout information from synthetic images. The pipeline includes\n               followingsteps:                                                      \n                                                                                    \n                  1. ImageGenerationusingFlux.1-Dev[4]                              \n                    A diverse set of synthetic images is generated using Flux.1-Dev, a text-to-image model.\n                    Theinputpromptsaregenerictextualdescriptions,sampledfromtheLayoutSAMdataset,\n                    whichisbasedonSA-1B.Theimagesareresizedtomaintainthesameaspectratioasthe\n                    originalSA-1Bimages,withthelongeredgesetto1024pixels. Thisstepprovidesavisually\n                    complexbaseforextractinglayoutstructures.                       \n                  2. Multi-labelTaggingusingRAM++[24]                               \n                    ThegeneratedimagesaretaggedusingRAM++,thenext-generationmodelofRAM,which\n                    supportsopen-setrecognition. Thesetagsofferhigh-levelsemanticguidanceforsubsequent\n                    grounding.                                                      \n                  3. ObjectDetectionviaGroundingDINO[35]                            \n                    Usingtheimageanditspredictedtagsasinput,GroundingDINOperformsopen-setobject\n                    detection.Itoutputsboundingboxesandclasslabelsforalldetectedentities.Thedetectionis\n                    configuredwithabox_thresholdof0.35andatext_thresholdof0.25. Eachdetected\n                    boundingboxistreatedasaninstanceboundingbox, andthecorrespondingpredicted\n                    labelisrecordedastheinstancedescription.                        \n                  4. DetailedCaptioningwithQwen2.5-VL[43]                           \n                    EachboundingboxregioniscroppedfromtheoriginalimageandfedintoQwen2.5-VL\n                                          17\n\ntogenerateafine-grainedcaption. Theseregion-levelcaptionsarestoredasthedetailed\n                    descriptionforeachinstance,enrichingthesemanticinformationbeyondcategorylabels.\n                  5. DensityFiltering                                               \n                    Toensurehighlayoutcomplexity,onlyimageswith15ormoredetectedinstances(asoutput\n                    byGroundingDINO)areretained. Thisresultsinadenselayoutdistributionsuitablefor\n                    layout-conditionedgenerationtasks. ThedistributionofinstancecountisshowninFig.10.\n               Finally,theDenseLayoutdatasetcontains5,000imagesand90,339instances,withanaverageof\n               18.1instancesperimage.                                               \n                                                                                    \n                                                                                    \n                           1200                                                     \n                           1000                                                     \n                                                                                    \n                           800                                                      \n                           600                                                      \n                           400                                                      \n                                                                                    \n                           200                                                      \n                            0 15 20  25 30  35 40  45 50                            \n                                       Number of Instances per Image                \n                          tnuoC                                                     \n                          egamI                                                     \n                                    DenseLayout: Instance Count Distribution        \n                                                      Mean: 18.1                    \n                                                      Median: 17.0 Std: 3.7         \n                        Figure10: InstancecountdistributionperimageinDenseLayout.   \n               AnnotationFormat. Theannotationforeachimageconsistsof:               \n                   • global_caption: theoriginalpromptusedforimagegeneration.       \n                   • image_info: metadataoftheimage,includingheightandwidth.        \n                   • instance_info: alistofinstances,eachwith:                      \n                     – bbox: theboundingboxoftheinstance,formattingas[x ,y ,x ,y ]. \n                                                      1 1 2 2                       \n                     – description: thecategorylabelpredictedbyGroundingDINO.       \n                     – detail_description: a fine-grained caption generated by Qwen2.5-VL for the\n                       croppedregion.                                               \n               D.2 SamplesofDenseLayoutDataset                                      \n                                      \"instance_info\": [                            \n                                      {                                             \n                                        \"bbox\": [129,489,283,642],                  \n                                        \"description\": \"nightstand\",                \n                                        \"detail_description\": \"The nightstand is dark brown,\n                                           compact, with a drawer.\"                 \n                                      },                                            \n                                      {                                             \n                                        \"bbox\": [306,170,430,339],                  \n                                        \"description\": \"picture frame\",             \n                                        \"detail_description\": \"Brown wooden frame containing a\n                                           watercolor painting of green leaves on a white\n                                           background.\"                             \n                                      },                                            \n                                      {                                             \n                                        \"bbox\": [603,170,727,340],                  \n                                        \"description\": \"picture frame\",             \n                                        \"detail_description\": \"A simple brown wooden frame holds\n                                           a botanical print with detailed leaves and stems.\"\n                                      },                                            \n                                      ...                                           \n                                      ]                                             \n                           Figure11: AsampleofDenseLayoutanditsannotation.          \n                                          18                                        \n\n[表格内容]\n |  |  |  |  |  |  |  |  |  |  |  |  |  |  | M\nMe | ean: 18.1\ndian: 17.0 | \n |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | Std: 3.7 | \n |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | \n |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | \n\n\n\"instance_info\": [\n{\n\"bbox\": [129,489,283,642],\n\"description\": \"nightstand\",\n\"detail_description\": \"The nightstand is dark brown,\ncompact, with a drawer.\"\n},\n{\n\"bbox\": [306,170,430,339],\n\"description\": \"picture frame\",\n\"detail_description\": \"Brown wooden frame containing a\nwatercolor painting of green leaves on a white\nbackground.\"\n},\n{\n\"bbox\": [603,170,727,340],\n\"description\": \"picture frame\",\n\"detail_description\": \"A simple brown wooden frame holds\na botanical print with detailed leaves and stems.\"\n},\n...\n]\n\nThis is a photo showcasing a traditional Chinese-style pavilion and This is a photo depicting a traditional Asian floating market scene. In This is a photo showcasing a Chinese-style building and a statue. The\n               boats on the river. The pavilion is located on the right side of the the picture, two women are sitting in their respective boats, each building is brightly colored, with a red roof and a yellow door, and the\n               picture, with a beautifully decorated roof and a golden plaque hanging wearing a traditional conical hat, and dressed in brightly colored statue is located in front of the building, standing on a pedestal. The\n               in the middle. In front of the pavilion is a row of wooden boats, each traditional clothing. The woman on the left is wearing a blue top and a surrounding environment is a spacious square, with several pedestrians\n               with a red flag hanging on it, and people are gathered on the boat dock. dark skirt, while the woman on the right is wearing a yellow top and a walking around the building. The background is a clear blue sky and lush\n               The river is calm, and the sky is clear, with a few clouds leisurely light-colored skirt. Their boats are filled with a variety of goods, trees.\n               drifting by.      including fresh flowers, fruits, and other food items. The boats are\n                                 adorned with bright colored cloths, adding a festive atmosphere to the\n                                 scene. The entire scene is captured under natural light, presenting a\n                                 tranquil and vibrant market atmosphere.            \n               This is a photo showcasing a modern interior design style, with the This is a realistic-style photograph depicting a city street scene after This is a photograph showcasing a famous archway in a city. The archway\n               focus on a spacious and bright room. The room is furnished with wooden a flood. In the photo, vehicles and pedestrians are struggling to is a reddish-brown structure, adorned with golden decorations and\n               furniture, including a long wooden table and a matching wooden bench. On navigate through the flooded streets. A yellow tricycle is parked in the sculptures, and its design is very intricate. The archway is located on\n               the table, there are some books and a laptop, while on the bench, there middle of the street, surrounded by vehicles covered with blue a wide street, with pedestrians and cyclists weaving through the road.\n               is a black desk lamp. The room's lighting comes from several minimalist waterproof cloth. Pedestrians are using umbrellas to shield themselves The sky is clear, with a few clouds scattered in the blue sky. The\n               chandeliers and a large window, through which you can see the green from the rain, some are pushing bicycles, while others are walking. The buildings in the background have a Mediterranean style, with the domes\n               plants outside. The floor is covered with a red carpet, and there are buildings on both sides of the street are submerged in water, and the of some buildings visible in the distance.\n               several decorations on the floor, including a red vase and a decorative wires and poles above the street are also submerged, adding to the\n               sculpture. The entire scene is illuminated by natural light, creating a severity of the flooding. The entire scene is shrouded in a gloomy\n               warm and comfortable atmosphere. atmosphere, with no sunlight piercing through the clouds.\n               T f t b r a a ph oo h a r ri or e r f e es fe n e s sg g w c ei .r r a o ns o a n c v t Iu s d a e ia nn s r r n d l a s e gp f, a d h r n s p ao oi d m a w t nt , a r i to ts l k t r t l e h as os h e d nh fu e r d qo r r i e uw tf e h n n ic ha o s la ec a u f e s e r s r ai b e e o t nn up , n r dg ir s t e le e b e pa ds v o o s e ie e t f , ar nn r h cu gt a t a er si l w h n fa ,n i e d ul g w t l t o h f t l ha o e h ra e d t n e un rv e r c rd ei n a e s as v d . k lc ii b i y a sd u t I ap i i n i te ag l o s m, r d n t o re i a h f sw oe n l e i pi wn g l ht s w b l eh oa , o a e r fn o c d ea d i d k . w n e g wg oy c n r ir oe l o ta dl u s u hs el d t n s no i r d wl w n u , ha f g c in ec t t td no a u h e cl r e i eo l e cn sr a s h l ,. r i ot g a l uh wO e n l de in r d s s t ,h T m d e d a p sx th a e e ap ai g p c f tr is n i o i te n i c r g es ei f t a u rs ds i s t r ni c i e so ga e a v n l n e i a. ap t f n n sh i e dT so s g l a h t c u e fe wo u r m h l i l e e e os ns p n l rc dh t i t m au oo u n s e ll ww r , t p sc e a . dt ,a i eu s l l n T sr ti o o c h ie hn c n l e g rg a g u ni o t d a ss ut e r i l , gh d o n t s he b g a au i e r nr wi n , a dr hn i o it t h s s tu ce h o t hn hr e l a d ed i d t e e to c i u c wd hr e n e o a e n g r lb o t o a ly lf e a f t s i r e e ga c a d ax h o r rq tc f o f w eu h s i i i fu t s g t as rr h , u h di oc e r ot mh w e c re , a i o n o l t h m es uw t h o p dt ti a l l a st r a d e wt ih . i x iu d s n te et T o g g hs h h l e se e e a o ca h m m on if s n c e ld no c r t o ec u o r r su l s i f s p s c u i t l no u a ,n r n e da T w g s s i a be eh i l u n v ni t a r c fe cs h s r l er h s o u wa ei a u d l ss a n i p , m n d n es a o d e g de e d d et np e m r ss jh r e b e t oo n t y d ro yt a i- f io l s b as n e r nt s gs s v i sy t h t e c l a to r r k oe i hw u a n r ec c l b b s a t u tu ss u b i hi a ui r u l el n nn e i d d d sg s l i si h . d n qn b ia i g ug e n T n s a n em h g ri c .o e s a en h d n , e Te s o d t s hr q f sh en u m oe f a d o m o su r i d ec r kr e f e e yb f r an p a i e n rt e in s r ee o s e g r p s p n l w, l cq a t a a e lu v s li ea e a s kt t ar d r is o re c c n ,. w h u ge r i i r ,x e wT t t t t s ih h e a se t te c i or . h g t n mi s r u eo T aq e r w r h u y a a a e fa l l rc er s l eo s we l s s m q a t . sp u wi b y io a hs s l T ts r i , e h te e ts s e id ep w , r n i a i e go sc t f i h a o o r nu e s ,\n                                                                                    \n                                 adding a warm and sacred atmosphere to the interior. clouds scattered in the blue sky.\n               Figure12: RepresentativesamplesfromDenseLayoutdatasetdemonstrating: (a)High-densityscene\n               with≥15instances,(b)Complexinstancerelationshipswithpreciseattributespecifications.\n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                          19\n\nE  Moreresultswithtextual-onlycontent                                \n               E.1 InstanceAssemblebasedonSD3-Medium                                \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                    x                                                               \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                  Figure13: MoreresultsofInstanceAssemblebasedonSD3-Mediumwithtextual-onlycontent.\n                                                                                    \n                                                                                    \n               E.2 InstanceAssemblebasedonFlux.1-Dev                                \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                   Figure14: MoreresultsofInstanceAssemblebasedonFlux.1-Devwithtextual-onlycontent.\n                                                                                    \n               E.3 InstanceAssemblebasedonFlux.1-Schnell                            \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                  Figure15: MoreresultsofInstanceAssemblebasedonFlux.1-Schnellwithtextual-onlycontent.\n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                          20                                        \n                                                                                    \n                                                                                    \n\n[表格内容]\n |  |  |  |  |  | \n |  |  |  |  |  | \n |  |  |  |  |  | \n |  |  |  |  |  |\n\nF  Moreresultswithadditionalvisualcontent                            \n               F.1 AdditionalImage                                                  \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                             Figure16: Moreresultswithadditionalimage.              \n                                                                                    \n                                                                                    \n               F.2 AdditionalDepth                                                  \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                              Figure17: Moreresultswithadditionaldepth.             \n                                                                                    \n               F.3 AdditionalEdge                                                   \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                              Figure18: Moreresultswithadditionaledge.              \n                                                                                    \n                                                                                    \n                                          21                                        \n                                                                                    \n                                                                                    \n\n[表格内容]\nFigure16: Moreresultswithadditionalimage.\n.2 AdditionalDepth\nFigure17: Moreresultswithadditionaldepth.\n.3 AdditionalEdge\n\nFigure18: Moreresultswithadditionaledge.\n21"
  },
  "pdf_url": "/uploads/e7ea42616f1a6cf6158fe8cd88ea1a9b.pdf"
}