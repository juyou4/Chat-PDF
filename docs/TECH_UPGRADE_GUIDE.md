# ChatPDF Pro - æŠ€æœ¯æ ˆå‡çº§æ–¹æ¡ˆ

## ğŸ“Š å½“å‰æŠ€æœ¯æ ˆ vs å»ºè®®æŠ€æœ¯æ ˆå¯¹æ¯”

### å½“å‰å®ç° âœ…

| æ¨¡å— | å½“å‰æŠ€æœ¯ | ä¼˜åŠ¿ | ä¸è¶³ |
|------|---------|------|------|
| **åç«¯æ¡†æ¶** | FastAPI âœ… | é«˜æ€§èƒ½ã€å¼‚æ­¥æ”¯æŒ | - |
| **å‰ç«¯ç•Œé¢** | React + Tailwind | ä¸“ä¸šUIã€å¯å®šåˆ¶æ€§å¼º | å¼€å‘æ—¶é—´è¾ƒé•¿ |
| **PDFè§£æ** | PyPDF2 | ç®€å•æ˜“ç”¨ | ä¸æ”¯æŒè¡¨æ ¼ã€å›¾ç‰‡ |
| **æ–‡æœ¬å¤„ç†** | ç®€å•åˆ†é¡µ | å®ç°ç®€å• | æ— æ™ºèƒ½åˆ†å— |
| **å‘é‡æ£€ç´¢** | âŒ æœªå®ç° | - | é•¿æ–‡æ¡£æ£€ç´¢ä¸å‡†ç¡® |
| **å‘é‡æ•°æ®åº“** | âŒ æœªå®ç° | - | æ— è¯­ä¹‰æ£€ç´¢ |
| **LLM API** | 10+æä¾›å•† âœ… | é€‰æ‹©ä¸°å¯Œ | - |
| **æµå¼å“åº”** | âŒ æœªå®ç° | - | æ— æ‰“å­—æœºæ•ˆæœ |

---

## ğŸš€ æ¨èå‡çº§æ–¹æ¡ˆ

### æ–¹æ¡ˆAï¼šæ¸è¿›å¼å‡çº§ï¼ˆæ¨èï¼‰

ä¿ç•™ç°æœ‰æ¶æ„ï¼Œé€æ­¥æ·»åŠ æ–°åŠŸèƒ½ï¼š

#### 1ï¸âƒ£ **PDFè§£æå‡çº§** - ä¼˜å…ˆçº§ï¼šâ­â­â­â­â­

**ä» PyPDF2 â†’ pdfplumber + fitz**

**ä¼˜åŠ¿:**
- âœ… pdfplumber: æ›´å¥½çš„è¡¨æ ¼æå–
- âœ… fitz (PyMuPDF): æå–å›¾ç‰‡å’Œå‡†ç¡®å®šä½
- âœ… å…¼å®¹æ€§å¥½ï¼Œå¯ä»¥åŒæ—¶ä½¿ç”¨

**å®ç°æ–¹æ¡ˆ:**
```python
# å®‰è£…ä¾èµ–
pip install pdfplumber pymupdf

# æ–°å¢ PDF è§£ææ¨¡å—
import pdfplumber
import fitz  # PyMuPDF

def extract_text_with_pdfplumber(pdf_path):
    """ä½¿ç”¨pdfplumberæå–æ–‡æœ¬å’Œè¡¨æ ¼"""
    pages_content = []
    
    with pdfplumber.open(pdf_path) as pdf:
        for page_num, page in enumerate(pdf.pages):
            # æå–æ–‡æœ¬
            text = page.extract_text() or ""
            
            # æå–è¡¨æ ¼
            tables = page.extract_tables()
            
            # æå–å›¾ç‰‡ä¿¡æ¯
            images = page.images
            
            pages_content.append({
                "page": page_num + 1,
                "text": text,
                "tables": tables,
                "has_images": len(images) > 0,
                "image_count": len(images)
            })
    
    return pages_content

def extract_images_with_fitz(pdf_path, output_folder="./images"):
    """ä½¿ç”¨PyMuPDFæå–å›¾ç‰‡"""
    import os
    os.makedirs(output_folder, exist_ok=True)
    
    doc = fitz.open(pdf_path)
    images_info = []
    
    for page_num in range(len(doc)):
        page = doc[page_num]
        image_list = page.get_images()
        
        for img_index, img in enumerate(image_list):
            xref = img[0]
            base_image = doc.extract_image(xref)
            image_bytes = base_image["image"]
            
            # ä¿å­˜å›¾ç‰‡
            image_path = f"{output_folder}/page{page_num+1}_img{img_index}.png"
            with open(image_path, "wb") as f:
                f.write(image_bytes)
            
            images_info.append({
                "page": page_num + 1,
                "path": image_path,
                "size": base_image["width"] * base_image["height"]
            })
    
    return images_info
```

#### 2ï¸âƒ£ **å‘é‡æ£€ç´¢ç³»ç»Ÿ** - ä¼˜å…ˆçº§ï¼šâ­â­â­â­â­

**æ·»åŠ  FAISS + LangChain**

**ä¼˜åŠ¿:**
- âœ… å¤§å¹…æå‡é•¿æ–‡æ¡£æ£€ç´¢å‡†ç¡®æ€§
- âœ… è¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢
- âœ… å‡å°‘tokenæ¶ˆè€—ï¼ˆåªæ£€ç´¢ç›¸å…³ç‰‡æ®µï¼‰

**å®ç°æ–¹æ¡ˆ:**
```python
# å®‰è£…ä¾èµ–
pip install langchain faiss-cpu sentence-transformers openai

# å‘é‡æ£€ç´¢æ¨¡å—
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.docstore.document import Document

class VectorStore:
    def __init__(self, embedding_provider="openai", api_key=None):
        """åˆå§‹åŒ–å‘é‡å­˜å‚¨"""
        if embedding_provider == "openai":
            self.embeddings = OpenAIEmbeddings(openai_api_key=api_key)
        else:
            # ä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼ˆå…è´¹ï¼Œä½†è¾ƒæ…¢ï¼‰
            self.embeddings = HuggingFaceEmbeddings(
                model_name="BAAI/bge-small-zh-v1.5"  # ä¸­æ–‡ä¼˜åŒ–
            )
        
        self.vector_stores = {}  # doc_id -> FAISS store
    
    def create_vector_store(self, doc_id, pages_content):
        """ä¸ºæ–‡æ¡£åˆ›å»ºå‘é‡å­˜å‚¨"""
        # æ–‡æœ¬åˆ†å—
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,  # æ¯å—500å­—ç¬¦
            chunk_overlap=50,  # é‡å 50å­—ç¬¦
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?", " "]
        )
        
        # åˆ›å»ºæ–‡æ¡£
        documents = []
        for page in pages_content:
            chunks = text_splitter.split_text(page["content"])
            for i, chunk in enumerate(chunks):
                documents.append(Document(
                    page_content=chunk,
                    metadata={
                        "page": page["page"],
                        "chunk_id": i,
                        "doc_id": doc_id
                    }
                ))
        
        # åˆ›å»ºå‘é‡å­˜å‚¨
        vector_store = FAISS.from_documents(documents, self.embeddings)
        self.vector_stores[doc_id] = vector_store
        
        return len(documents)
    
    def search_similar(self, doc_id, query, k=3):
        """æ£€ç´¢ç›¸ä¼¼ç‰‡æ®µ"""
        if doc_id not in self.vector_stores:
            return []
        
        results = self.vector_stores[doc_id].similarity_search_with_score(
            query, k=k
        )
        
        return [{
            "content": doc.page_content,
            "page": doc.metadata["page"],
            "score": float(score)
        } for doc, score in results]
    
    def save(self, doc_id, path):
        """ä¿å­˜å‘é‡å­˜å‚¨åˆ°ç£ç›˜"""
        if doc_id in self.vector_stores:
            self.vector_stores[doc_id].save_local(path)
    
    def load(self, doc_id, path):
        """ä»ç£ç›˜åŠ è½½å‘é‡å­˜å‚¨"""
        self.vector_stores[doc_id] = FAISS.load_local(
            path, self.embeddings
        )
```

**åç«¯APIæ–°å¢ç«¯ç‚¹:**
```python
# åœ¨ chatpdf_pro_backend.py ä¸­æ·»åŠ 

from typing import Optional
vector_store_manager = None  # å…¨å±€å‘é‡å­˜å‚¨ç®¡ç†å™¨

class VectorSearchRequest(BaseModel):
    doc_id: str
    query: str
    api_key: str
    k: int = 3

@app.post("/vector/create")
async def create_vector_store(doc_id: str, embedding_provider: str = "local"):
    """ä¸ºæ–‡æ¡£åˆ›å»ºå‘é‡å­˜å‚¨"""
    global vector_store_manager
    
    if doc_id not in documents_store:
        raise HTTPException(status_code=404, detail="æ–‡æ¡£æœªæ‰¾åˆ°")
    
    if vector_store_manager is None:
        vector_store_manager = VectorStore(embedding_provider)
    
    doc = documents_store[doc_id]
    num_chunks = vector_store_manager.create_vector_store(
        doc_id, doc["data"]["pages"]
    )
    
    return {
        "doc_id": doc_id,
        "chunks_created": num_chunks,
        "message": "å‘é‡å­˜å‚¨åˆ›å»ºæˆåŠŸ"
    }

@app.post("/vector/search")
async def vector_search(request: VectorSearchRequest):
    """å‘é‡æ£€ç´¢ç›¸å…³å†…å®¹"""
    global vector_store_manager
    
    if vector_store_manager is None or request.doc_id not in vector_store_manager.vector_stores:
        raise HTTPException(status_code=404, detail="å‘é‡å­˜å‚¨æœªæ‰¾åˆ°ï¼Œè¯·å…ˆåˆ›å»º")
    
    results = vector_store_manager.search_similar(
        request.doc_id, request.query, request.k
    )
    
    return {
        "results": results,
        "query": request.query
    }
```

#### 3ï¸âƒ£ **æµå¼å“åº”ï¼ˆSSEï¼‰** - ä¼˜å…ˆçº§ï¼šâ­â­â­â­

**æ·»åŠ æ‰“å­—æœºæ•ˆæœ**

**ä¼˜åŠ¿:**
- âœ… æ›´å¥½çš„ç”¨æˆ·ä½“éªŒ
- âœ… å®æ—¶çœ‹åˆ°AIæ€è€ƒè¿‡ç¨‹
- âœ… é™ä½ç­‰å¾…ç„¦è™‘

**åç«¯å®ç°:**
```python
from fastapi.responses import StreamingResponse
import json

async def generate_stream_response(messages, api_key, model, provider):
    """æµå¼ç”ŸæˆAIå“åº”"""
    
    if provider == "openai":
        async with httpx.AsyncClient(timeout=120.0) as client:
            async with client.stream(
                "POST",
                "https://api.openai.com/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": model,
                    "messages": messages,
                    "stream": True  # å¼€å¯æµå¼
                }
            ) as response:
                async for line in response.aiter_lines():
                    if line.startswith("data: "):
                        data = line[6:]
                        if data == "[DONE]":
                            break
                        try:
                            chunk = json.loads(data)
                            if "choices" in chunk and len(chunk["choices"]) > 0:
                                delta = chunk["choices"][0].get("delta", {})
                                content = delta.get("content", "")
                                if content:
                                    yield f"data: {json.dumps({'content': content})}\n\n"
                        except:
                            continue

@app.post("/chat/stream")
async def chat_stream(request: ChatRequest):
    """æµå¼å¯¹è¯"""
    if request.doc_id not in documents_store:
        raise HTTPException(status_code=404, detail="æ–‡æ¡£æœªæ‰¾åˆ°")
    
    # æ„å»ºæ¶ˆæ¯ï¼ˆåŒä¹‹å‰çš„chatç«¯ç‚¹ï¼‰
    # ... çœç•¥æ¶ˆæ¯æ„å»ºä»£ç  ...
    
    return StreamingResponse(
        generate_stream_response(messages, request.api_key, request.model, request.api_provider),
        media_type="text/event-stream"
    )
```

**å‰ç«¯å®ç°ï¼ˆReactï¼‰:**
```jsx
const sendMessageStream = async () => {
  const response = await fetch(`${API_BASE_URL}/chat/stream`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      doc_id: docId,
      question: inputMessage,
      api_key: apiKey,
      model: model,
      api_provider: apiProvider
    })
  });

  const reader = response.body.getReader();
  const decoder = new TextDecoder();
  
  let assistantMessage = "";
  setMessages(prev => [...prev, { type: 'assistant', content: '', streaming: true }]);
  
  while (true) {
    const { done, value } = await reader.read();
    if (done) break;
    
    const chunk = decoder.decode(value);
    const lines = chunk.split('\n');
    
    for (const line of lines) {
      if (line.startsWith('data: ')) {
        const data = JSON.parse(line.slice(6));
        assistantMessage += data.content;
        
        // æ›´æ–°æœ€åä¸€æ¡æ¶ˆæ¯
        setMessages(prev => {
          const newMessages = [...prev];
          newMessages[newMessages.length - 1].content = assistantMessage;
          return newMessages;
        });
      }
    }
  }
  
  // æ ‡è®°æµå¼å®Œæˆ
  setMessages(prev => {
    const newMessages = [...prev];
    newMessages[newMessages.length - 1].streaming = false;
    return newMessages;
  });
};
```

#### 4ï¸âƒ£ **ç¼“å­˜æœºåˆ¶ï¼ˆRedisï¼‰** - ä¼˜å…ˆçº§ï¼šâ­â­â­

**æ·»åŠ å“åº”ç¼“å­˜**

**ä¼˜åŠ¿:**
- âœ… ç›¸åŒé—®é¢˜ä¸é‡å¤è°ƒç”¨API
- âœ… èŠ‚çœæˆæœ¬
- âœ… æå‡å“åº”é€Ÿåº¦

**å®ç°:**
```python
# å®‰è£…
pip install redis

# ç¼“å­˜ç®¡ç†
import redis
import hashlib

class ResponseCache:
    def __init__(self, redis_url="redis://localhost:6379"):
        self.redis_client = redis.from_url(redis_url)
        self.ttl = 3600  # ç¼“å­˜1å°æ—¶
    
    def get_cache_key(self, doc_id, question, model):
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_str = f"{doc_id}:{question}:{model}"
        return hashlib.md5(key_str.encode()).hexdigest()
    
    def get(self, doc_id, question, model):
        """è·å–ç¼“å­˜"""
        key = self.get_cache_key(doc_id, question, model)
        cached = self.redis_client.get(key)
        if cached:
            return json.loads(cached)
        return None
    
    def set(self, doc_id, question, model, response):
        """è®¾ç½®ç¼“å­˜"""
        key = self.get_cache_key(doc_id, question, model)
        self.redis_client.setex(
            key, 
            self.ttl, 
            json.dumps(response)
        )

# åœ¨APIä¸­ä½¿ç”¨
cache = ResponseCache()

@app.post("/chat")
async def chat_with_pdf(request: ChatRequest):
    # æ£€æŸ¥ç¼“å­˜
    cached_response = cache.get(request.doc_id, request.question, request.model)
    if cached_response:
        return {**cached_response, "cached": True}
    
    # ... æ­£å¸¸å¤„ç† ...
    
    # ç¼“å­˜å“åº”
    cache.set(request.doc_id, request.question, request.model, result)
    return result
```

---

### æ–¹æ¡ˆBï¼šå‰ç«¯æŠ€æœ¯æ›¿æ¢ï¼ˆå¯é€‰ï¼‰

#### é€‰é¡¹1ï¼šGradioï¼ˆæœ€å¿«å®ç°ï¼‰

**ä¼˜åŠ¿:**
- âš¡ 5åˆ†é’Ÿæ­å»ºUI
- ğŸ“± è‡ªåŠ¨ç”Ÿæˆç§»åŠ¨ç«¯
- ğŸ”„ å†…ç½®æµå¼è¾“å‡º

**ç¼ºç‚¹:**
- âŒ å®šåˆ¶æ€§å·®
- âŒ æ— æ³•å®ç°å¤æ‚äº¤äº’
- âŒ ä¸é€‚åˆå•†ä¸šåŒ–

**å®ç°ç¤ºä¾‹:**
```python
import gradio as gr

def chat_interface(pdf_file, question, api_key, model_choice):
    """GradioèŠå¤©ç•Œé¢"""
    # ä¸Šä¼ PDF
    doc_id = upload_pdf(pdf_file)
    
    # è°ƒç”¨API
    response = call_ai_api(doc_id, question, api_key, model_choice)
    
    return response["answer"]

# åˆ›å»ºç•Œé¢
demo = gr.Interface(
    fn=chat_interface,
    inputs=[
        gr.File(label="ä¸Šä¼ PDF"),
        gr.Textbox(label="æé—®"),
        gr.Textbox(label="API Key", type="password"),
        gr.Dropdown(["GPT-4", "Claude", "Qwen"], label="æ¨¡å‹")
    ],
    outputs=gr.Textbox(label="AIå›ç­”"),
    title="ChatPDF Pro - Gradioç‰ˆ",
    description="ä¸Šä¼ PDFæ–‡æ¡£ï¼Œå‘AIæé—®"
)

demo.launch()
```

#### é€‰é¡¹2ï¼šVue3 + Element Plus

**ä¼˜åŠ¿:**
- âœ… ç»„ä»¶åº“ä¸°å¯Œ
- âœ… å›½å†…ç”Ÿæ€å¥½
- âœ… å¼€å‘æ•ˆç‡é«˜

**ç¼ºç‚¹:**
- éœ€è¦é‡å†™å‰ç«¯

**å»ºè®®:** é™¤éæœ‰ç‰¹æ®Šéœ€æ±‚ï¼Œå¦åˆ™ä¿ç•™Reactå³å¯

---

### æ–¹æ¡ˆCï¼šå®Œæ•´æ¶æ„å‡çº§

#### æ–°å¢æ¨¡å—æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   å‰ç«¯å±‚                          â”‚
â”‚  React + Tailwind  â”‚  æµå¼æ˜¾ç¤º  â”‚  å‘é‡æ£€ç´¢UI     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  APIç½‘å…³å±‚                        â”‚
â”‚        FastAPI  â”‚  SSEæ”¯æŒ  â”‚  è¯·æ±‚é™æµ          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                     â”‚              â”‚
â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
â”‚PDFå¤„ç†  â”‚      â”‚  å‘é‡æ£€ç´¢     â”‚  â”‚ AIè°ƒç”¨   â”‚
â”‚        â”‚      â”‚              â”‚  â”‚          â”‚
â”‚pdfplumberâ”‚    â”‚  LangChain   â”‚  â”‚ 10+æ¨¡å‹  â”‚
â”‚  fitz   â”‚      â”‚    FAISS     â”‚  â”‚  æ”¯æŒ    â”‚
â”‚        â”‚      â”‚  Embedding   â”‚  â”‚          â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
    â”‚                    â”‚             â”‚
â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
â”‚              æ•°æ®å­˜å‚¨å±‚                      â”‚
â”‚  æ–‡æ¡£å­˜å‚¨  â”‚  å‘é‡å­˜å‚¨  â”‚  Redisç¼“å­˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ å‡çº§ä¼˜å…ˆçº§å»ºè®®

### ç«‹å³å®æ–½ï¼ˆ1-2å¤©ï¼‰

1. **PDFè§£æå‡çº§** â†’ pdfplumber
   - æˆæœ¬ï¼šä½
   - æ”¶ç›Šï¼šé«˜ï¼ˆæ”¯æŒè¡¨æ ¼ï¼‰
   - é£é™©ï¼šä½

2. **æµå¼å“åº”ï¼ˆSSEï¼‰**
   - æˆæœ¬ï¼šä¸­
   - æ”¶ç›Šï¼šé«˜ï¼ˆç”¨æˆ·ä½“éªŒï¼‰
   - é£é™©ï¼šä½

### è¿‘æœŸå®æ–½ï¼ˆ1å‘¨å†…ï¼‰

3. **å‘é‡æ£€ç´¢ç³»ç»Ÿ** â†’ FAISS + LangChain
   - æˆæœ¬ï¼šä¸­
   - æ”¶ç›Šï¼šæé«˜ï¼ˆå‡†ç¡®æ€§æå‡ï¼‰
   - é£é™©ï¼šä¸­

4. **ç¼“å­˜æœºåˆ¶** â†’ Redis
   - æˆæœ¬ï¼šä½
   - æ”¶ç›Šï¼šä¸­ï¼ˆæˆæœ¬èŠ‚çœï¼‰
   - é£é™©ï¼šä½

### ä¸­æœŸè§„åˆ’ï¼ˆ1ä¸ªæœˆå†…ï¼‰

5. **å›¾ç‰‡æå–** â†’ PyMuPDF
6. **å¼‚æ­¥ä»»åŠ¡é˜Ÿåˆ—** â†’ Celery
7. **æ€§èƒ½ç›‘æ§** â†’ Prometheus

---

## ğŸ’° æˆæœ¬æ•ˆç›Šåˆ†æ

### å‡çº§æˆæœ¬

| å‡çº§é¡¹ | å¼€å‘æ—¶é—´ | é¢å¤–æˆæœ¬ | ROI |
|--------|---------|---------|-----|
| pdfplumber | 2å°æ—¶ | $0 | â­â­â­â­â­ |
| æµå¼å“åº” | 4å°æ—¶ | $0 | â­â­â­â­â­ |
| å‘é‡æ£€ç´¢ | 8å°æ—¶ | Embeddingè´¹ç”¨ | â­â­â­â­ |
| Redisç¼“å­˜ | 3å°æ—¶ | æœåŠ¡å™¨($5/æœˆ) | â­â­â­â­ |
| å›¾ç‰‡æå– | 3å°æ—¶ | $0 | â­â­â­ |

### æ”¶ç›Šåˆ†æ

**å‘é‡æ£€ç´¢æ”¶ç›Š:**
- å‡†ç¡®æ€§æå‡ï¼š30-50%
- TokenèŠ‚çœï¼š40-60%
- æˆæœ¬èŠ‚çœï¼šæ¯1000æ¬¡æŸ¥è¯¢çº¦$5-10

**ç¼“å­˜æ”¶ç›Š:**
- é‡å¤é—®é¢˜å‘½ä¸­ç‡ï¼š20-40%
- å“åº”é€Ÿåº¦ï¼š10xæå‡
- æˆæœ¬èŠ‚çœï¼š20-40%

---

## ğŸ¯ æ¨èå®æ–½è·¯çº¿

### é˜¶æ®µ1ï¼šåŸºç¡€å¢å¼ºï¼ˆæœ¬å‘¨ï¼‰
```bash
# Day 1-2
pip install pdfplumber
# å®ç°è¡¨æ ¼æå–

# Day 3-4
# å®ç°æµå¼å“åº”ï¼ˆSSEï¼‰

# Day 5
# æµ‹è¯•å’Œä¼˜åŒ–
```

### é˜¶æ®µ2ï¼šæ™ºèƒ½æ£€ç´¢ï¼ˆä¸‹å‘¨ï¼‰
```bash
# Week 2
pip install langchain faiss-cpu sentence-transformers
# å®ç°å‘é‡æ£€ç´¢ç³»ç»Ÿ
# é›†æˆåˆ°ç°æœ‰API
```

### é˜¶æ®µ3ï¼šæ€§èƒ½ä¼˜åŒ–ï¼ˆç¬¬ä¸‰å‘¨ï¼‰
```bash
# Week 3
pip install redis
# æ·»åŠ ç¼“å­˜æœºåˆ¶
# æ€§èƒ½æµ‹è¯•
```

---

## ğŸ“ ç»“è®ºå’Œå»ºè®®

### å¿…é¡»å‡çº§ â­â­â­â­â­
1. **pdfplumber** - æ”¯æŒè¡¨æ ¼ï¼Œæˆæœ¬æä½
2. **æµå¼å“åº”** - å¤§å¹…æå‡ä½“éªŒ
3. **å‘é‡æ£€ç´¢** - æ ¸å¿ƒåŠŸèƒ½ï¼Œå¿…ä¸å¯å°‘

### å¼ºçƒˆæ¨è â­â­â­â­
4. **Redisç¼“å­˜** - èŠ‚çœæˆæœ¬
5. **å›¾ç‰‡æå–** - å®Œæ•´åŠŸèƒ½

### å¯é€‰å‡çº§ â­â­â­
6. **Gradioç‰ˆæœ¬** - ä»…ç”¨äºå¿«é€ŸDemo
7. **Celeryé˜Ÿåˆ—** - å¤§è§„æ¨¡éƒ¨ç½²æ—¶éœ€è¦

### ä¸æ¨è â­
- **Spring Booté‡å†™** - ç°æœ‰FastAPIè¶³å¤Ÿå¥½
- **Vue3é‡å†™å‰ç«¯** - Reactå·²ç»å¾ˆæˆç†Ÿ

---

**ç«‹å³å¼€å§‹å‡çº§ï¼šå…ˆå®ç°pdfplumber + æµå¼å“åº”ï¼Œå¿«é€Ÿè§æ•ˆï¼** ğŸš€
